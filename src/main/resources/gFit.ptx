//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-20732876
// Cuda compilation tools, release 8.0, V8.0.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	gaussFitter
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry gaussFitter(
	.param .u64 gaussFitter_param_0,
	.param .u32 gaussFitter_param_1,
	.param .u64 gaussFitter_param_2,
	.param .u32 gaussFitter_param_3,
	.param .u16 gaussFitter_param_4,
	.param .u64 gaussFitter_param_5,
	.param .u32 gaussFitter_param_6,
	.param .u64 gaussFitter_param_7,
	.param .u32 gaussFitter_param_8,
	.param .f64 gaussFitter_param_9,
	.param .u32 gaussFitter_param_10
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<210>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<494>;
	.reg .f64 	%fd<1858>;
	.reg .b64 	%rd<277>;


	mov.u64 	%rd276, __local_depot0;
	cvta.local.u64 	%SP, %rd276;
	ld.param.u64 	%rd14, [gaussFitter_param_0];
	ld.param.u32 	%r151, [gaussFitter_param_1];
	ld.param.u64 	%rd15, [gaussFitter_param_2];
	ld.param.u16 	%rs5, [gaussFitter_param_4];
	ld.param.u64 	%rd16, [gaussFitter_param_5];
	ld.param.u64 	%rd17, [gaussFitter_param_7];
	cvta.to.global.u64 	%rd1, %rd14;
	mov.u32 	%r152, %nctaid.x;
	mov.u32 	%r153, %ctaid.y;
	mov.u32 	%r154, %ctaid.x;
	mad.lo.s32 	%r1, %r152, %r153, %r154;
	cvt.u32.u16	%r2, %rs5;
	mul.wide.u16 	%r3, %rs5, %rs5;
	div.s32 	%r155, %r151, %r3;
	setp.ge.s32	%p1, %r1, %r155;
	@%p1 bra 	BB0_314;

	mul.lo.s32 	%r4, %r1, 7;
	mul.lo.s32 	%r5, %r1, %r3;
	setp.eq.s32	%p2, %r3, 0;
	mov.f64 	%fd1712, 0d0000000000000000;
	mov.f64 	%fd1709, %fd1712;
	mov.f64 	%fd1706, %fd1712;
	mov.u32 	%r452, 0;
	mov.f64 	%fd1711, %fd1712;
	mov.f64 	%fd1708, %fd1712;
	mov.f64 	%fd1705, %fd1712;
	@%p2 bra 	BB0_3;

BB0_2:
	add.s32 	%r157, %r452, %r5;
	mul.wide.s32 	%rd18, %r157, 4;
	add.s64 	%rd19, %rd1, %rd18;
	ldu.global.u32 	%r158, [%rd19];
	cvt.rn.f64.s32	%fd549, %r158;
	add.f64 	%fd1706, %fd1706, %fd549;
	rem.s32 	%r159, %r452, %r2;
	mul.lo.s32 	%r160, %r159, %r158;
	cvt.rn.f64.s32	%fd550, %r160;
	add.f64 	%fd1709, %fd1709, %fd550;
	div.s32 	%r161, %r452, %r2;
	mul.lo.s32 	%r162, %r158, %r161;
	cvt.rn.f64.s32	%fd551, %r162;
	add.f64 	%fd1712, %fd1712, %fd551;
	add.s32 	%r452, %r452, 1;
	setp.lt.s32	%p3, %r452, %r3;
	mov.f64 	%fd1705, %fd1706;
	mov.f64 	%fd1708, %fd1709;
	mov.f64 	%fd1711, %fd1712;
	@%p3 bra 	BB0_2;

BB0_3:
	cvta.to.global.u64 	%rd20, %rd15;
	mul.wide.s32 	%rd21, %r4, 8;
	add.s64 	%rd2, %rd20, %rd21;
	div.rn.f64 	%fd554, %fd1708, %fd1705;
	st.global.f64 	[%rd2+8], %fd554;
	div.rn.f64 	%fd555, %fd1711, %fd1705;
	st.global.f64 	[%rd2+16], %fd555;
	cvt.rn.f64.s32	%fd556, %r3;
	div.rn.f64 	%fd10, %fd1705, %fd556;
	mov.f64 	%fd1715, 0d0000000000000000;
	mov.u32 	%r453, 0;
	mov.f64 	%fd1714, %fd1715;
	@%p2 bra 	BB0_5;

BB0_4:
	add.s32 	%r164, %r453, %r5;
	mul.wide.s32 	%rd22, %r164, 4;
	add.s64 	%rd23, %rd1, %rd22;
	ld.global.u32 	%r165, [%rd23];
	cvt.rn.f64.s32	%fd557, %r165;
	sub.f64 	%fd558, %fd557, %fd10;
	fma.rn.f64 	%fd1715, %fd558, %fd558, %fd1715;
	add.s32 	%r453, %r453, 1;
	setp.lt.s32	%p5, %r453, %r3;
	mov.f64 	%fd1714, %fd1715;
	@%p5 bra 	BB0_4;

BB0_5:
	cvta.to.global.u64 	%rd24, %rd17;
	cvta.to.global.u64 	%rd25, %rd16;
	ld.global.f64 	%fd559, [%rd25];
	ld.global.f64 	%fd560, [%rd2];
	mul.f64 	%fd1722, %fd560, %fd559;
	ld.global.f64 	%fd561, [%rd25+8];
	mul.f64 	%fd1721, %fd560, %fd561;
	ld.global.f64 	%fd562, [%rd25+96];
	mul.f64 	%fd16, %fd560, %fd562;
	ld.global.f64 	%fd563, [%rd25+104];
	mul.f64 	%fd17, %fd560, %fd563;
	add.s64 	%rd3, %rd24, %rd21;
	ld.global.f64 	%fd564, [%rd3];
	mul.f64 	%fd565, %fd560, %fd564;
	ld.global.f64 	%fd566, [%rd3+48];
	ld.global.f64 	%fd18, [%rd3+24];
	st.global.f64 	[%rd3], %fd565;
	ld.global.f64 	%fd567, [%rd2];
	mul.f64 	%fd568, %fd567, %fd566;
	st.global.f64 	[%rd3+48], %fd568;
	ld.global.f64 	%fd569, [%rd2+24];
	fma.rn.f64 	%fd1716, %fd18, 0dC008000000000000, %fd569;
	fma.rn.f64 	%fd20, %fd18, 0d4000000000000000, %fd569;
	setp.gtu.f64	%p6, %fd1716, %fd20;
	@%p6 bra 	BB0_16;

	ld.global.f64 	%fd21, [%rd3+32];
	ld.global.f64 	%fd571, [%rd2+32];
	fma.rn.f64 	%fd22, %fd21, 0dC008000000000000, %fd571;
	fma.rn.f64 	%fd23, %fd21, 0d4000000000000000, %fd571;
	mov.f64 	%fd1718, 0d3FF0000000000000;

BB0_7:
	add.f64 	%fd572, %fd1716, %fd1716;
	mul.f64 	%fd573, %fd1716, %fd572;
	rcp.rn.f64 	%fd28, %fd573;
	setp.gtu.f64	%p7, %fd22, %fd23;
	mov.f64 	%fd1717, %fd22;
	@%p7 bra 	BB0_15;

BB0_8:
	mov.f64 	%fd29, %fd1717;
	mov.f64 	%fd1720, 0d0000000000000000;
	@%p2 bra 	BB0_14;

	add.f64 	%fd576, %fd29, %fd29;
	mul.f64 	%fd577, %fd29, %fd576;
	rcp.rn.f64 	%fd33, %fd577;
	ld.global.f64 	%fd34, [%rd2];
	ld.global.f64 	%fd35, [%rd2+8];
	ld.global.f64 	%fd36, [%rd2+16];
	mov.f64 	%fd1720, 0d0000000000000000;
	mov.u32 	%r454, 0;

BB0_10:
	rem.s32 	%r169, %r454, %r2;
	div.s32 	%r170, %r454, %r2;
	cvt.rn.f64.s32	%fd578, %r169;
	sub.f64 	%fd579, %fd578, %fd35;
	mul.f64 	%fd580, %fd28, %fd579;
	mul.f64 	%fd581, %fd579, 0d8000000000000000;
	cvt.rn.f64.s32	%fd582, %r170;
	sub.f64 	%fd583, %fd582, %fd36;
	mul.f64 	%fd584, %fd581, %fd583;
	fma.rn.f64 	%fd585, %fd579, %fd580, %fd584;
	mul.f64 	%fd586, %fd33, %fd583;
	fma.rn.f64 	%fd38, %fd583, %fd586, %fd585;
	neg.f64 	%fd587, %fd38;
	mov.f64 	%fd588, 0d4338000000000000;
	mov.f64 	%fd589, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd590, %fd587, %fd589, %fd588;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r11, %temp}, %fd590;
	}
	mov.f64 	%fd591, 0dC338000000000000;
	add.rn.f64 	%fd592, %fd590, %fd591;
	mov.f64 	%fd593, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd594, %fd592, %fd593, %fd587;
	mov.f64 	%fd595, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd596, %fd592, %fd595, %fd594;
	mov.f64 	%fd597, 0d3E928AF3FCA213EA;
	mov.f64 	%fd598, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd599, %fd598, %fd596, %fd597;
	mov.f64 	%fd600, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd601, %fd599, %fd596, %fd600;
	mov.f64 	%fd602, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd603, %fd601, %fd596, %fd602;
	mov.f64 	%fd604, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd605, %fd603, %fd596, %fd604;
	mov.f64 	%fd606, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd607, %fd605, %fd596, %fd606;
	mov.f64 	%fd608, 0d3F81111111122322;
	fma.rn.f64 	%fd609, %fd607, %fd596, %fd608;
	mov.f64 	%fd610, 0d3FA55555555502A1;
	fma.rn.f64 	%fd611, %fd609, %fd596, %fd610;
	mov.f64 	%fd612, 0d3FC5555555555511;
	fma.rn.f64 	%fd613, %fd611, %fd596, %fd612;
	mov.f64 	%fd614, 0d3FE000000000000B;
	fma.rn.f64 	%fd615, %fd613, %fd596, %fd614;
	mov.f64 	%fd616, 0d3FF0000000000000;
	fma.rn.f64 	%fd617, %fd615, %fd596, %fd616;
	fma.rn.f64 	%fd618, %fd617, %fd596, %fd616;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd618;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd618;
	}
	shl.b32 	%r171, %r11, 20;
	add.s32 	%r172, %r13, %r171;
	mov.b64 	%fd1719, {%r12, %r172};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r173}, %fd587;
	}
	mov.b32 	 %f6, %r173;
	abs.f32 	%f1, %f6;
	setp.lt.f32	%p9, %f1, 0f4086232B;
	@%p9 bra 	BB0_13;

	setp.gt.f64	%p10, %fd38, 0d8000000000000000;
	mov.f64 	%fd619, 0d7FF0000000000000;
	sub.f64 	%fd620, %fd619, %fd38;
	selp.f64	%fd1719, 0d0000000000000000, %fd620, %p10;
	setp.geu.f32	%p11, %f1, 0f40874800;
	@%p11 bra 	BB0_13;

	shr.u32 	%r174, %r11, 31;
	add.s32 	%r175, %r11, %r174;
	shr.s32 	%r176, %r175, 1;
	shl.b32 	%r177, %r176, 20;
	add.s32 	%r178, %r177, %r13;
	mov.b64 	%fd621, {%r12, %r178};
	sub.s32 	%r179, %r11, %r176;
	shl.b32 	%r180, %r179, 20;
	add.s32 	%r181, %r180, 1072693248;
	mov.u32 	%r182, 0;
	mov.b64 	%fd622, {%r182, %r181};
	mul.f64 	%fd1719, %fd621, %fd622;

BB0_13:
	add.s32 	%r183, %r454, %r5;
	mul.wide.s32 	%rd28, %r183, 4;
	add.s64 	%rd29, %rd1, %rd28;
	ld.global.u32 	%r184, [%rd29];
	cvt.rn.f64.s32	%fd623, %r184;
	mul.f64 	%fd624, %fd34, %fd1719;
	sub.f64 	%fd625, %fd624, %fd623;
	fma.rn.f64 	%fd1720, %fd625, %fd625, %fd1720;
	add.s32 	%r454, %r454, 1;
	setp.lt.s32	%p12, %r454, %r3;
	@%p12 bra 	BB0_10;

BB0_14:
	div.rn.f64 	%fd626, %fd1720, %fd1714;
	setp.lt.f64	%p13, %fd626, %fd1718;
	selp.f64	%fd1718, %fd626, %fd1718, %p13;
	selp.f64	%fd1722, %fd1716, %fd1722, %p13;
	selp.f64	%fd1721, %fd29, %fd1721, %p13;
	add.f64 	%fd48, %fd29, %fd21;
	setp.le.f64	%p14, %fd48, %fd23;
	mov.f64 	%fd1717, %fd48;
	@%p14 bra 	BB0_8;

BB0_15:
	add.f64 	%fd1716, %fd1716, %fd18;
	setp.le.f64	%p15, %fd1716, %fd20;
	@%p15 bra 	BB0_7;

BB0_16:
	st.global.f64 	[%rd2+24], %fd1722;
	st.global.f64 	[%rd2+32], %fd1721;
	ld.global.f64 	%fd630, [%rd25];
	ld.global.f64 	%fd631, [%rd2];
	mul.f64 	%fd55, %fd631, %fd630;
	ld.global.f64 	%fd632, [%rd25+8];
	mul.f64 	%fd56, %fd631, %fd632;
	add.f64 	%fd633, %fd1722, %fd1722;
	mul.f64 	%fd634, %fd1722, %fd633;
	rcp.rn.f64 	%fd1725, %fd634;
	add.f64 	%fd635, %fd1721, %fd1721;
	mul.f64 	%fd636, %fd1721, %fd635;
	rcp.rn.f64 	%fd1723, %fd636;
	mov.u16 	%rs8, 1;
	mov.u32 	%r459, 1;
	mov.f64 	%fd1757, 0d3FF0000000000000;
	mov.f64 	%fd1755, %fd1757;
	mov.u32 	%r457, 0;
	mov.f64 	%fd1724, 0d0000000000000000;
	bra.uni 	BB0_17;

BB0_196:
	neg.f64 	%fd1253, %fd353;
	st.global.f64 	[%rd3+48], %fd1253;
	mov.f64 	%fd1731, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1731;
	bra.uni 	BB0_223;

BB0_206:
	neg.f64 	%fd1308, %fd368;
	st.global.f64 	[%rd3+48], %fd1308;
	mov.f64 	%fd1734, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1734;
	bra.uni 	BB0_223;

BB0_192:
	neg.f64 	%fd1250, %fd352;
	st.global.f64 	[%rd5], %fd1250;
	mov.f64 	%fd1729, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1729;
	bra.uni 	BB0_223;

BB0_17:
	mov.f64 	%fd63, %fd1757;
	mov.f64 	%fd62, %fd1755;
	setp.eq.s32	%p16, %r457, 0;
	@%p16 bra 	BB0_208;
	bra.uni 	BB0_18;

BB0_208:
	ld.global.f64 	%fd369, [%rd3];
	ld.global.f64 	%fd1310, [%rd2];
	add.f64 	%fd370, %fd1310, %fd369;
	setp.gt.f64	%p139, %fd370, %fd55;
	setp.lt.f64	%p140, %fd370, %fd56;
	and.pred  	%p141, %p139, %p140;
	@%p141 bra 	BB0_212;
	bra.uni 	BB0_209;

BB0_212:
	st.global.f64 	[%rd2], %fd370;
	mov.f64 	%fd1824, 0d0000000000000000;
	@%p2 bra 	BB0_218;

	ld.global.f64 	%fd371, [%rd2+8];
	add.f64 	%fd372, %fd1724, %fd1724;
	ld.global.f64 	%fd373, [%rd2+16];
	ld.global.f64 	%fd374, [%rd2+48];
	mov.f64 	%fd1824, 0d0000000000000000;
	mov.u32 	%r482, 0;

BB0_214:
	rem.s32 	%r349, %r482, %r2;
	cvt.rn.f64.s32	%fd1315, %r349;
	sub.f64 	%fd1316, %fd1315, %fd371;
	mul.f64 	%fd1317, %fd1725, %fd1316;
	mul.f64 	%fd1318, %fd1316, %fd1317;
	mul.f64 	%fd1319, %fd372, %fd1316;
	div.s32 	%r350, %r482, %r2;
	cvt.rn.f64.s32	%fd1320, %r350;
	sub.f64 	%fd1321, %fd1320, %fd373;
	mul.f64 	%fd1322, %fd1319, %fd1321;
	sub.f64 	%fd1323, %fd1318, %fd1322;
	mul.f64 	%fd1324, %fd1723, %fd1321;
	fma.rn.f64 	%fd376, %fd1321, %fd1324, %fd1323;
	neg.f64 	%fd1325, %fd376;
	mov.f64 	%fd1326, 0d4338000000000000;
	mov.f64 	%fd1327, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1328, %fd1325, %fd1327, %fd1326;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r102, %temp}, %fd1328;
	}
	mov.f64 	%fd1329, 0dC338000000000000;
	add.rn.f64 	%fd1330, %fd1328, %fd1329;
	mov.f64 	%fd1331, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1332, %fd1330, %fd1331, %fd1325;
	mov.f64 	%fd1333, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1334, %fd1330, %fd1333, %fd1332;
	mov.f64 	%fd1335, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1336, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1337, %fd1336, %fd1334, %fd1335;
	mov.f64 	%fd1338, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1339, %fd1337, %fd1334, %fd1338;
	mov.f64 	%fd1340, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1341, %fd1339, %fd1334, %fd1340;
	mov.f64 	%fd1342, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1343, %fd1341, %fd1334, %fd1342;
	mov.f64 	%fd1344, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1345, %fd1343, %fd1334, %fd1344;
	mov.f64 	%fd1346, 0d3F81111111122322;
	fma.rn.f64 	%fd1347, %fd1345, %fd1334, %fd1346;
	mov.f64 	%fd1348, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1349, %fd1347, %fd1334, %fd1348;
	mov.f64 	%fd1350, 0d3FC5555555555511;
	fma.rn.f64 	%fd1351, %fd1349, %fd1334, %fd1350;
	mov.f64 	%fd1352, 0d3FE000000000000B;
	fma.rn.f64 	%fd1353, %fd1351, %fd1334, %fd1352;
	mov.f64 	%fd1354, 0d3FF0000000000000;
	fma.rn.f64 	%fd1355, %fd1353, %fd1334, %fd1354;
	fma.rn.f64 	%fd1356, %fd1355, %fd1334, %fd1354;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r103, %temp}, %fd1356;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r104}, %fd1356;
	}
	shl.b32 	%r351, %r102, 20;
	add.s32 	%r352, %r104, %r351;
	mov.b64 	%fd1823, {%r103, %r352};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r353}, %fd1325;
	}
	mov.b32 	 %f9, %r353;
	abs.f32 	%f4, %f9;
	setp.lt.f32	%p144, %f4, 0f4086232B;
	@%p144 bra 	BB0_217;

	setp.gt.f64	%p145, %fd376, 0d8000000000000000;
	mov.f64 	%fd1357, 0d7FF0000000000000;
	sub.f64 	%fd1358, %fd1357, %fd376;
	selp.f64	%fd1823, 0d0000000000000000, %fd1358, %p145;
	setp.geu.f32	%p146, %f4, 0f40874800;
	@%p146 bra 	BB0_217;

	shr.u32 	%r354, %r102, 31;
	add.s32 	%r355, %r102, %r354;
	shr.s32 	%r356, %r355, 1;
	shl.b32 	%r357, %r356, 20;
	add.s32 	%r358, %r357, %r104;
	mov.b64 	%fd1359, {%r103, %r358};
	sub.s32 	%r359, %r102, %r356;
	shl.b32 	%r360, %r359, 20;
	add.s32 	%r361, %r360, 1072693248;
	mov.u32 	%r362, 0;
	mov.b64 	%fd1360, {%r362, %r361};
	mul.f64 	%fd1823, %fd1359, %fd1360;

BB0_217:
	fma.rn.f64 	%fd1361, %fd370, %fd1823, %fd374;
	add.s32 	%r363, %r482, %r5;
	mul.wide.s32 	%rd192, %r363, 4;
	add.s64 	%rd193, %rd1, %rd192;
	ld.global.u32 	%r364, [%rd193];
	cvt.rn.f64.s32	%fd1362, %r364;
	sub.f64 	%fd1363, %fd1361, %fd1362;
	fma.rn.f64 	%fd1824, %fd1363, %fd1363, %fd1824;
	add.s32 	%r482, %r482, 1;
	setp.lt.s32	%p147, %r482, %r3;
	@%p147 bra 	BB0_214;

BB0_218:
	div.rn.f64 	%fd1758, %fd1824, %fd1714;
	setp.lt.f64	%p148, %fd1758, %fd63;
	mov.f64 	%fd1756, %fd63;
	@%p148 bra 	BB0_223;

	ld.global.f64 	%fd1364, [%rd3];
	sub.f64 	%fd1365, %fd370, %fd1364;
	st.global.f64 	[%rd2], %fd1365;
	ld.global.f64 	%fd384, [%rd3];
	setp.lt.f64	%p149, %fd384, 0d0000000000000000;
	@%p149 bra 	BB0_221;
	bra.uni 	BB0_220;

BB0_221:
	mul.f64 	%fd1367, %fd384, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3], %fd1367;
	bra.uni 	BB0_222;

BB0_18:
	setp.eq.s32	%p17, %r457, 6;
	@%p17 bra 	BB0_194;
	bra.uni 	BB0_19;

BB0_194:
	ld.global.f64 	%fd353, [%rd3+48];
	ld.global.f64 	%fd1252, [%rd2+48];
	add.f64 	%fd354, %fd1252, %fd353;
	setp.gt.f64	%p128, %fd354, %fd16;
	setp.lt.f64	%p129, %fd354, %fd17;
	and.pred  	%p130, %p128, %p129;
	@%p130 bra 	BB0_198;
	bra.uni 	BB0_195;

BB0_198:
	st.global.f64 	[%rd2+48], %fd354;
	mov.f64 	%fd1822, 0d0000000000000000;
	@%p2 bra 	BB0_204;

	ld.global.f64 	%fd355, [%rd2];
	ld.global.f64 	%fd356, [%rd2+8];
	add.f64 	%fd357, %fd1724, %fd1724;
	ld.global.f64 	%fd358, [%rd2+16];
	mov.f64 	%fd1822, 0d0000000000000000;
	mov.u32 	%r481, 0;

BB0_200:
	rem.s32 	%r322, %r481, %r2;
	cvt.rn.f64.s32	%fd1257, %r322;
	sub.f64 	%fd1258, %fd1257, %fd356;
	mul.f64 	%fd1259, %fd1725, %fd1258;
	mul.f64 	%fd1260, %fd1258, %fd1259;
	mul.f64 	%fd1261, %fd357, %fd1258;
	div.s32 	%r323, %r481, %r2;
	cvt.rn.f64.s32	%fd1262, %r323;
	sub.f64 	%fd1263, %fd1262, %fd358;
	mul.f64 	%fd1264, %fd1261, %fd1263;
	sub.f64 	%fd1265, %fd1260, %fd1264;
	mul.f64 	%fd1266, %fd1723, %fd1263;
	fma.rn.f64 	%fd360, %fd1263, %fd1266, %fd1265;
	neg.f64 	%fd1267, %fd360;
	mov.f64 	%fd1268, 0d4338000000000000;
	mov.f64 	%fd1269, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1270, %fd1267, %fd1269, %fd1268;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r95, %temp}, %fd1270;
	}
	mov.f64 	%fd1271, 0dC338000000000000;
	add.rn.f64 	%fd1272, %fd1270, %fd1271;
	mov.f64 	%fd1273, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1274, %fd1272, %fd1273, %fd1267;
	mov.f64 	%fd1275, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1276, %fd1272, %fd1275, %fd1274;
	mov.f64 	%fd1277, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1278, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1279, %fd1278, %fd1276, %fd1277;
	mov.f64 	%fd1280, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1281, %fd1279, %fd1276, %fd1280;
	mov.f64 	%fd1282, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1283, %fd1281, %fd1276, %fd1282;
	mov.f64 	%fd1284, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1285, %fd1283, %fd1276, %fd1284;
	mov.f64 	%fd1286, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1287, %fd1285, %fd1276, %fd1286;
	mov.f64 	%fd1288, 0d3F81111111122322;
	fma.rn.f64 	%fd1289, %fd1287, %fd1276, %fd1288;
	mov.f64 	%fd1290, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1291, %fd1289, %fd1276, %fd1290;
	mov.f64 	%fd1292, 0d3FC5555555555511;
	fma.rn.f64 	%fd1293, %fd1291, %fd1276, %fd1292;
	mov.f64 	%fd1294, 0d3FE000000000000B;
	fma.rn.f64 	%fd1295, %fd1293, %fd1276, %fd1294;
	mov.f64 	%fd1296, 0d3FF0000000000000;
	fma.rn.f64 	%fd1297, %fd1295, %fd1276, %fd1296;
	fma.rn.f64 	%fd1298, %fd1297, %fd1276, %fd1296;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r96, %temp}, %fd1298;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r97}, %fd1298;
	}
	shl.b32 	%r324, %r95, 20;
	add.s32 	%r325, %r97, %r324;
	mov.b64 	%fd1821, {%r96, %r325};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r326}, %fd1267;
	}
	mov.b32 	 %f8, %r326;
	abs.f32 	%f3, %f8;
	setp.lt.f32	%p133, %f3, 0f4086232B;
	@%p133 bra 	BB0_203;

	setp.gt.f64	%p134, %fd360, 0d8000000000000000;
	mov.f64 	%fd1299, 0d7FF0000000000000;
	sub.f64 	%fd1300, %fd1299, %fd360;
	selp.f64	%fd1821, 0d0000000000000000, %fd1300, %p134;
	setp.geu.f32	%p135, %f3, 0f40874800;
	@%p135 bra 	BB0_203;

	shr.u32 	%r327, %r95, 31;
	add.s32 	%r328, %r95, %r327;
	shr.s32 	%r329, %r328, 1;
	shl.b32 	%r330, %r329, 20;
	add.s32 	%r331, %r330, %r97;
	mov.b64 	%fd1301, {%r96, %r331};
	sub.s32 	%r332, %r95, %r329;
	shl.b32 	%r333, %r332, 20;
	add.s32 	%r334, %r333, 1072693248;
	mov.u32 	%r335, 0;
	mov.b64 	%fd1302, {%r335, %r334};
	mul.f64 	%fd1821, %fd1301, %fd1302;

BB0_203:
	fma.rn.f64 	%fd1303, %fd355, %fd1821, %fd354;
	add.s32 	%r336, %r481, %r5;
	mul.wide.s32 	%rd184, %r336, 4;
	add.s64 	%rd185, %rd1, %rd184;
	ld.global.u32 	%r337, [%rd185];
	cvt.rn.f64.s32	%fd1304, %r337;
	sub.f64 	%fd1305, %fd1303, %fd1304;
	fma.rn.f64 	%fd1822, %fd1305, %fd1305, %fd1822;
	add.s32 	%r481, %r481, 1;
	setp.lt.s32	%p136, %r481, %r3;
	@%p136 bra 	BB0_200;

BB0_204:
	div.rn.f64 	%fd1758, %fd1822, %fd1714;
	setp.lt.f64	%p137, %fd1758, %fd63;
	mov.f64 	%fd1733, %fd62;
	mov.f64 	%fd1756, %fd1733;
	@%p137 bra 	BB0_223;

	ld.global.f64 	%fd1306, [%rd3+48];
	sub.f64 	%fd1307, %fd354, %fd1306;
	st.global.f64 	[%rd2+48], %fd1307;
	ld.global.f64 	%fd368, [%rd3+48];
	setp.lt.f64	%p138, %fd368, 0d0000000000000000;
	@%p138 bra 	BB0_207;
	bra.uni 	BB0_206;

BB0_207:
	mul.f64 	%fd1309, %fd368, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3+48], %fd1309;
	mov.f64 	%fd1735, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1735;
	bra.uni 	BB0_223;

BB0_209:
	setp.lt.f64	%p142, %fd369, 0d0000000000000000;
	@%p142 bra 	BB0_211;
	bra.uni 	BB0_210;

BB0_211:
	mul.f64 	%fd1312, %fd369, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3], %fd1312;
	bra.uni 	BB0_222;

BB0_19:
	mad.lo.s32 	%r192, %r1, 7, %r457;
	mul.wide.s32 	%rd32, %r192, 8;
	add.s64 	%rd4, %rd20, %rd32;
	add.s64 	%rd5, %rd24, %rd32;
	ld.global.f64 	%fd64, [%rd5];
	ld.global.f64 	%fd637, [%rd4];
	add.f64 	%fd65, %fd637, %fd64;
	shl.b32 	%r193, %r457, 1;
	mul.wide.s32 	%rd35, %r193, 8;
	add.s64 	%rd6, %rd25, %rd35;
	ld.global.f64 	%fd638, [%rd6];
	setp.leu.f64	%p18, %fd65, %fd638;
	@%p18 bra 	BB0_21;

	ld.global.f64 	%fd639, [%rd6+8];
	setp.lt.f64	%p19, %fd65, %fd639;
	@%p19 bra 	BB0_24;
	bra.uni 	BB0_21;

BB0_24:
	st.global.f64 	[%rd4], %fd65;
	ld.global.f64 	%fd1759, [%rd2+40];
	abs.f64 	%fd642, %fd1759;
	setp.neu.f64	%p21, %fd642, 0d7FF0000000000000;
	@%p21 bra 	BB0_26;

	mov.f64 	%fd643, 0d0000000000000000;
	mul.rn.f64 	%fd1759, %fd1759, %fd643;

BB0_26:
	mul.f64 	%fd644, %fd1759, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r460, %fd644;
	add.u64 	%rd36, %SP, 0;
	cvta.to.local.u64 	%rd37, %rd36;
	st.local.u32 	[%rd37], %r460;
	cvt.rn.f64.s32	%fd645, %r460;
	neg.f64 	%fd646, %fd645;
	mov.f64 	%fd647, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd648, %fd646, %fd647, %fd1759;
	mov.f64 	%fd649, 0d3C91A62633145C00;
	fma.rn.f64 	%fd650, %fd646, %fd649, %fd648;
	mov.f64 	%fd651, 0d397B839A252049C0;
	fma.rn.f64 	%fd1760, %fd646, %fd651, %fd650;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r194}, %fd1759;
	}
	and.b32  	%r195, %r194, 2145386496;
	setp.lt.u32	%p22, %r195, 1105199104;
	@%p22 bra 	BB0_28;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1759;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1760, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r460, [%rd37];

BB0_28:
	add.s32 	%r20, %r460, 1;
	and.b32  	%r196, %r20, 1;
	shl.b32 	%r197, %r196, 3;
	setp.eq.b32	%p23, %r196, 1;
	selp.f64	%fd652, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p23;
	mul.wide.u32 	%rd40, %r197, 8;
	mov.u64 	%rd41, __cudart_sin_cos_coeffs;
	add.s64 	%rd42, %rd40, %rd41;
	ld.const.f64 	%fd653, [%rd42+8];
	mul.rn.f64 	%fd72, %fd1760, %fd1760;
	fma.rn.f64 	%fd654, %fd652, %fd72, %fd653;
	ld.const.f64 	%fd655, [%rd42+16];
	fma.rn.f64 	%fd656, %fd654, %fd72, %fd655;
	ld.const.f64 	%fd657, [%rd42+24];
	fma.rn.f64 	%fd658, %fd656, %fd72, %fd657;
	ld.const.f64 	%fd659, [%rd42+32];
	fma.rn.f64 	%fd660, %fd658, %fd72, %fd659;
	ld.const.f64 	%fd661, [%rd42+40];
	fma.rn.f64 	%fd662, %fd660, %fd72, %fd661;
	ld.const.f64 	%fd663, [%rd42+48];
	fma.rn.f64 	%fd73, %fd662, %fd72, %fd663;
	fma.rn.f64 	%fd1761, %fd73, %fd1760, %fd1760;
	setp.eq.s32	%p24, %r196, 0;
	@%p24 bra 	BB0_30;

	mov.f64 	%fd664, 0d3FF0000000000000;
	fma.rn.f64 	%fd1761, %fd73, %fd72, %fd664;

BB0_30:
	and.b32  	%r198, %r20, 2;
	setp.eq.s32	%p25, %r198, 0;
	@%p25 bra 	BB0_32;

	mov.f64 	%fd665, 0d0000000000000000;
	mov.f64 	%fd666, 0dBFF0000000000000;
	fma.rn.f64 	%fd1761, %fd1761, %fd666, %fd665;

BB0_32:
	ld.global.f64 	%fd1762, [%rd2+40];
	abs.f64 	%fd667, %fd1762;
	setp.neu.f64	%p26, %fd667, 0d7FF0000000000000;
	@%p26 bra 	BB0_34;

	mov.f64 	%fd668, 0d0000000000000000;
	mul.rn.f64 	%fd1762, %fd1762, %fd668;

BB0_34:
	mul.f64 	%fd669, %fd1762, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r461, %fd669;
	st.local.u32 	[%rd37], %r461;
	cvt.rn.f64.s32	%fd670, %r461;
	neg.f64 	%fd671, %fd670;
	fma.rn.f64 	%fd673, %fd671, %fd647, %fd1762;
	fma.rn.f64 	%fd675, %fd671, %fd649, %fd673;
	fma.rn.f64 	%fd1763, %fd671, %fd651, %fd675;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r199}, %fd1762;
	}
	and.b32  	%r200, %r199, 2145386496;
	setp.lt.u32	%p27, %r200, 1105199104;
	@%p27 bra 	BB0_36;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1762;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1763, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r461, [%rd37];

BB0_36:
	add.s32 	%r24, %r461, 1;
	and.b32  	%r201, %r24, 1;
	shl.b32 	%r202, %r201, 3;
	setp.eq.b32	%p28, %r201, 1;
	selp.f64	%fd677, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p28;
	mul.wide.u32 	%rd47, %r202, 8;
	add.s64 	%rd49, %rd47, %rd41;
	ld.const.f64 	%fd678, [%rd49+8];
	mul.rn.f64 	%fd85, %fd1763, %fd1763;
	fma.rn.f64 	%fd679, %fd677, %fd85, %fd678;
	ld.const.f64 	%fd680, [%rd49+16];
	fma.rn.f64 	%fd681, %fd679, %fd85, %fd680;
	ld.const.f64 	%fd682, [%rd49+24];
	fma.rn.f64 	%fd683, %fd681, %fd85, %fd682;
	ld.const.f64 	%fd684, [%rd49+32];
	fma.rn.f64 	%fd685, %fd683, %fd85, %fd684;
	ld.const.f64 	%fd686, [%rd49+40];
	fma.rn.f64 	%fd687, %fd685, %fd85, %fd686;
	ld.const.f64 	%fd688, [%rd49+48];
	fma.rn.f64 	%fd86, %fd687, %fd85, %fd688;
	fma.rn.f64 	%fd1764, %fd86, %fd1763, %fd1763;
	setp.eq.s32	%p29, %r201, 0;
	@%p29 bra 	BB0_38;

	mov.f64 	%fd689, 0d3FF0000000000000;
	fma.rn.f64 	%fd1764, %fd86, %fd85, %fd689;

BB0_38:
	and.b32  	%r203, %r24, 2;
	setp.eq.s32	%p30, %r203, 0;
	@%p30 bra 	BB0_40;

	mov.f64 	%fd690, 0d0000000000000000;
	mov.f64 	%fd691, 0dBFF0000000000000;
	fma.rn.f64 	%fd1764, %fd1764, %fd691, %fd690;

BB0_40:
	ld.global.f64 	%fd692, [%rd2+24];
	add.f64 	%fd693, %fd692, %fd692;
	mul.f64 	%fd694, %fd692, %fd693;
	mul.f64 	%fd695, %fd1761, %fd1764;
	div.rn.f64 	%fd92, %fd695, %fd694;
	ld.global.f64 	%fd1765, [%rd2+40];
	abs.f64 	%fd696, %fd1765;
	setp.neu.f64	%p31, %fd696, 0d7FF0000000000000;
	@%p31 bra 	BB0_42;

	mov.f64 	%fd697, 0d0000000000000000;
	mul.rn.f64 	%fd1765, %fd1765, %fd697;

BB0_42:
	mul.f64 	%fd698, %fd1765, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r462, %fd698;
	st.local.u32 	[%rd37], %r462;
	cvt.rn.f64.s32	%fd699, %r462;
	neg.f64 	%fd700, %fd699;
	fma.rn.f64 	%fd702, %fd700, %fd647, %fd1765;
	fma.rn.f64 	%fd704, %fd700, %fd649, %fd702;
	fma.rn.f64 	%fd1766, %fd700, %fd651, %fd704;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r204}, %fd1765;
	}
	and.b32  	%r205, %r204, 2145386496;
	setp.lt.u32	%p32, %r205, 1105199104;
	@%p32 bra 	BB0_44;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1765;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1766, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r462, [%rd37];

BB0_44:
	and.b32  	%r206, %r462, 1;
	shl.b32 	%r207, %r206, 3;
	setp.eq.b32	%p33, %r206, 1;
	selp.f64	%fd706, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p33;
	mul.wide.u32 	%rd54, %r207, 8;
	add.s64 	%rd56, %rd54, %rd41;
	ld.const.f64 	%fd707, [%rd56+8];
	mul.rn.f64 	%fd99, %fd1766, %fd1766;
	fma.rn.f64 	%fd708, %fd706, %fd99, %fd707;
	ld.const.f64 	%fd709, [%rd56+16];
	fma.rn.f64 	%fd710, %fd708, %fd99, %fd709;
	ld.const.f64 	%fd711, [%rd56+24];
	fma.rn.f64 	%fd712, %fd710, %fd99, %fd711;
	ld.const.f64 	%fd713, [%rd56+32];
	fma.rn.f64 	%fd714, %fd712, %fd99, %fd713;
	ld.const.f64 	%fd715, [%rd56+40];
	fma.rn.f64 	%fd716, %fd714, %fd99, %fd715;
	ld.const.f64 	%fd717, [%rd56+48];
	fma.rn.f64 	%fd100, %fd716, %fd99, %fd717;
	fma.rn.f64 	%fd1767, %fd100, %fd1766, %fd1766;
	setp.eq.s32	%p34, %r206, 0;
	@%p34 bra 	BB0_46;

	mov.f64 	%fd718, 0d3FF0000000000000;
	fma.rn.f64 	%fd1767, %fd100, %fd99, %fd718;

BB0_46:
	and.b32  	%r208, %r462, 2;
	setp.eq.s32	%p35, %r208, 0;
	@%p35 bra 	BB0_48;

	mov.f64 	%fd719, 0d0000000000000000;
	mov.f64 	%fd720, 0dBFF0000000000000;
	fma.rn.f64 	%fd1767, %fd1767, %fd720, %fd719;

BB0_48:
	ld.global.f64 	%fd1768, [%rd2+40];
	abs.f64 	%fd721, %fd1768;
	setp.neu.f64	%p36, %fd721, 0d7FF0000000000000;
	@%p36 bra 	BB0_50;

	mov.f64 	%fd722, 0d0000000000000000;
	mul.rn.f64 	%fd1768, %fd1768, %fd722;

BB0_50:
	mul.f64 	%fd723, %fd1768, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r463, %fd723;
	st.local.u32 	[%rd37], %r463;
	cvt.rn.f64.s32	%fd724, %r463;
	neg.f64 	%fd725, %fd724;
	fma.rn.f64 	%fd727, %fd725, %fd647, %fd1768;
	fma.rn.f64 	%fd729, %fd725, %fd649, %fd727;
	fma.rn.f64 	%fd1769, %fd725, %fd651, %fd729;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r209}, %fd1768;
	}
	and.b32  	%r210, %r209, 2145386496;
	setp.lt.u32	%p37, %r210, 1105199104;
	@%p37 bra 	BB0_52;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1768;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1769, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r463, [%rd37];

BB0_52:
	and.b32  	%r211, %r463, 1;
	shl.b32 	%r212, %r211, 3;
	setp.eq.b32	%p38, %r211, 1;
	selp.f64	%fd731, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p38;
	mul.wide.u32 	%rd61, %r212, 8;
	add.s64 	%rd63, %rd61, %rd41;
	ld.const.f64 	%fd732, [%rd63+8];
	mul.rn.f64 	%fd112, %fd1769, %fd1769;
	fma.rn.f64 	%fd733, %fd731, %fd112, %fd732;
	ld.const.f64 	%fd734, [%rd63+16];
	fma.rn.f64 	%fd735, %fd733, %fd112, %fd734;
	ld.const.f64 	%fd736, [%rd63+24];
	fma.rn.f64 	%fd737, %fd735, %fd112, %fd736;
	ld.const.f64 	%fd738, [%rd63+32];
	fma.rn.f64 	%fd739, %fd737, %fd112, %fd738;
	ld.const.f64 	%fd740, [%rd63+40];
	fma.rn.f64 	%fd741, %fd739, %fd112, %fd740;
	ld.const.f64 	%fd742, [%rd63+48];
	fma.rn.f64 	%fd113, %fd741, %fd112, %fd742;
	fma.rn.f64 	%fd1770, %fd113, %fd1769, %fd1769;
	setp.eq.s32	%p39, %r211, 0;
	@%p39 bra 	BB0_54;

	mov.f64 	%fd743, 0d3FF0000000000000;
	fma.rn.f64 	%fd1770, %fd113, %fd112, %fd743;

BB0_54:
	and.b32  	%r213, %r463, 2;
	setp.eq.s32	%p40, %r213, 0;
	@%p40 bra 	BB0_56;

	mov.f64 	%fd744, 0d0000000000000000;
	mov.f64 	%fd745, 0dBFF0000000000000;
	fma.rn.f64 	%fd1770, %fd1770, %fd745, %fd744;

BB0_56:
	ld.global.f64 	%fd746, [%rd2+32];
	add.f64 	%fd747, %fd746, %fd746;
	mul.f64 	%fd748, %fd746, %fd747;
	mul.f64 	%fd749, %fd1767, %fd1770;
	div.rn.f64 	%fd750, %fd749, %fd748;
	add.f64 	%fd1725, %fd92, %fd750;
	ld.global.f64 	%fd751, [%rd2+40];
	add.f64 	%fd1771, %fd751, %fd751;
	abs.f64 	%fd752, %fd1771;
	setp.neu.f64	%p41, %fd752, 0d7FF0000000000000;
	@%p41 bra 	BB0_58;

	mov.f64 	%fd753, 0d0000000000000000;
	mul.rn.f64 	%fd1771, %fd1771, %fd753;

BB0_58:
	mul.f64 	%fd754, %fd1771, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r464, %fd754;
	st.local.u32 	[%rd37], %r464;
	cvt.rn.f64.s32	%fd755, %r464;
	neg.f64 	%fd756, %fd755;
	fma.rn.f64 	%fd758, %fd756, %fd647, %fd1771;
	fma.rn.f64 	%fd760, %fd756, %fd649, %fd758;
	fma.rn.f64 	%fd1772, %fd756, %fd651, %fd760;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r214}, %fd1771;
	}
	and.b32  	%r215, %r214, 2145386496;
	setp.lt.u32	%p42, %r215, 1105199104;
	@%p42 bra 	BB0_60;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1771;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1772, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r464, [%rd37];

BB0_60:
	and.b32  	%r216, %r464, 1;
	shl.b32 	%r217, %r216, 3;
	setp.eq.b32	%p43, %r216, 1;
	selp.f64	%fd762, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p43;
	mul.wide.u32 	%rd68, %r217, 8;
	add.s64 	%rd70, %rd68, %rd41;
	ld.const.f64 	%fd763, [%rd70+8];
	mul.rn.f64 	%fd126, %fd1772, %fd1772;
	fma.rn.f64 	%fd764, %fd762, %fd126, %fd763;
	ld.const.f64 	%fd765, [%rd70+16];
	fma.rn.f64 	%fd766, %fd764, %fd126, %fd765;
	ld.const.f64 	%fd767, [%rd70+24];
	fma.rn.f64 	%fd768, %fd766, %fd126, %fd767;
	ld.const.f64 	%fd769, [%rd70+32];
	fma.rn.f64 	%fd770, %fd768, %fd126, %fd769;
	ld.const.f64 	%fd771, [%rd70+40];
	fma.rn.f64 	%fd772, %fd770, %fd126, %fd771;
	ld.const.f64 	%fd773, [%rd70+48];
	fma.rn.f64 	%fd127, %fd772, %fd126, %fd773;
	fma.rn.f64 	%fd1773, %fd127, %fd1772, %fd1772;
	setp.eq.s32	%p44, %r216, 0;
	@%p44 bra 	BB0_62;

	mov.f64 	%fd774, 0d3FF0000000000000;
	fma.rn.f64 	%fd1773, %fd127, %fd126, %fd774;

BB0_62:
	and.b32  	%r218, %r464, 2;
	setp.eq.s32	%p45, %r218, 0;
	@%p45 bra 	BB0_64;

	mov.f64 	%fd775, 0d0000000000000000;
	mov.f64 	%fd776, 0dBFF0000000000000;
	fma.rn.f64 	%fd1773, %fd1773, %fd776, %fd775;

BB0_64:
	ld.global.f64 	%fd777, [%rd2+24];
	mul.f64 	%fd778, %fd777, 0dC010000000000000;
	mul.f64 	%fd779, %fd777, %fd778;
	div.rn.f64 	%fd133, %fd1773, %fd779;
	ld.global.f64 	%fd780, [%rd2+40];
	add.f64 	%fd1774, %fd780, %fd780;
	abs.f64 	%fd781, %fd1774;
	setp.neu.f64	%p46, %fd781, 0d7FF0000000000000;
	@%p46 bra 	BB0_66;

	mov.f64 	%fd782, 0d0000000000000000;
	mul.rn.f64 	%fd1774, %fd1774, %fd782;

BB0_66:
	mul.f64 	%fd783, %fd1774, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r465, %fd783;
	st.local.u32 	[%rd37], %r465;
	cvt.rn.f64.s32	%fd784, %r465;
	neg.f64 	%fd785, %fd784;
	fma.rn.f64 	%fd787, %fd785, %fd647, %fd1774;
	fma.rn.f64 	%fd789, %fd785, %fd649, %fd787;
	fma.rn.f64 	%fd1775, %fd785, %fd651, %fd789;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r219}, %fd1774;
	}
	and.b32  	%r220, %r219, 2145386496;
	setp.lt.u32	%p47, %r220, 1105199104;
	@%p47 bra 	BB0_68;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1774;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1775, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r465, [%rd37];

BB0_68:
	and.b32  	%r221, %r465, 1;
	shl.b32 	%r222, %r221, 3;
	setp.eq.b32	%p48, %r221, 1;
	selp.f64	%fd791, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p48;
	mul.wide.u32 	%rd75, %r222, 8;
	add.s64 	%rd77, %rd75, %rd41;
	ld.const.f64 	%fd792, [%rd77+8];
	mul.rn.f64 	%fd140, %fd1775, %fd1775;
	fma.rn.f64 	%fd793, %fd791, %fd140, %fd792;
	ld.const.f64 	%fd794, [%rd77+16];
	fma.rn.f64 	%fd795, %fd793, %fd140, %fd794;
	ld.const.f64 	%fd796, [%rd77+24];
	fma.rn.f64 	%fd797, %fd795, %fd140, %fd796;
	ld.const.f64 	%fd798, [%rd77+32];
	fma.rn.f64 	%fd799, %fd797, %fd140, %fd798;
	ld.const.f64 	%fd800, [%rd77+40];
	fma.rn.f64 	%fd801, %fd799, %fd140, %fd800;
	ld.const.f64 	%fd802, [%rd77+48];
	fma.rn.f64 	%fd141, %fd801, %fd140, %fd802;
	fma.rn.f64 	%fd1776, %fd141, %fd1775, %fd1775;
	setp.eq.s32	%p49, %r221, 0;
	@%p49 bra 	BB0_70;

	mov.f64 	%fd803, 0d3FF0000000000000;
	fma.rn.f64 	%fd1776, %fd141, %fd140, %fd803;

BB0_70:
	and.b32  	%r223, %r465, 2;
	setp.eq.s32	%p50, %r223, 0;
	@%p50 bra 	BB0_72;

	mov.f64 	%fd804, 0d0000000000000000;
	mov.f64 	%fd805, 0dBFF0000000000000;
	fma.rn.f64 	%fd1776, %fd1776, %fd805, %fd804;

BB0_72:
	ld.global.f64 	%fd806, [%rd2+32];
	mul.f64 	%fd807, %fd806, 0d4010000000000000;
	mul.f64 	%fd808, %fd806, %fd807;
	div.rn.f64 	%fd809, %fd1776, %fd808;
	add.f64 	%fd1724, %fd133, %fd809;
	ld.global.f64 	%fd1777, [%rd2+40];
	abs.f64 	%fd810, %fd1777;
	setp.neu.f64	%p51, %fd810, 0d7FF0000000000000;
	@%p51 bra 	BB0_74;

	mov.f64 	%fd811, 0d0000000000000000;
	mul.rn.f64 	%fd1777, %fd1777, %fd811;

BB0_74:
	mul.f64 	%fd812, %fd1777, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r466, %fd812;
	st.local.u32 	[%rd37], %r466;
	cvt.rn.f64.s32	%fd813, %r466;
	neg.f64 	%fd814, %fd813;
	fma.rn.f64 	%fd816, %fd814, %fd647, %fd1777;
	fma.rn.f64 	%fd818, %fd814, %fd649, %fd816;
	fma.rn.f64 	%fd1778, %fd814, %fd651, %fd818;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r224}, %fd1777;
	}
	and.b32  	%r225, %r224, 2145386496;
	setp.lt.u32	%p52, %r225, 1105199104;
	@%p52 bra 	BB0_76;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1777;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1778, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r466, [%rd37];

BB0_76:
	and.b32  	%r226, %r466, 1;
	shl.b32 	%r227, %r226, 3;
	setp.eq.b32	%p53, %r226, 1;
	selp.f64	%fd820, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p53;
	mul.wide.u32 	%rd82, %r227, 8;
	add.s64 	%rd84, %rd82, %rd41;
	ld.const.f64 	%fd821, [%rd84+8];
	mul.rn.f64 	%fd154, %fd1778, %fd1778;
	fma.rn.f64 	%fd822, %fd820, %fd154, %fd821;
	ld.const.f64 	%fd823, [%rd84+16];
	fma.rn.f64 	%fd824, %fd822, %fd154, %fd823;
	ld.const.f64 	%fd825, [%rd84+24];
	fma.rn.f64 	%fd826, %fd824, %fd154, %fd825;
	ld.const.f64 	%fd827, [%rd84+32];
	fma.rn.f64 	%fd828, %fd826, %fd154, %fd827;
	ld.const.f64 	%fd829, [%rd84+40];
	fma.rn.f64 	%fd830, %fd828, %fd154, %fd829;
	ld.const.f64 	%fd831, [%rd84+48];
	fma.rn.f64 	%fd155, %fd830, %fd154, %fd831;
	fma.rn.f64 	%fd1779, %fd155, %fd1778, %fd1778;
	setp.eq.s32	%p54, %r226, 0;
	@%p54 bra 	BB0_78;

	mov.f64 	%fd832, 0d3FF0000000000000;
	fma.rn.f64 	%fd1779, %fd155, %fd154, %fd832;

BB0_78:
	and.b32  	%r228, %r466, 2;
	setp.eq.s32	%p55, %r228, 0;
	@%p55 bra 	BB0_80;

	mov.f64 	%fd833, 0d0000000000000000;
	mov.f64 	%fd834, 0dBFF0000000000000;
	fma.rn.f64 	%fd1779, %fd1779, %fd834, %fd833;

BB0_80:
	ld.global.f64 	%fd1780, [%rd2+40];
	abs.f64 	%fd835, %fd1780;
	setp.neu.f64	%p56, %fd835, 0d7FF0000000000000;
	@%p56 bra 	BB0_82;

	mov.f64 	%fd836, 0d0000000000000000;
	mul.rn.f64 	%fd1780, %fd1780, %fd836;

BB0_82:
	mul.f64 	%fd837, %fd1780, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r467, %fd837;
	st.local.u32 	[%rd37], %r467;
	cvt.rn.f64.s32	%fd838, %r467;
	neg.f64 	%fd839, %fd838;
	fma.rn.f64 	%fd841, %fd839, %fd647, %fd1780;
	fma.rn.f64 	%fd843, %fd839, %fd649, %fd841;
	fma.rn.f64 	%fd1781, %fd839, %fd651, %fd843;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r229}, %fd1780;
	}
	and.b32  	%r230, %r229, 2145386496;
	setp.lt.u32	%p57, %r230, 1105199104;
	@%p57 bra 	BB0_84;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1780;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1781, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r467, [%rd37];

BB0_84:
	and.b32  	%r231, %r467, 1;
	shl.b32 	%r232, %r231, 3;
	setp.eq.b32	%p58, %r231, 1;
	selp.f64	%fd845, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p58;
	mul.wide.u32 	%rd89, %r232, 8;
	add.s64 	%rd91, %rd89, %rd41;
	ld.const.f64 	%fd846, [%rd91+8];
	mul.rn.f64 	%fd167, %fd1781, %fd1781;
	fma.rn.f64 	%fd847, %fd845, %fd167, %fd846;
	ld.const.f64 	%fd848, [%rd91+16];
	fma.rn.f64 	%fd849, %fd847, %fd167, %fd848;
	ld.const.f64 	%fd850, [%rd91+24];
	fma.rn.f64 	%fd851, %fd849, %fd167, %fd850;
	ld.const.f64 	%fd852, [%rd91+32];
	fma.rn.f64 	%fd853, %fd851, %fd167, %fd852;
	ld.const.f64 	%fd854, [%rd91+40];
	fma.rn.f64 	%fd855, %fd853, %fd167, %fd854;
	ld.const.f64 	%fd856, [%rd91+48];
	fma.rn.f64 	%fd168, %fd855, %fd167, %fd856;
	fma.rn.f64 	%fd1782, %fd168, %fd1781, %fd1781;
	setp.eq.s32	%p59, %r231, 0;
	@%p59 bra 	BB0_86;

	mov.f64 	%fd857, 0d3FF0000000000000;
	fma.rn.f64 	%fd1782, %fd168, %fd167, %fd857;

BB0_86:
	and.b32  	%r233, %r467, 2;
	setp.eq.s32	%p60, %r233, 0;
	@%p60 bra 	BB0_88;

	mov.f64 	%fd858, 0d0000000000000000;
	mov.f64 	%fd859, 0dBFF0000000000000;
	fma.rn.f64 	%fd1782, %fd1782, %fd859, %fd858;

BB0_88:
	ld.global.f64 	%fd860, [%rd2+24];
	add.f64 	%fd861, %fd860, %fd860;
	mul.f64 	%fd862, %fd860, %fd861;
	mul.f64 	%fd863, %fd1779, %fd1782;
	div.rn.f64 	%fd174, %fd863, %fd862;
	ld.global.f64 	%fd1783, [%rd2+40];
	abs.f64 	%fd864, %fd1783;
	setp.neu.f64	%p61, %fd864, 0d7FF0000000000000;
	@%p61 bra 	BB0_90;

	mov.f64 	%fd865, 0d0000000000000000;
	mul.rn.f64 	%fd1783, %fd1783, %fd865;

BB0_90:
	mul.f64 	%fd866, %fd1783, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r468, %fd866;
	st.local.u32 	[%rd37], %r468;
	cvt.rn.f64.s32	%fd867, %r468;
	neg.f64 	%fd868, %fd867;
	fma.rn.f64 	%fd870, %fd868, %fd647, %fd1783;
	fma.rn.f64 	%fd872, %fd868, %fd649, %fd870;
	fma.rn.f64 	%fd1784, %fd868, %fd651, %fd872;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r234}, %fd1783;
	}
	and.b32  	%r235, %r234, 2145386496;
	setp.lt.u32	%p62, %r235, 1105199104;
	@%p62 bra 	BB0_92;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1783;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1784, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r468, [%rd37];

BB0_92:
	add.s32 	%r46, %r468, 1;
	and.b32  	%r236, %r46, 1;
	shl.b32 	%r237, %r236, 3;
	setp.eq.b32	%p63, %r236, 1;
	selp.f64	%fd874, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p63;
	mul.wide.u32 	%rd96, %r237, 8;
	add.s64 	%rd98, %rd96, %rd41;
	ld.const.f64 	%fd875, [%rd98+8];
	mul.rn.f64 	%fd181, %fd1784, %fd1784;
	fma.rn.f64 	%fd876, %fd874, %fd181, %fd875;
	ld.const.f64 	%fd877, [%rd98+16];
	fma.rn.f64 	%fd878, %fd876, %fd181, %fd877;
	ld.const.f64 	%fd879, [%rd98+24];
	fma.rn.f64 	%fd880, %fd878, %fd181, %fd879;
	ld.const.f64 	%fd881, [%rd98+32];
	fma.rn.f64 	%fd882, %fd880, %fd181, %fd881;
	ld.const.f64 	%fd883, [%rd98+40];
	fma.rn.f64 	%fd884, %fd882, %fd181, %fd883;
	ld.const.f64 	%fd885, [%rd98+48];
	fma.rn.f64 	%fd182, %fd884, %fd181, %fd885;
	fma.rn.f64 	%fd1785, %fd182, %fd1784, %fd1784;
	setp.eq.s32	%p64, %r236, 0;
	@%p64 bra 	BB0_94;

	mov.f64 	%fd886, 0d3FF0000000000000;
	fma.rn.f64 	%fd1785, %fd182, %fd181, %fd886;

BB0_94:
	and.b32  	%r238, %r46, 2;
	setp.eq.s32	%p65, %r238, 0;
	@%p65 bra 	BB0_96;

	mov.f64 	%fd887, 0d0000000000000000;
	mov.f64 	%fd888, 0dBFF0000000000000;
	fma.rn.f64 	%fd1785, %fd1785, %fd888, %fd887;

BB0_96:
	ld.global.f64 	%fd1786, [%rd2+40];
	abs.f64 	%fd889, %fd1786;
	setp.neu.f64	%p66, %fd889, 0d7FF0000000000000;
	@%p66 bra 	BB0_98;

	mov.f64 	%fd890, 0d0000000000000000;
	mul.rn.f64 	%fd1786, %fd1786, %fd890;

BB0_98:
	mul.f64 	%fd891, %fd1786, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r469, %fd891;
	st.local.u32 	[%rd37], %r469;
	cvt.rn.f64.s32	%fd892, %r469;
	neg.f64 	%fd893, %fd892;
	fma.rn.f64 	%fd895, %fd893, %fd647, %fd1786;
	fma.rn.f64 	%fd897, %fd893, %fd649, %fd895;
	fma.rn.f64 	%fd1787, %fd893, %fd651, %fd897;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r239}, %fd1786;
	}
	and.b32  	%r240, %r239, 2145386496;
	setp.lt.u32	%p67, %r240, 1105199104;
	@%p67 bra 	BB0_100;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1786;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1787, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r469, [%rd37];

BB0_100:
	add.s32 	%r50, %r469, 1;
	and.b32  	%r241, %r50, 1;
	shl.b32 	%r242, %r241, 3;
	setp.eq.b32	%p68, %r241, 1;
	selp.f64	%fd899, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p68;
	mul.wide.u32 	%rd103, %r242, 8;
	add.s64 	%rd105, %rd103, %rd41;
	ld.const.f64 	%fd900, [%rd105+8];
	mul.rn.f64 	%fd194, %fd1787, %fd1787;
	fma.rn.f64 	%fd901, %fd899, %fd194, %fd900;
	ld.const.f64 	%fd902, [%rd105+16];
	fma.rn.f64 	%fd903, %fd901, %fd194, %fd902;
	ld.const.f64 	%fd904, [%rd105+24];
	fma.rn.f64 	%fd905, %fd903, %fd194, %fd904;
	ld.const.f64 	%fd906, [%rd105+32];
	fma.rn.f64 	%fd907, %fd905, %fd194, %fd906;
	ld.const.f64 	%fd908, [%rd105+40];
	fma.rn.f64 	%fd909, %fd907, %fd194, %fd908;
	ld.const.f64 	%fd910, [%rd105+48];
	fma.rn.f64 	%fd195, %fd909, %fd194, %fd910;
	fma.rn.f64 	%fd1788, %fd195, %fd1787, %fd1787;
	setp.eq.s32	%p69, %r241, 0;
	@%p69 bra 	BB0_102;

	mov.f64 	%fd911, 0d3FF0000000000000;
	fma.rn.f64 	%fd1788, %fd195, %fd194, %fd911;

BB0_102:
	and.b32  	%r243, %r50, 2;
	setp.eq.s32	%p70, %r243, 0;
	@%p70 bra 	BB0_104;

	mov.f64 	%fd912, 0d0000000000000000;
	mov.f64 	%fd913, 0dBFF0000000000000;
	fma.rn.f64 	%fd1788, %fd1788, %fd913, %fd912;

BB0_104:
	ld.global.f64 	%fd915, [%rd2+32];
	add.f64 	%fd916, %fd915, %fd915;
	mul.f64 	%fd917, %fd915, %fd916;
	mul.f64 	%fd918, %fd1785, %fd1788;
	div.rn.f64 	%fd919, %fd918, %fd917;
	add.f64 	%fd1723, %fd174, %fd919;
	mov.f64 	%fd1790, 0d0000000000000000;
	@%p2 bra 	BB0_110;

	ld.global.f64 	%fd202, [%rd2];
	ld.global.f64 	%fd203, [%rd2+8];
	ld.global.f64 	%fd205, [%rd2+16];
	ld.global.f64 	%fd206, [%rd2+48];
	mov.f64 	%fd1790, 0d0000000000000000;
	mov.u32 	%r470, 0;

BB0_106:
	add.f64 	%fd1703, %fd1724, %fd1724;
	rem.s32 	%r250, %r470, %r2;
	cvt.rn.f64.s32	%fd921, %r250;
	sub.f64 	%fd922, %fd921, %fd203;
	mul.f64 	%fd923, %fd1725, %fd922;
	mul.f64 	%fd924, %fd922, %fd923;
	mul.f64 	%fd925, %fd1703, %fd922;
	div.s32 	%r251, %r470, %r2;
	cvt.rn.f64.s32	%fd926, %r251;
	sub.f64 	%fd927, %fd926, %fd205;
	mul.f64 	%fd928, %fd925, %fd927;
	sub.f64 	%fd929, %fd924, %fd928;
	mul.f64 	%fd930, %fd1723, %fd927;
	fma.rn.f64 	%fd208, %fd927, %fd930, %fd929;
	neg.f64 	%fd931, %fd208;
	mov.f64 	%fd932, 0d4338000000000000;
	mov.f64 	%fd933, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd934, %fd931, %fd933, %fd932;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd934;
	}
	mov.f64 	%fd935, 0dC338000000000000;
	add.rn.f64 	%fd936, %fd934, %fd935;
	mov.f64 	%fd937, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd938, %fd936, %fd937, %fd931;
	mov.f64 	%fd939, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd940, %fd936, %fd939, %fd938;
	mov.f64 	%fd941, 0d3E928AF3FCA213EA;
	mov.f64 	%fd942, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd943, %fd942, %fd940, %fd941;
	mov.f64 	%fd944, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd945, %fd943, %fd940, %fd944;
	mov.f64 	%fd946, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd947, %fd945, %fd940, %fd946;
	mov.f64 	%fd948, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd949, %fd947, %fd940, %fd948;
	mov.f64 	%fd950, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd951, %fd949, %fd940, %fd950;
	mov.f64 	%fd952, 0d3F81111111122322;
	fma.rn.f64 	%fd953, %fd951, %fd940, %fd952;
	mov.f64 	%fd954, 0d3FA55555555502A1;
	fma.rn.f64 	%fd955, %fd953, %fd940, %fd954;
	mov.f64 	%fd956, 0d3FC5555555555511;
	fma.rn.f64 	%fd957, %fd955, %fd940, %fd956;
	mov.f64 	%fd958, 0d3FE000000000000B;
	fma.rn.f64 	%fd959, %fd957, %fd940, %fd958;
	mov.f64 	%fd960, 0d3FF0000000000000;
	fma.rn.f64 	%fd961, %fd959, %fd940, %fd960;
	fma.rn.f64 	%fd962, %fd961, %fd940, %fd960;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd962;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r56}, %fd962;
	}
	shl.b32 	%r252, %r54, 20;
	add.s32 	%r253, %r56, %r252;
	mov.b64 	%fd1789, {%r55, %r253};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r254}, %fd931;
	}
	mov.b32 	 %f7, %r254;
	abs.f32 	%f2, %f7;
	setp.lt.f32	%p72, %f2, 0f4086232B;
	@%p72 bra 	BB0_109;

	setp.gt.f64	%p73, %fd208, 0d8000000000000000;
	mov.f64 	%fd963, 0d7FF0000000000000;
	sub.f64 	%fd964, %fd963, %fd208;
	selp.f64	%fd1789, 0d0000000000000000, %fd964, %p73;
	setp.geu.f32	%p74, %f2, 0f40874800;
	@%p74 bra 	BB0_109;

	shr.u32 	%r255, %r54, 31;
	add.s32 	%r256, %r54, %r255;
	shr.s32 	%r257, %r256, 1;
	shl.b32 	%r258, %r257, 20;
	add.s32 	%r259, %r258, %r56;
	mov.b64 	%fd965, {%r55, %r259};
	sub.s32 	%r260, %r54, %r257;
	shl.b32 	%r261, %r260, 20;
	add.s32 	%r262, %r261, 1072693248;
	mov.u32 	%r263, 0;
	mov.b64 	%fd966, {%r263, %r262};
	mul.f64 	%fd1789, %fd965, %fd966;

BB0_109:
	fma.rn.f64 	%fd967, %fd202, %fd1789, %fd206;
	add.s32 	%r264, %r470, %r5;
	mul.wide.s32 	%rd109, %r264, 4;
	add.s64 	%rd110, %rd1, %rd109;
	ld.global.u32 	%r265, [%rd110];
	cvt.rn.f64.s32	%fd968, %r265;
	sub.f64 	%fd969, %fd967, %fd968;
	fma.rn.f64 	%fd1790, %fd969, %fd969, %fd1790;
	add.s32 	%r470, %r470, 1;
	setp.lt.s32	%p75, %r470, %r3;
	@%p75 bra 	BB0_106;

BB0_110:
	div.rn.f64 	%fd1758, %fd1790, %fd1714;
	setp.lt.f64	%p76, %fd1758, %fd63;
	mov.f64 	%fd1756, %fd62;
	@%p76 bra 	BB0_223;

	ld.global.f64 	%fd970, [%rd5];
	ld.global.f64 	%fd971, [%rd4];
	sub.f64 	%fd972, %fd971, %fd970;
	st.global.f64 	[%rd4], %fd972;
	ld.global.f64 	%fd1791, [%rd2+40];
	abs.f64 	%fd973, %fd1791;
	setp.neu.f64	%p77, %fd973, 0d7FF0000000000000;
	@%p77 bra 	BB0_113;

	mov.f64 	%fd974, 0d0000000000000000;
	mul.rn.f64 	%fd1791, %fd1791, %fd974;

BB0_113:
	mul.f64 	%fd975, %fd1791, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r471, %fd975;
	st.local.u32 	[%rd37], %r471;
	cvt.rn.f64.s32	%fd976, %r471;
	neg.f64 	%fd977, %fd976;
	fma.rn.f64 	%fd979, %fd977, %fd647, %fd1791;
	fma.rn.f64 	%fd981, %fd977, %fd649, %fd979;
	fma.rn.f64 	%fd1792, %fd977, %fd651, %fd981;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r266}, %fd1791;
	}
	and.b32  	%r267, %r266, 2145386496;
	setp.lt.u32	%p78, %r267, 1105199104;
	@%p78 bra 	BB0_115;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1791;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1792, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r471, [%rd37];

BB0_115:
	add.s32 	%r61, %r471, 1;
	and.b32  	%r268, %r61, 1;
	shl.b32 	%r269, %r268, 3;
	setp.eq.b32	%p79, %r268, 1;
	selp.f64	%fd983, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p79;
	mul.wide.u32 	%rd115, %r269, 8;
	add.s64 	%rd117, %rd115, %rd41;
	ld.const.f64 	%fd984, [%rd117+8];
	mul.rn.f64 	%fd222, %fd1792, %fd1792;
	fma.rn.f64 	%fd985, %fd983, %fd222, %fd984;
	ld.const.f64 	%fd986, [%rd117+16];
	fma.rn.f64 	%fd987, %fd985, %fd222, %fd986;
	ld.const.f64 	%fd988, [%rd117+24];
	fma.rn.f64 	%fd989, %fd987, %fd222, %fd988;
	ld.const.f64 	%fd990, [%rd117+32];
	fma.rn.f64 	%fd991, %fd989, %fd222, %fd990;
	ld.const.f64 	%fd992, [%rd117+40];
	fma.rn.f64 	%fd993, %fd991, %fd222, %fd992;
	ld.const.f64 	%fd994, [%rd117+48];
	fma.rn.f64 	%fd223, %fd993, %fd222, %fd994;
	fma.rn.f64 	%fd1793, %fd223, %fd1792, %fd1792;
	setp.eq.s32	%p80, %r268, 0;
	@%p80 bra 	BB0_117;

	mov.f64 	%fd995, 0d3FF0000000000000;
	fma.rn.f64 	%fd1793, %fd223, %fd222, %fd995;

BB0_117:
	and.b32  	%r270, %r61, 2;
	setp.eq.s32	%p81, %r270, 0;
	@%p81 bra 	BB0_119;

	mov.f64 	%fd996, 0d0000000000000000;
	mov.f64 	%fd997, 0dBFF0000000000000;
	fma.rn.f64 	%fd1793, %fd1793, %fd997, %fd996;

BB0_119:
	ld.global.f64 	%fd1794, [%rd2+40];
	abs.f64 	%fd998, %fd1794;
	setp.neu.f64	%p82, %fd998, 0d7FF0000000000000;
	@%p82 bra 	BB0_121;

	mov.f64 	%fd999, 0d0000000000000000;
	mul.rn.f64 	%fd1794, %fd1794, %fd999;

BB0_121:
	mul.f64 	%fd1000, %fd1794, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r472, %fd1000;
	st.local.u32 	[%rd37], %r472;
	cvt.rn.f64.s32	%fd1001, %r472;
	neg.f64 	%fd1002, %fd1001;
	fma.rn.f64 	%fd1004, %fd1002, %fd647, %fd1794;
	fma.rn.f64 	%fd1006, %fd1002, %fd649, %fd1004;
	fma.rn.f64 	%fd1795, %fd1002, %fd651, %fd1006;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r271}, %fd1794;
	}
	and.b32  	%r272, %r271, 2145386496;
	setp.lt.u32	%p83, %r272, 1105199104;
	@%p83 bra 	BB0_123;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1794;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1795, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r472, [%rd37];

BB0_123:
	add.s32 	%r65, %r472, 1;
	and.b32  	%r273, %r65, 1;
	shl.b32 	%r274, %r273, 3;
	setp.eq.b32	%p84, %r273, 1;
	selp.f64	%fd1008, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p84;
	mul.wide.u32 	%rd122, %r274, 8;
	add.s64 	%rd124, %rd122, %rd41;
	ld.const.f64 	%fd1009, [%rd124+8];
	mul.rn.f64 	%fd235, %fd1795, %fd1795;
	fma.rn.f64 	%fd1010, %fd1008, %fd235, %fd1009;
	ld.const.f64 	%fd1011, [%rd124+16];
	fma.rn.f64 	%fd1012, %fd1010, %fd235, %fd1011;
	ld.const.f64 	%fd1013, [%rd124+24];
	fma.rn.f64 	%fd1014, %fd1012, %fd235, %fd1013;
	ld.const.f64 	%fd1015, [%rd124+32];
	fma.rn.f64 	%fd1016, %fd1014, %fd235, %fd1015;
	ld.const.f64 	%fd1017, [%rd124+40];
	fma.rn.f64 	%fd1018, %fd1016, %fd235, %fd1017;
	ld.const.f64 	%fd1019, [%rd124+48];
	fma.rn.f64 	%fd236, %fd1018, %fd235, %fd1019;
	fma.rn.f64 	%fd1796, %fd236, %fd1795, %fd1795;
	setp.eq.s32	%p85, %r273, 0;
	@%p85 bra 	BB0_125;

	mov.f64 	%fd1020, 0d3FF0000000000000;
	fma.rn.f64 	%fd1796, %fd236, %fd235, %fd1020;

BB0_125:
	and.b32  	%r275, %r65, 2;
	setp.eq.s32	%p86, %r275, 0;
	@%p86 bra 	BB0_127;

	mov.f64 	%fd1021, 0d0000000000000000;
	mov.f64 	%fd1022, 0dBFF0000000000000;
	fma.rn.f64 	%fd1796, %fd1796, %fd1022, %fd1021;

BB0_127:
	ld.global.f64 	%fd1023, [%rd2+24];
	add.f64 	%fd1024, %fd1023, %fd1023;
	mul.f64 	%fd1025, %fd1023, %fd1024;
	mul.f64 	%fd1026, %fd1793, %fd1796;
	div.rn.f64 	%fd242, %fd1026, %fd1025;
	ld.global.f64 	%fd1797, [%rd2+40];
	abs.f64 	%fd1027, %fd1797;
	setp.neu.f64	%p87, %fd1027, 0d7FF0000000000000;
	@%p87 bra 	BB0_129;

	mov.f64 	%fd1028, 0d0000000000000000;
	mul.rn.f64 	%fd1797, %fd1797, %fd1028;

BB0_129:
	mul.f64 	%fd1029, %fd1797, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r473, %fd1029;
	st.local.u32 	[%rd37], %r473;
	cvt.rn.f64.s32	%fd1030, %r473;
	neg.f64 	%fd1031, %fd1030;
	fma.rn.f64 	%fd1033, %fd1031, %fd647, %fd1797;
	fma.rn.f64 	%fd1035, %fd1031, %fd649, %fd1033;
	fma.rn.f64 	%fd1798, %fd1031, %fd651, %fd1035;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r276}, %fd1797;
	}
	and.b32  	%r277, %r276, 2145386496;
	setp.lt.u32	%p88, %r277, 1105199104;
	@%p88 bra 	BB0_131;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1797;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1798, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r473, [%rd37];

BB0_131:
	and.b32  	%r278, %r473, 1;
	shl.b32 	%r279, %r278, 3;
	setp.eq.b32	%p89, %r278, 1;
	selp.f64	%fd1037, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p89;
	mul.wide.u32 	%rd129, %r279, 8;
	add.s64 	%rd131, %rd129, %rd41;
	ld.const.f64 	%fd1038, [%rd131+8];
	mul.rn.f64 	%fd249, %fd1798, %fd1798;
	fma.rn.f64 	%fd1039, %fd1037, %fd249, %fd1038;
	ld.const.f64 	%fd1040, [%rd131+16];
	fma.rn.f64 	%fd1041, %fd1039, %fd249, %fd1040;
	ld.const.f64 	%fd1042, [%rd131+24];
	fma.rn.f64 	%fd1043, %fd1041, %fd249, %fd1042;
	ld.const.f64 	%fd1044, [%rd131+32];
	fma.rn.f64 	%fd1045, %fd1043, %fd249, %fd1044;
	ld.const.f64 	%fd1046, [%rd131+40];
	fma.rn.f64 	%fd1047, %fd1045, %fd249, %fd1046;
	ld.const.f64 	%fd1048, [%rd131+48];
	fma.rn.f64 	%fd250, %fd1047, %fd249, %fd1048;
	fma.rn.f64 	%fd1799, %fd250, %fd1798, %fd1798;
	setp.eq.s32	%p90, %r278, 0;
	@%p90 bra 	BB0_133;

	mov.f64 	%fd1049, 0d3FF0000000000000;
	fma.rn.f64 	%fd1799, %fd250, %fd249, %fd1049;

BB0_133:
	and.b32  	%r280, %r473, 2;
	setp.eq.s32	%p91, %r280, 0;
	@%p91 bra 	BB0_135;

	mov.f64 	%fd1050, 0d0000000000000000;
	mov.f64 	%fd1051, 0dBFF0000000000000;
	fma.rn.f64 	%fd1799, %fd1799, %fd1051, %fd1050;

BB0_135:
	ld.global.f64 	%fd1800, [%rd2+40];
	abs.f64 	%fd1052, %fd1800;
	setp.neu.f64	%p92, %fd1052, 0d7FF0000000000000;
	@%p92 bra 	BB0_137;

	mov.f64 	%fd1053, 0d0000000000000000;
	mul.rn.f64 	%fd1800, %fd1800, %fd1053;

BB0_137:
	mul.f64 	%fd1054, %fd1800, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r474, %fd1054;
	st.local.u32 	[%rd37], %r474;
	cvt.rn.f64.s32	%fd1055, %r474;
	neg.f64 	%fd1056, %fd1055;
	fma.rn.f64 	%fd1058, %fd1056, %fd647, %fd1800;
	fma.rn.f64 	%fd1060, %fd1056, %fd649, %fd1058;
	fma.rn.f64 	%fd1801, %fd1056, %fd651, %fd1060;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r281}, %fd1800;
	}
	and.b32  	%r282, %r281, 2145386496;
	setp.lt.u32	%p93, %r282, 1105199104;
	@%p93 bra 	BB0_139;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1800;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1801, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r474, [%rd37];

BB0_139:
	and.b32  	%r283, %r474, 1;
	shl.b32 	%r284, %r283, 3;
	setp.eq.b32	%p94, %r283, 1;
	selp.f64	%fd1062, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p94;
	mul.wide.u32 	%rd136, %r284, 8;
	add.s64 	%rd138, %rd136, %rd41;
	ld.const.f64 	%fd1063, [%rd138+8];
	mul.rn.f64 	%fd262, %fd1801, %fd1801;
	fma.rn.f64 	%fd1064, %fd1062, %fd262, %fd1063;
	ld.const.f64 	%fd1065, [%rd138+16];
	fma.rn.f64 	%fd1066, %fd1064, %fd262, %fd1065;
	ld.const.f64 	%fd1067, [%rd138+24];
	fma.rn.f64 	%fd1068, %fd1066, %fd262, %fd1067;
	ld.const.f64 	%fd1069, [%rd138+32];
	fma.rn.f64 	%fd1070, %fd1068, %fd262, %fd1069;
	ld.const.f64 	%fd1071, [%rd138+40];
	fma.rn.f64 	%fd1072, %fd1070, %fd262, %fd1071;
	ld.const.f64 	%fd1073, [%rd138+48];
	fma.rn.f64 	%fd263, %fd1072, %fd262, %fd1073;
	fma.rn.f64 	%fd1802, %fd263, %fd1801, %fd1801;
	setp.eq.s32	%p95, %r283, 0;
	@%p95 bra 	BB0_141;

	mov.f64 	%fd1074, 0d3FF0000000000000;
	fma.rn.f64 	%fd1802, %fd263, %fd262, %fd1074;

BB0_141:
	and.b32  	%r285, %r474, 2;
	setp.eq.s32	%p96, %r285, 0;
	@%p96 bra 	BB0_143;

	mov.f64 	%fd1075, 0d0000000000000000;
	mov.f64 	%fd1076, 0dBFF0000000000000;
	fma.rn.f64 	%fd1802, %fd1802, %fd1076, %fd1075;

BB0_143:
	ld.global.f64 	%fd1077, [%rd2+32];
	add.f64 	%fd1078, %fd1077, %fd1077;
	mul.f64 	%fd1079, %fd1077, %fd1078;
	mul.f64 	%fd1080, %fd1799, %fd1802;
	div.rn.f64 	%fd1081, %fd1080, %fd1079;
	add.f64 	%fd1725, %fd242, %fd1081;
	ld.global.f64 	%fd1082, [%rd2+40];
	add.f64 	%fd1803, %fd1082, %fd1082;
	abs.f64 	%fd1083, %fd1803;
	setp.neu.f64	%p97, %fd1083, 0d7FF0000000000000;
	@%p97 bra 	BB0_145;

	mov.f64 	%fd1084, 0d0000000000000000;
	mul.rn.f64 	%fd1803, %fd1803, %fd1084;

BB0_145:
	mul.f64 	%fd1085, %fd1803, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r475, %fd1085;
	st.local.u32 	[%rd37], %r475;
	cvt.rn.f64.s32	%fd1086, %r475;
	neg.f64 	%fd1087, %fd1086;
	fma.rn.f64 	%fd1089, %fd1087, %fd647, %fd1803;
	fma.rn.f64 	%fd1091, %fd1087, %fd649, %fd1089;
	fma.rn.f64 	%fd1804, %fd1087, %fd651, %fd1091;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r286}, %fd1803;
	}
	and.b32  	%r287, %r286, 2145386496;
	setp.lt.u32	%p98, %r287, 1105199104;
	@%p98 bra 	BB0_147;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1803;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1804, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r475, [%rd37];

BB0_147:
	and.b32  	%r288, %r475, 1;
	shl.b32 	%r289, %r288, 3;
	setp.eq.b32	%p99, %r288, 1;
	selp.f64	%fd1093, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p99;
	mul.wide.u32 	%rd143, %r289, 8;
	add.s64 	%rd145, %rd143, %rd41;
	ld.const.f64 	%fd1094, [%rd145+8];
	mul.rn.f64 	%fd276, %fd1804, %fd1804;
	fma.rn.f64 	%fd1095, %fd1093, %fd276, %fd1094;
	ld.const.f64 	%fd1096, [%rd145+16];
	fma.rn.f64 	%fd1097, %fd1095, %fd276, %fd1096;
	ld.const.f64 	%fd1098, [%rd145+24];
	fma.rn.f64 	%fd1099, %fd1097, %fd276, %fd1098;
	ld.const.f64 	%fd1100, [%rd145+32];
	fma.rn.f64 	%fd1101, %fd1099, %fd276, %fd1100;
	ld.const.f64 	%fd1102, [%rd145+40];
	fma.rn.f64 	%fd1103, %fd1101, %fd276, %fd1102;
	ld.const.f64 	%fd1104, [%rd145+48];
	fma.rn.f64 	%fd277, %fd1103, %fd276, %fd1104;
	fma.rn.f64 	%fd1805, %fd277, %fd1804, %fd1804;
	setp.eq.s32	%p100, %r288, 0;
	@%p100 bra 	BB0_149;

	mov.f64 	%fd1105, 0d3FF0000000000000;
	fma.rn.f64 	%fd1805, %fd277, %fd276, %fd1105;

BB0_149:
	and.b32  	%r290, %r475, 2;
	setp.eq.s32	%p101, %r290, 0;
	@%p101 bra 	BB0_151;

	mov.f64 	%fd1106, 0d0000000000000000;
	mov.f64 	%fd1107, 0dBFF0000000000000;
	fma.rn.f64 	%fd1805, %fd1805, %fd1107, %fd1106;

BB0_151:
	ld.global.f64 	%fd1108, [%rd2+24];
	mul.f64 	%fd1109, %fd1108, 0dC010000000000000;
	mul.f64 	%fd1110, %fd1108, %fd1109;
	div.rn.f64 	%fd283, %fd1805, %fd1110;
	ld.global.f64 	%fd1111, [%rd2+40];
	add.f64 	%fd1806, %fd1111, %fd1111;
	abs.f64 	%fd1112, %fd1806;
	setp.neu.f64	%p102, %fd1112, 0d7FF0000000000000;
	@%p102 bra 	BB0_153;

	mov.f64 	%fd1113, 0d0000000000000000;
	mul.rn.f64 	%fd1806, %fd1806, %fd1113;

BB0_153:
	mul.f64 	%fd1114, %fd1806, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r476, %fd1114;
	st.local.u32 	[%rd37], %r476;
	cvt.rn.f64.s32	%fd1115, %r476;
	neg.f64 	%fd1116, %fd1115;
	fma.rn.f64 	%fd1118, %fd1116, %fd647, %fd1806;
	fma.rn.f64 	%fd1120, %fd1116, %fd649, %fd1118;
	fma.rn.f64 	%fd1807, %fd1116, %fd651, %fd1120;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r291}, %fd1806;
	}
	and.b32  	%r292, %r291, 2145386496;
	setp.lt.u32	%p103, %r292, 1105199104;
	@%p103 bra 	BB0_155;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1806;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1807, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r476, [%rd37];

BB0_155:
	and.b32  	%r293, %r476, 1;
	shl.b32 	%r294, %r293, 3;
	setp.eq.b32	%p104, %r293, 1;
	selp.f64	%fd1122, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p104;
	mul.wide.u32 	%rd150, %r294, 8;
	add.s64 	%rd152, %rd150, %rd41;
	ld.const.f64 	%fd1123, [%rd152+8];
	mul.rn.f64 	%fd290, %fd1807, %fd1807;
	fma.rn.f64 	%fd1124, %fd1122, %fd290, %fd1123;
	ld.const.f64 	%fd1125, [%rd152+16];
	fma.rn.f64 	%fd1126, %fd1124, %fd290, %fd1125;
	ld.const.f64 	%fd1127, [%rd152+24];
	fma.rn.f64 	%fd1128, %fd1126, %fd290, %fd1127;
	ld.const.f64 	%fd1129, [%rd152+32];
	fma.rn.f64 	%fd1130, %fd1128, %fd290, %fd1129;
	ld.const.f64 	%fd1131, [%rd152+40];
	fma.rn.f64 	%fd1132, %fd1130, %fd290, %fd1131;
	ld.const.f64 	%fd1133, [%rd152+48];
	fma.rn.f64 	%fd291, %fd1132, %fd290, %fd1133;
	fma.rn.f64 	%fd1808, %fd291, %fd1807, %fd1807;
	setp.eq.s32	%p105, %r293, 0;
	@%p105 bra 	BB0_157;

	mov.f64 	%fd1134, 0d3FF0000000000000;
	fma.rn.f64 	%fd1808, %fd291, %fd290, %fd1134;

BB0_157:
	and.b32  	%r295, %r476, 2;
	setp.eq.s32	%p106, %r295, 0;
	@%p106 bra 	BB0_159;

	mov.f64 	%fd1135, 0d0000000000000000;
	mov.f64 	%fd1136, 0dBFF0000000000000;
	fma.rn.f64 	%fd1808, %fd1808, %fd1136, %fd1135;

BB0_159:
	ld.global.f64 	%fd1137, [%rd2+32];
	mul.f64 	%fd1138, %fd1137, 0d4010000000000000;
	mul.f64 	%fd1139, %fd1137, %fd1138;
	div.rn.f64 	%fd1140, %fd1808, %fd1139;
	add.f64 	%fd1724, %fd283, %fd1140;
	ld.global.f64 	%fd1809, [%rd2+40];
	abs.f64 	%fd1141, %fd1809;
	setp.neu.f64	%p107, %fd1141, 0d7FF0000000000000;
	@%p107 bra 	BB0_161;

	mov.f64 	%fd1142, 0d0000000000000000;
	mul.rn.f64 	%fd1809, %fd1809, %fd1142;

BB0_161:
	mul.f64 	%fd1143, %fd1809, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r477, %fd1143;
	st.local.u32 	[%rd37], %r477;
	cvt.rn.f64.s32	%fd1144, %r477;
	neg.f64 	%fd1145, %fd1144;
	fma.rn.f64 	%fd1147, %fd1145, %fd647, %fd1809;
	fma.rn.f64 	%fd1149, %fd1145, %fd649, %fd1147;
	fma.rn.f64 	%fd1810, %fd1145, %fd651, %fd1149;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r296}, %fd1809;
	}
	and.b32  	%r297, %r296, 2145386496;
	setp.lt.u32	%p108, %r297, 1105199104;
	@%p108 bra 	BB0_163;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1809;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1810, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r477, [%rd37];

BB0_163:
	and.b32  	%r298, %r477, 1;
	shl.b32 	%r299, %r298, 3;
	setp.eq.b32	%p109, %r298, 1;
	selp.f64	%fd1151, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p109;
	mul.wide.u32 	%rd157, %r299, 8;
	add.s64 	%rd159, %rd157, %rd41;
	ld.const.f64 	%fd1152, [%rd159+8];
	mul.rn.f64 	%fd304, %fd1810, %fd1810;
	fma.rn.f64 	%fd1153, %fd1151, %fd304, %fd1152;
	ld.const.f64 	%fd1154, [%rd159+16];
	fma.rn.f64 	%fd1155, %fd1153, %fd304, %fd1154;
	ld.const.f64 	%fd1156, [%rd159+24];
	fma.rn.f64 	%fd1157, %fd1155, %fd304, %fd1156;
	ld.const.f64 	%fd1158, [%rd159+32];
	fma.rn.f64 	%fd1159, %fd1157, %fd304, %fd1158;
	ld.const.f64 	%fd1160, [%rd159+40];
	fma.rn.f64 	%fd1161, %fd1159, %fd304, %fd1160;
	ld.const.f64 	%fd1162, [%rd159+48];
	fma.rn.f64 	%fd305, %fd1161, %fd304, %fd1162;
	fma.rn.f64 	%fd1811, %fd305, %fd1810, %fd1810;
	setp.eq.s32	%p110, %r298, 0;
	@%p110 bra 	BB0_165;

	mov.f64 	%fd1163, 0d3FF0000000000000;
	fma.rn.f64 	%fd1811, %fd305, %fd304, %fd1163;

BB0_165:
	and.b32  	%r300, %r477, 2;
	setp.eq.s32	%p111, %r300, 0;
	@%p111 bra 	BB0_167;

	mov.f64 	%fd1164, 0d0000000000000000;
	mov.f64 	%fd1165, 0dBFF0000000000000;
	fma.rn.f64 	%fd1811, %fd1811, %fd1165, %fd1164;

BB0_167:
	ld.global.f64 	%fd1812, [%rd2+40];
	abs.f64 	%fd1166, %fd1812;
	setp.neu.f64	%p112, %fd1166, 0d7FF0000000000000;
	@%p112 bra 	BB0_169;

	mov.f64 	%fd1167, 0d0000000000000000;
	mul.rn.f64 	%fd1812, %fd1812, %fd1167;

BB0_169:
	mul.f64 	%fd1168, %fd1812, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r478, %fd1168;
	st.local.u32 	[%rd37], %r478;
	cvt.rn.f64.s32	%fd1169, %r478;
	neg.f64 	%fd1170, %fd1169;
	fma.rn.f64 	%fd1172, %fd1170, %fd647, %fd1812;
	fma.rn.f64 	%fd1174, %fd1170, %fd649, %fd1172;
	fma.rn.f64 	%fd1813, %fd1170, %fd651, %fd1174;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r301}, %fd1812;
	}
	and.b32  	%r302, %r301, 2145386496;
	setp.lt.u32	%p113, %r302, 1105199104;
	@%p113 bra 	BB0_171;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1812;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1813, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r478, [%rd37];

BB0_171:
	and.b32  	%r303, %r478, 1;
	shl.b32 	%r304, %r303, 3;
	setp.eq.b32	%p114, %r303, 1;
	selp.f64	%fd1176, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p114;
	mul.wide.u32 	%rd164, %r304, 8;
	add.s64 	%rd166, %rd164, %rd41;
	ld.const.f64 	%fd1177, [%rd166+8];
	mul.rn.f64 	%fd317, %fd1813, %fd1813;
	fma.rn.f64 	%fd1178, %fd1176, %fd317, %fd1177;
	ld.const.f64 	%fd1179, [%rd166+16];
	fma.rn.f64 	%fd1180, %fd1178, %fd317, %fd1179;
	ld.const.f64 	%fd1181, [%rd166+24];
	fma.rn.f64 	%fd1182, %fd1180, %fd317, %fd1181;
	ld.const.f64 	%fd1183, [%rd166+32];
	fma.rn.f64 	%fd1184, %fd1182, %fd317, %fd1183;
	ld.const.f64 	%fd1185, [%rd166+40];
	fma.rn.f64 	%fd1186, %fd1184, %fd317, %fd1185;
	ld.const.f64 	%fd1187, [%rd166+48];
	fma.rn.f64 	%fd318, %fd1186, %fd317, %fd1187;
	fma.rn.f64 	%fd1814, %fd318, %fd1813, %fd1813;
	setp.eq.s32	%p115, %r303, 0;
	@%p115 bra 	BB0_173;

	mov.f64 	%fd1188, 0d3FF0000000000000;
	fma.rn.f64 	%fd1814, %fd318, %fd317, %fd1188;

BB0_173:
	and.b32  	%r305, %r478, 2;
	setp.eq.s32	%p116, %r305, 0;
	@%p116 bra 	BB0_175;

	mov.f64 	%fd1189, 0d0000000000000000;
	mov.f64 	%fd1190, 0dBFF0000000000000;
	fma.rn.f64 	%fd1814, %fd1814, %fd1190, %fd1189;

BB0_175:
	ld.global.f64 	%fd1191, [%rd2+24];
	add.f64 	%fd1192, %fd1191, %fd1191;
	mul.f64 	%fd1193, %fd1191, %fd1192;
	mul.f64 	%fd1194, %fd1811, %fd1814;
	div.rn.f64 	%fd324, %fd1194, %fd1193;
	ld.global.f64 	%fd1815, [%rd2+40];
	abs.f64 	%fd1195, %fd1815;
	setp.neu.f64	%p117, %fd1195, 0d7FF0000000000000;
	@%p117 bra 	BB0_177;

	mov.f64 	%fd1196, 0d0000000000000000;
	mul.rn.f64 	%fd1815, %fd1815, %fd1196;

BB0_177:
	mul.f64 	%fd1197, %fd1815, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r479, %fd1197;
	st.local.u32 	[%rd37], %r479;
	cvt.rn.f64.s32	%fd1198, %r479;
	neg.f64 	%fd1199, %fd1198;
	fma.rn.f64 	%fd1201, %fd1199, %fd647, %fd1815;
	fma.rn.f64 	%fd1203, %fd1199, %fd649, %fd1201;
	fma.rn.f64 	%fd1816, %fd1199, %fd651, %fd1203;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r306}, %fd1815;
	}
	and.b32  	%r307, %r306, 2145386496;
	setp.lt.u32	%p118, %r307, 1105199104;
	@%p118 bra 	BB0_179;

	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1815;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1816, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r479, [%rd37];

BB0_179:
	add.s32 	%r87, %r479, 1;
	and.b32  	%r308, %r87, 1;
	shl.b32 	%r309, %r308, 3;
	setp.eq.b32	%p119, %r308, 1;
	selp.f64	%fd1205, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p119;
	mul.wide.u32 	%rd171, %r309, 8;
	add.s64 	%rd173, %rd171, %rd41;
	ld.const.f64 	%fd1206, [%rd173+8];
	mul.rn.f64 	%fd331, %fd1816, %fd1816;
	fma.rn.f64 	%fd1207, %fd1205, %fd331, %fd1206;
	ld.const.f64 	%fd1208, [%rd173+16];
	fma.rn.f64 	%fd1209, %fd1207, %fd331, %fd1208;
	ld.const.f64 	%fd1210, [%rd173+24];
	fma.rn.f64 	%fd1211, %fd1209, %fd331, %fd1210;
	ld.const.f64 	%fd1212, [%rd173+32];
	fma.rn.f64 	%fd1213, %fd1211, %fd331, %fd1212;
	ld.const.f64 	%fd1214, [%rd173+40];
	fma.rn.f64 	%fd1215, %fd1213, %fd331, %fd1214;
	ld.const.f64 	%fd1216, [%rd173+48];
	fma.rn.f64 	%fd332, %fd1215, %fd331, %fd1216;
	fma.rn.f64 	%fd1817, %fd332, %fd1816, %fd1816;
	setp.eq.s32	%p120, %r308, 0;
	@%p120 bra 	BB0_181;

	mov.f64 	%fd1217, 0d3FF0000000000000;
	fma.rn.f64 	%fd1817, %fd332, %fd331, %fd1217;

BB0_181:
	and.b32  	%r310, %r87, 2;
	setp.eq.s32	%p121, %r310, 0;
	@%p121 bra 	BB0_183;

	mov.f64 	%fd1218, 0d0000000000000000;
	mov.f64 	%fd1219, 0dBFF0000000000000;
	fma.rn.f64 	%fd1817, %fd1817, %fd1219, %fd1218;

BB0_183:
	ld.global.f64 	%fd1818, [%rd2+40];
	abs.f64 	%fd1220, %fd1818;
	setp.neu.f64	%p122, %fd1220, 0d7FF0000000000000;
	@%p122 bra 	BB0_185;

	mov.f64 	%fd1221, 0d0000000000000000;
	mul.rn.f64 	%fd1818, %fd1818, %fd1221;

BB0_185:
	mul.f64 	%fd1222, %fd1818, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r480, %fd1222;
	st.local.u32 	[%rd37], %r480;
	cvt.rn.f64.s32	%fd1223, %r480;
	neg.f64 	%fd1224, %fd1223;
	fma.rn.f64 	%fd1226, %fd1224, %fd647, %fd1818;
	fma.rn.f64 	%fd1228, %fd1224, %fd649, %fd1226;
	fma.rn.f64 	%fd1819, %fd1224, %fd651, %fd1228;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r311}, %fd1818;
	}
	and.b32  	%r312, %r311, 2145386496;
	setp.lt.u32	%p123, %r312, 1105199104;
	@%p123 bra 	BB0_187;

	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1818;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd36;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1819, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r480, [%rd37];

BB0_187:
	add.s32 	%r91, %r480, 1;
	and.b32  	%r313, %r91, 1;
	shl.b32 	%r314, %r313, 3;
	setp.eq.b32	%p124, %r313, 1;
	selp.f64	%fd1230, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p124;
	mul.wide.u32 	%rd178, %r314, 8;
	add.s64 	%rd180, %rd178, %rd41;
	ld.const.f64 	%fd1231, [%rd180+8];
	mul.rn.f64 	%fd344, %fd1819, %fd1819;
	fma.rn.f64 	%fd1232, %fd1230, %fd344, %fd1231;
	ld.const.f64 	%fd1233, [%rd180+16];
	fma.rn.f64 	%fd1234, %fd1232, %fd344, %fd1233;
	ld.const.f64 	%fd1235, [%rd180+24];
	fma.rn.f64 	%fd1236, %fd1234, %fd344, %fd1235;
	ld.const.f64 	%fd1237, [%rd180+32];
	fma.rn.f64 	%fd1238, %fd1236, %fd344, %fd1237;
	ld.const.f64 	%fd1239, [%rd180+40];
	fma.rn.f64 	%fd1240, %fd1238, %fd344, %fd1239;
	ld.const.f64 	%fd1241, [%rd180+48];
	fma.rn.f64 	%fd345, %fd1240, %fd344, %fd1241;
	fma.rn.f64 	%fd1820, %fd345, %fd1819, %fd1819;
	setp.eq.s32	%p125, %r313, 0;
	@%p125 bra 	BB0_189;

	mov.f64 	%fd1242, 0d3FF0000000000000;
	fma.rn.f64 	%fd1820, %fd345, %fd344, %fd1242;

BB0_189:
	and.b32  	%r315, %r91, 2;
	setp.eq.s32	%p126, %r315, 0;
	@%p126 bra 	BB0_191;

	mov.f64 	%fd1243, 0d0000000000000000;
	mov.f64 	%fd1244, 0dBFF0000000000000;
	fma.rn.f64 	%fd1820, %fd1820, %fd1244, %fd1243;

BB0_191:
	ld.global.f64 	%fd1245, [%rd2+32];
	add.f64 	%fd1246, %fd1245, %fd1245;
	mul.f64 	%fd1247, %fd1245, %fd1246;
	mul.f64 	%fd1248, %fd1817, %fd1820;
	div.rn.f64 	%fd1249, %fd1248, %fd1247;
	add.f64 	%fd1723, %fd324, %fd1249;
	ld.global.f64 	%fd352, [%rd5];
	setp.lt.f64	%p127, %fd352, 0d0000000000000000;
	@%p127 bra 	BB0_193;
	bra.uni 	BB0_192;

BB0_193:
	mul.f64 	%fd1251, %fd352, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd5], %fd1251;
	mov.f64 	%fd1730, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1730;
	bra.uni 	BB0_223;

BB0_21:
	setp.lt.f64	%p20, %fd64, 0d0000000000000000;
	@%p20 bra 	BB0_23;
	bra.uni 	BB0_22;

BB0_23:
	mul.f64 	%fd641, %fd64, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd5], %fd641;
	mov.f64 	%fd1727, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1727;
	bra.uni 	BB0_223;

BB0_195:
	setp.lt.f64	%p131, %fd353, 0d0000000000000000;
	@%p131 bra 	BB0_197;
	bra.uni 	BB0_196;

BB0_197:
	mul.f64 	%fd1254, %fd353, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3+48], %fd1254;
	mov.f64 	%fd1732, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1732;
	bra.uni 	BB0_223;

BB0_210:
	neg.f64 	%fd1311, %fd369;
	st.global.f64 	[%rd3], %fd1311;
	bra.uni 	BB0_222;

BB0_22:
	neg.f64 	%fd640, %fd64;
	st.global.f64 	[%rd5], %fd640;
	mov.f64 	%fd1728, %fd62;
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd1728;
	bra.uni 	BB0_223;

BB0_220:
	neg.f64 	%fd1366, %fd384;
	st.global.f64 	[%rd3], %fd1366;

BB0_222:
	mov.f64 	%fd1758, %fd63;
	mov.f64 	%fd1756, %fd63;

BB0_223:
	mov.f64 	%fd1757, %fd1758;
	mov.f64 	%fd1755, %fd1756;
	add.s32 	%r458, %r457, 1;
	setp.lt.s32	%p150, %r457, 6;
	@%p150 bra 	BB0_226;

	mov.u32 	%r458, 0;
	setp.lt.s32	%p151, %r459, 51;
	@%p151 bra 	BB0_226;

	ld.param.f64 	%fd1702, [gaussFitter_param_9];
	sub.f64 	%fd1368, %fd1755, %fd1757;
	setp.lt.f64	%p152, %fd1368, %fd1702;
	selp.b16	%rs8, 0, %rs8, %p152;

BB0_226:
	mov.u32 	%r457, %r458;
	ld.param.u32 	%r451, [gaussFitter_param_10];
	setp.gt.s32	%p153, %r459, %r451;
	selp.b16	%rs8, 0, %rs8, %p153;
	and.b16  	%rs7, %rs8, 255;
	add.s32 	%r459, %r459, 1;
	setp.ne.s16	%p154, %rs7, 0;
	@%p154 bra 	BB0_17;

	ld.global.f64 	%fd1825, [%rd2+40];
	abs.f64 	%fd1369, %fd1825;
	setp.neu.f64	%p155, %fd1369, 0d7FF0000000000000;
	@%p155 bra 	BB0_229;

	mov.f64 	%fd1370, 0d0000000000000000;
	mul.rn.f64 	%fd1825, %fd1825, %fd1370;

BB0_229:
	mul.f64 	%fd1371, %fd1825, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r483, %fd1371;
	add.u64 	%rd197, %SP, 0;
	cvta.to.local.u64 	%rd198, %rd197;
	st.local.u32 	[%rd198], %r483;
	cvt.rn.f64.s32	%fd1372, %r483;
	neg.f64 	%fd1373, %fd1372;
	mov.f64 	%fd1374, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd1375, %fd1373, %fd1374, %fd1825;
	mov.f64 	%fd1376, 0d3C91A62633145C00;
	fma.rn.f64 	%fd1377, %fd1373, %fd1376, %fd1375;
	mov.f64 	%fd1378, 0d397B839A252049C0;
	fma.rn.f64 	%fd1826, %fd1373, %fd1378, %fd1377;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r372}, %fd1825;
	}
	and.b32  	%r373, %r372, 2145386496;
	setp.lt.u32	%p156, %r373, 1105199104;
	@%p156 bra 	BB0_231;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1825;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1826, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r483, [%rd198];

BB0_231:
	add.s32 	%r112, %r483, 1;
	and.b32  	%r374, %r112, 1;
	shl.b32 	%r375, %r374, 3;
	setp.eq.b32	%p157, %r374, 1;
	selp.f64	%fd1379, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p157;
	mul.wide.u32 	%rd201, %r375, 8;
	mov.u64 	%rd202, __cudart_sin_cos_coeffs;
	add.s64 	%rd203, %rd201, %rd202;
	ld.const.f64 	%fd1380, [%rd203+8];
	mul.rn.f64 	%fd396, %fd1826, %fd1826;
	fma.rn.f64 	%fd1381, %fd1379, %fd396, %fd1380;
	ld.const.f64 	%fd1382, [%rd203+16];
	fma.rn.f64 	%fd1383, %fd1381, %fd396, %fd1382;
	ld.const.f64 	%fd1384, [%rd203+24];
	fma.rn.f64 	%fd1385, %fd1383, %fd396, %fd1384;
	ld.const.f64 	%fd1386, [%rd203+32];
	fma.rn.f64 	%fd1387, %fd1385, %fd396, %fd1386;
	ld.const.f64 	%fd1388, [%rd203+40];
	fma.rn.f64 	%fd1389, %fd1387, %fd396, %fd1388;
	ld.const.f64 	%fd1390, [%rd203+48];
	fma.rn.f64 	%fd397, %fd1389, %fd396, %fd1390;
	fma.rn.f64 	%fd1827, %fd397, %fd1826, %fd1826;
	setp.eq.s32	%p158, %r374, 0;
	@%p158 bra 	BB0_233;

	mov.f64 	%fd1391, 0d3FF0000000000000;
	fma.rn.f64 	%fd1827, %fd397, %fd396, %fd1391;

BB0_233:
	and.b32  	%r376, %r112, 2;
	setp.eq.s32	%p159, %r376, 0;
	@%p159 bra 	BB0_235;

	mov.f64 	%fd1392, 0d0000000000000000;
	mov.f64 	%fd1393, 0dBFF0000000000000;
	fma.rn.f64 	%fd1827, %fd1827, %fd1393, %fd1392;

BB0_235:
	ld.global.f64 	%fd1828, [%rd2+40];
	abs.f64 	%fd1394, %fd1828;
	setp.neu.f64	%p160, %fd1394, 0d7FF0000000000000;
	@%p160 bra 	BB0_237;

	mov.f64 	%fd1395, 0d0000000000000000;
	mul.rn.f64 	%fd1828, %fd1828, %fd1395;

BB0_237:
	mul.f64 	%fd1396, %fd1828, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r484, %fd1396;
	st.local.u32 	[%rd198], %r484;
	cvt.rn.f64.s32	%fd1397, %r484;
	neg.f64 	%fd1398, %fd1397;
	fma.rn.f64 	%fd1400, %fd1398, %fd1374, %fd1828;
	fma.rn.f64 	%fd1402, %fd1398, %fd1376, %fd1400;
	fma.rn.f64 	%fd1829, %fd1398, %fd1378, %fd1402;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r377}, %fd1828;
	}
	and.b32  	%r378, %r377, 2145386496;
	setp.lt.u32	%p161, %r378, 1105199104;
	@%p161 bra 	BB0_239;

	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1828;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1829, [retval0+0];
	
	//{
	}// Callseq End 21
	ld.local.u32 	%r484, [%rd198];

BB0_239:
	add.s32 	%r116, %r484, 1;
	and.b32  	%r379, %r116, 1;
	shl.b32 	%r380, %r379, 3;
	setp.eq.b32	%p162, %r379, 1;
	selp.f64	%fd1404, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p162;
	mul.wide.u32 	%rd208, %r380, 8;
	add.s64 	%rd210, %rd208, %rd202;
	ld.const.f64 	%fd1405, [%rd210+8];
	mul.rn.f64 	%fd409, %fd1829, %fd1829;
	fma.rn.f64 	%fd1406, %fd1404, %fd409, %fd1405;
	ld.const.f64 	%fd1407, [%rd210+16];
	fma.rn.f64 	%fd1408, %fd1406, %fd409, %fd1407;
	ld.const.f64 	%fd1409, [%rd210+24];
	fma.rn.f64 	%fd1410, %fd1408, %fd409, %fd1409;
	ld.const.f64 	%fd1411, [%rd210+32];
	fma.rn.f64 	%fd1412, %fd1410, %fd409, %fd1411;
	ld.const.f64 	%fd1413, [%rd210+40];
	fma.rn.f64 	%fd1414, %fd1412, %fd409, %fd1413;
	ld.const.f64 	%fd1415, [%rd210+48];
	fma.rn.f64 	%fd410, %fd1414, %fd409, %fd1415;
	fma.rn.f64 	%fd1830, %fd410, %fd1829, %fd1829;
	setp.eq.s32	%p163, %r379, 0;
	@%p163 bra 	BB0_241;

	mov.f64 	%fd1416, 0d3FF0000000000000;
	fma.rn.f64 	%fd1830, %fd410, %fd409, %fd1416;

BB0_241:
	and.b32  	%r381, %r116, 2;
	setp.eq.s32	%p164, %r381, 0;
	@%p164 bra 	BB0_243;

	mov.f64 	%fd1417, 0d0000000000000000;
	mov.f64 	%fd1418, 0dBFF0000000000000;
	fma.rn.f64 	%fd1830, %fd1830, %fd1418, %fd1417;

BB0_243:
	ld.global.f64 	%fd1419, [%rd2+24];
	add.f64 	%fd1420, %fd1419, %fd1419;
	mul.f64 	%fd1421, %fd1419, %fd1420;
	mul.f64 	%fd1422, %fd1827, %fd1830;
	div.rn.f64 	%fd416, %fd1422, %fd1421;
	ld.global.f64 	%fd1831, [%rd2+40];
	abs.f64 	%fd1423, %fd1831;
	setp.neu.f64	%p165, %fd1423, 0d7FF0000000000000;
	@%p165 bra 	BB0_245;

	mov.f64 	%fd1424, 0d0000000000000000;
	mul.rn.f64 	%fd1831, %fd1831, %fd1424;

BB0_245:
	mul.f64 	%fd1425, %fd1831, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r485, %fd1425;
	st.local.u32 	[%rd198], %r485;
	cvt.rn.f64.s32	%fd1426, %r485;
	neg.f64 	%fd1427, %fd1426;
	fma.rn.f64 	%fd1429, %fd1427, %fd1374, %fd1831;
	fma.rn.f64 	%fd1431, %fd1427, %fd1376, %fd1429;
	fma.rn.f64 	%fd1832, %fd1427, %fd1378, %fd1431;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r382}, %fd1831;
	}
	and.b32  	%r383, %r382, 2145386496;
	setp.lt.u32	%p166, %r383, 1105199104;
	@%p166 bra 	BB0_247;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1831;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1832, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r485, [%rd198];

BB0_247:
	and.b32  	%r384, %r485, 1;
	shl.b32 	%r385, %r384, 3;
	setp.eq.b32	%p167, %r384, 1;
	selp.f64	%fd1433, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p167;
	mul.wide.u32 	%rd215, %r385, 8;
	add.s64 	%rd217, %rd215, %rd202;
	ld.const.f64 	%fd1434, [%rd217+8];
	mul.rn.f64 	%fd423, %fd1832, %fd1832;
	fma.rn.f64 	%fd1435, %fd1433, %fd423, %fd1434;
	ld.const.f64 	%fd1436, [%rd217+16];
	fma.rn.f64 	%fd1437, %fd1435, %fd423, %fd1436;
	ld.const.f64 	%fd1438, [%rd217+24];
	fma.rn.f64 	%fd1439, %fd1437, %fd423, %fd1438;
	ld.const.f64 	%fd1440, [%rd217+32];
	fma.rn.f64 	%fd1441, %fd1439, %fd423, %fd1440;
	ld.const.f64 	%fd1442, [%rd217+40];
	fma.rn.f64 	%fd1443, %fd1441, %fd423, %fd1442;
	ld.const.f64 	%fd1444, [%rd217+48];
	fma.rn.f64 	%fd424, %fd1443, %fd423, %fd1444;
	fma.rn.f64 	%fd1833, %fd424, %fd1832, %fd1832;
	setp.eq.s32	%p168, %r384, 0;
	@%p168 bra 	BB0_249;

	mov.f64 	%fd1445, 0d3FF0000000000000;
	fma.rn.f64 	%fd1833, %fd424, %fd423, %fd1445;

BB0_249:
	and.b32  	%r386, %r485, 2;
	setp.eq.s32	%p169, %r386, 0;
	@%p169 bra 	BB0_251;

	mov.f64 	%fd1446, 0d0000000000000000;
	mov.f64 	%fd1447, 0dBFF0000000000000;
	fma.rn.f64 	%fd1833, %fd1833, %fd1447, %fd1446;

BB0_251:
	ld.global.f64 	%fd1834, [%rd2+40];
	abs.f64 	%fd1448, %fd1834;
	setp.neu.f64	%p170, %fd1448, 0d7FF0000000000000;
	@%p170 bra 	BB0_253;

	mov.f64 	%fd1449, 0d0000000000000000;
	mul.rn.f64 	%fd1834, %fd1834, %fd1449;

BB0_253:
	mul.f64 	%fd1450, %fd1834, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r486, %fd1450;
	st.local.u32 	[%rd198], %r486;
	cvt.rn.f64.s32	%fd1451, %r486;
	neg.f64 	%fd1452, %fd1451;
	fma.rn.f64 	%fd1454, %fd1452, %fd1374, %fd1834;
	fma.rn.f64 	%fd1456, %fd1452, %fd1376, %fd1454;
	fma.rn.f64 	%fd1835, %fd1452, %fd1378, %fd1456;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r387}, %fd1834;
	}
	and.b32  	%r388, %r387, 2145386496;
	setp.lt.u32	%p171, %r388, 1105199104;
	@%p171 bra 	BB0_255;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1834;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1835, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r486, [%rd198];

BB0_255:
	and.b32  	%r389, %r486, 1;
	shl.b32 	%r390, %r389, 3;
	setp.eq.b32	%p172, %r389, 1;
	selp.f64	%fd1458, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p172;
	mul.wide.u32 	%rd222, %r390, 8;
	add.s64 	%rd224, %rd222, %rd202;
	ld.const.f64 	%fd1459, [%rd224+8];
	mul.rn.f64 	%fd436, %fd1835, %fd1835;
	fma.rn.f64 	%fd1460, %fd1458, %fd436, %fd1459;
	ld.const.f64 	%fd1461, [%rd224+16];
	fma.rn.f64 	%fd1462, %fd1460, %fd436, %fd1461;
	ld.const.f64 	%fd1463, [%rd224+24];
	fma.rn.f64 	%fd1464, %fd1462, %fd436, %fd1463;
	ld.const.f64 	%fd1465, [%rd224+32];
	fma.rn.f64 	%fd1466, %fd1464, %fd436, %fd1465;
	ld.const.f64 	%fd1467, [%rd224+40];
	fma.rn.f64 	%fd1468, %fd1466, %fd436, %fd1467;
	ld.const.f64 	%fd1469, [%rd224+48];
	fma.rn.f64 	%fd437, %fd1468, %fd436, %fd1469;
	fma.rn.f64 	%fd1836, %fd437, %fd1835, %fd1835;
	setp.eq.s32	%p173, %r389, 0;
	@%p173 bra 	BB0_257;

	mov.f64 	%fd1470, 0d3FF0000000000000;
	fma.rn.f64 	%fd1836, %fd437, %fd436, %fd1470;

BB0_257:
	and.b32  	%r391, %r486, 2;
	setp.eq.s32	%p174, %r391, 0;
	@%p174 bra 	BB0_259;

	mov.f64 	%fd1471, 0d0000000000000000;
	mov.f64 	%fd1472, 0dBFF0000000000000;
	fma.rn.f64 	%fd1836, %fd1836, %fd1472, %fd1471;

BB0_259:
	ld.global.f64 	%fd1473, [%rd2+32];
	add.f64 	%fd1474, %fd1473, %fd1473;
	mul.f64 	%fd1475, %fd1473, %fd1474;
	mul.f64 	%fd1476, %fd1833, %fd1836;
	div.rn.f64 	%fd1477, %fd1476, %fd1475;
	add.f64 	%fd443, %fd416, %fd1477;
	ld.global.f64 	%fd1478, [%rd2+40];
	add.f64 	%fd1837, %fd1478, %fd1478;
	abs.f64 	%fd1479, %fd1837;
	setp.neu.f64	%p175, %fd1479, 0d7FF0000000000000;
	@%p175 bra 	BB0_261;

	mov.f64 	%fd1480, 0d0000000000000000;
	mul.rn.f64 	%fd1837, %fd1837, %fd1480;

BB0_261:
	mul.f64 	%fd1481, %fd1837, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r487, %fd1481;
	st.local.u32 	[%rd198], %r487;
	cvt.rn.f64.s32	%fd1482, %r487;
	neg.f64 	%fd1483, %fd1482;
	fma.rn.f64 	%fd1485, %fd1483, %fd1374, %fd1837;
	fma.rn.f64 	%fd1487, %fd1483, %fd1376, %fd1485;
	fma.rn.f64 	%fd1838, %fd1483, %fd1378, %fd1487;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd1837;
	}
	and.b32  	%r393, %r392, 2145386496;
	setp.lt.u32	%p176, %r393, 1105199104;
	@%p176 bra 	BB0_263;

	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1837;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1838, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r487, [%rd198];

BB0_263:
	and.b32  	%r394, %r487, 1;
	shl.b32 	%r395, %r394, 3;
	setp.eq.b32	%p177, %r394, 1;
	selp.f64	%fd1489, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p177;
	mul.wide.u32 	%rd229, %r395, 8;
	add.s64 	%rd231, %rd229, %rd202;
	ld.const.f64 	%fd1490, [%rd231+8];
	mul.rn.f64 	%fd450, %fd1838, %fd1838;
	fma.rn.f64 	%fd1491, %fd1489, %fd450, %fd1490;
	ld.const.f64 	%fd1492, [%rd231+16];
	fma.rn.f64 	%fd1493, %fd1491, %fd450, %fd1492;
	ld.const.f64 	%fd1494, [%rd231+24];
	fma.rn.f64 	%fd1495, %fd1493, %fd450, %fd1494;
	ld.const.f64 	%fd1496, [%rd231+32];
	fma.rn.f64 	%fd1497, %fd1495, %fd450, %fd1496;
	ld.const.f64 	%fd1498, [%rd231+40];
	fma.rn.f64 	%fd1499, %fd1497, %fd450, %fd1498;
	ld.const.f64 	%fd1500, [%rd231+48];
	fma.rn.f64 	%fd451, %fd1499, %fd450, %fd1500;
	fma.rn.f64 	%fd1839, %fd451, %fd1838, %fd1838;
	setp.eq.s32	%p178, %r394, 0;
	@%p178 bra 	BB0_265;

	mov.f64 	%fd1501, 0d3FF0000000000000;
	fma.rn.f64 	%fd1839, %fd451, %fd450, %fd1501;

BB0_265:
	and.b32  	%r396, %r487, 2;
	setp.eq.s32	%p179, %r396, 0;
	@%p179 bra 	BB0_267;

	mov.f64 	%fd1502, 0d0000000000000000;
	mov.f64 	%fd1503, 0dBFF0000000000000;
	fma.rn.f64 	%fd1839, %fd1839, %fd1503, %fd1502;

BB0_267:
	ld.global.f64 	%fd1504, [%rd2+24];
	mul.f64 	%fd1505, %fd1504, 0dC010000000000000;
	mul.f64 	%fd1506, %fd1504, %fd1505;
	div.rn.f64 	%fd457, %fd1839, %fd1506;
	ld.global.f64 	%fd1507, [%rd2+40];
	add.f64 	%fd1840, %fd1507, %fd1507;
	abs.f64 	%fd1508, %fd1840;
	setp.neu.f64	%p180, %fd1508, 0d7FF0000000000000;
	@%p180 bra 	BB0_269;

	mov.f64 	%fd1509, 0d0000000000000000;
	mul.rn.f64 	%fd1840, %fd1840, %fd1509;

BB0_269:
	mul.f64 	%fd1510, %fd1840, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r488, %fd1510;
	st.local.u32 	[%rd198], %r488;
	cvt.rn.f64.s32	%fd1511, %r488;
	neg.f64 	%fd1512, %fd1511;
	fma.rn.f64 	%fd1514, %fd1512, %fd1374, %fd1840;
	fma.rn.f64 	%fd1516, %fd1512, %fd1376, %fd1514;
	fma.rn.f64 	%fd1841, %fd1512, %fd1378, %fd1516;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r397}, %fd1840;
	}
	and.b32  	%r398, %r397, 2145386496;
	setp.lt.u32	%p181, %r398, 1105199104;
	@%p181 bra 	BB0_271;

	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1840;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1841, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r488, [%rd198];

BB0_271:
	and.b32  	%r399, %r488, 1;
	shl.b32 	%r400, %r399, 3;
	setp.eq.b32	%p182, %r399, 1;
	selp.f64	%fd1518, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p182;
	mul.wide.u32 	%rd236, %r400, 8;
	add.s64 	%rd238, %rd236, %rd202;
	ld.const.f64 	%fd1519, [%rd238+8];
	mul.rn.f64 	%fd464, %fd1841, %fd1841;
	fma.rn.f64 	%fd1520, %fd1518, %fd464, %fd1519;
	ld.const.f64 	%fd1521, [%rd238+16];
	fma.rn.f64 	%fd1522, %fd1520, %fd464, %fd1521;
	ld.const.f64 	%fd1523, [%rd238+24];
	fma.rn.f64 	%fd1524, %fd1522, %fd464, %fd1523;
	ld.const.f64 	%fd1525, [%rd238+32];
	fma.rn.f64 	%fd1526, %fd1524, %fd464, %fd1525;
	ld.const.f64 	%fd1527, [%rd238+40];
	fma.rn.f64 	%fd1528, %fd1526, %fd464, %fd1527;
	ld.const.f64 	%fd1529, [%rd238+48];
	fma.rn.f64 	%fd465, %fd1528, %fd464, %fd1529;
	fma.rn.f64 	%fd1842, %fd465, %fd1841, %fd1841;
	setp.eq.s32	%p183, %r399, 0;
	@%p183 bra 	BB0_273;

	mov.f64 	%fd1530, 0d3FF0000000000000;
	fma.rn.f64 	%fd1842, %fd465, %fd464, %fd1530;

BB0_273:
	and.b32  	%r401, %r488, 2;
	setp.eq.s32	%p184, %r401, 0;
	@%p184 bra 	BB0_275;

	mov.f64 	%fd1531, 0d0000000000000000;
	mov.f64 	%fd1532, 0dBFF0000000000000;
	fma.rn.f64 	%fd1842, %fd1842, %fd1532, %fd1531;

BB0_275:
	ld.global.f64 	%fd1533, [%rd2+32];
	mul.f64 	%fd1534, %fd1533, 0d4010000000000000;
	mul.f64 	%fd1535, %fd1533, %fd1534;
	div.rn.f64 	%fd1536, %fd1842, %fd1535;
	add.f64 	%fd471, %fd457, %fd1536;
	ld.global.f64 	%fd1843, [%rd2+40];
	abs.f64 	%fd1537, %fd1843;
	setp.neu.f64	%p185, %fd1537, 0d7FF0000000000000;
	@%p185 bra 	BB0_277;

	mov.f64 	%fd1538, 0d0000000000000000;
	mul.rn.f64 	%fd1843, %fd1843, %fd1538;

BB0_277:
	mul.f64 	%fd1539, %fd1843, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r489, %fd1539;
	st.local.u32 	[%rd198], %r489;
	cvt.rn.f64.s32	%fd1540, %r489;
	neg.f64 	%fd1541, %fd1540;
	fma.rn.f64 	%fd1543, %fd1541, %fd1374, %fd1843;
	fma.rn.f64 	%fd1545, %fd1541, %fd1376, %fd1543;
	fma.rn.f64 	%fd1844, %fd1541, %fd1378, %fd1545;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r402}, %fd1843;
	}
	and.b32  	%r403, %r402, 2145386496;
	setp.lt.u32	%p186, %r403, 1105199104;
	@%p186 bra 	BB0_279;

	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1843;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1844, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r489, [%rd198];

BB0_279:
	and.b32  	%r404, %r489, 1;
	shl.b32 	%r405, %r404, 3;
	setp.eq.b32	%p187, %r404, 1;
	selp.f64	%fd1547, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p187;
	mul.wide.u32 	%rd243, %r405, 8;
	add.s64 	%rd245, %rd243, %rd202;
	ld.const.f64 	%fd1548, [%rd245+8];
	mul.rn.f64 	%fd478, %fd1844, %fd1844;
	fma.rn.f64 	%fd1549, %fd1547, %fd478, %fd1548;
	ld.const.f64 	%fd1550, [%rd245+16];
	fma.rn.f64 	%fd1551, %fd1549, %fd478, %fd1550;
	ld.const.f64 	%fd1552, [%rd245+24];
	fma.rn.f64 	%fd1553, %fd1551, %fd478, %fd1552;
	ld.const.f64 	%fd1554, [%rd245+32];
	fma.rn.f64 	%fd1555, %fd1553, %fd478, %fd1554;
	ld.const.f64 	%fd1556, [%rd245+40];
	fma.rn.f64 	%fd1557, %fd1555, %fd478, %fd1556;
	ld.const.f64 	%fd1558, [%rd245+48];
	fma.rn.f64 	%fd479, %fd1557, %fd478, %fd1558;
	fma.rn.f64 	%fd1845, %fd479, %fd1844, %fd1844;
	setp.eq.s32	%p188, %r404, 0;
	@%p188 bra 	BB0_281;

	mov.f64 	%fd1559, 0d3FF0000000000000;
	fma.rn.f64 	%fd1845, %fd479, %fd478, %fd1559;

BB0_281:
	and.b32  	%r406, %r489, 2;
	setp.eq.s32	%p189, %r406, 0;
	@%p189 bra 	BB0_283;

	mov.f64 	%fd1560, 0d0000000000000000;
	mov.f64 	%fd1561, 0dBFF0000000000000;
	fma.rn.f64 	%fd1845, %fd1845, %fd1561, %fd1560;

BB0_283:
	ld.global.f64 	%fd1846, [%rd2+40];
	abs.f64 	%fd1562, %fd1846;
	setp.neu.f64	%p190, %fd1562, 0d7FF0000000000000;
	@%p190 bra 	BB0_285;

	mov.f64 	%fd1563, 0d0000000000000000;
	mul.rn.f64 	%fd1846, %fd1846, %fd1563;

BB0_285:
	mul.f64 	%fd1564, %fd1846, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r490, %fd1564;
	st.local.u32 	[%rd198], %r490;
	cvt.rn.f64.s32	%fd1565, %r490;
	neg.f64 	%fd1566, %fd1565;
	fma.rn.f64 	%fd1568, %fd1566, %fd1374, %fd1846;
	fma.rn.f64 	%fd1570, %fd1566, %fd1376, %fd1568;
	fma.rn.f64 	%fd1847, %fd1566, %fd1378, %fd1570;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r407}, %fd1846;
	}
	and.b32  	%r408, %r407, 2145386496;
	setp.lt.u32	%p191, %r408, 1105199104;
	@%p191 bra 	BB0_287;

	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1846;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1847, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r490, [%rd198];

BB0_287:
	and.b32  	%r409, %r490, 1;
	shl.b32 	%r410, %r409, 3;
	setp.eq.b32	%p192, %r409, 1;
	selp.f64	%fd1572, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p192;
	mul.wide.u32 	%rd250, %r410, 8;
	add.s64 	%rd252, %rd250, %rd202;
	ld.const.f64 	%fd1573, [%rd252+8];
	mul.rn.f64 	%fd491, %fd1847, %fd1847;
	fma.rn.f64 	%fd1574, %fd1572, %fd491, %fd1573;
	ld.const.f64 	%fd1575, [%rd252+16];
	fma.rn.f64 	%fd1576, %fd1574, %fd491, %fd1575;
	ld.const.f64 	%fd1577, [%rd252+24];
	fma.rn.f64 	%fd1578, %fd1576, %fd491, %fd1577;
	ld.const.f64 	%fd1579, [%rd252+32];
	fma.rn.f64 	%fd1580, %fd1578, %fd491, %fd1579;
	ld.const.f64 	%fd1581, [%rd252+40];
	fma.rn.f64 	%fd1582, %fd1580, %fd491, %fd1581;
	ld.const.f64 	%fd1583, [%rd252+48];
	fma.rn.f64 	%fd492, %fd1582, %fd491, %fd1583;
	fma.rn.f64 	%fd1848, %fd492, %fd1847, %fd1847;
	setp.eq.s32	%p193, %r409, 0;
	@%p193 bra 	BB0_289;

	mov.f64 	%fd1584, 0d3FF0000000000000;
	fma.rn.f64 	%fd1848, %fd492, %fd491, %fd1584;

BB0_289:
	and.b32  	%r411, %r490, 2;
	setp.eq.s32	%p194, %r411, 0;
	@%p194 bra 	BB0_291;

	mov.f64 	%fd1585, 0d0000000000000000;
	mov.f64 	%fd1586, 0dBFF0000000000000;
	fma.rn.f64 	%fd1848, %fd1848, %fd1586, %fd1585;

BB0_291:
	ld.global.f64 	%fd1587, [%rd2+24];
	add.f64 	%fd1588, %fd1587, %fd1587;
	mul.f64 	%fd1589, %fd1587, %fd1588;
	mul.f64 	%fd1590, %fd1845, %fd1848;
	div.rn.f64 	%fd498, %fd1590, %fd1589;
	ld.global.f64 	%fd1849, [%rd2+40];
	abs.f64 	%fd1591, %fd1849;
	setp.neu.f64	%p195, %fd1591, 0d7FF0000000000000;
	@%p195 bra 	BB0_293;

	mov.f64 	%fd1592, 0d0000000000000000;
	mul.rn.f64 	%fd1849, %fd1849, %fd1592;

BB0_293:
	mul.f64 	%fd1593, %fd1849, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r491, %fd1593;
	st.local.u32 	[%rd198], %r491;
	cvt.rn.f64.s32	%fd1594, %r491;
	neg.f64 	%fd1595, %fd1594;
	fma.rn.f64 	%fd1597, %fd1595, %fd1374, %fd1849;
	fma.rn.f64 	%fd1599, %fd1595, %fd1376, %fd1597;
	fma.rn.f64 	%fd1850, %fd1595, %fd1378, %fd1599;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r412}, %fd1849;
	}
	and.b32  	%r413, %r412, 2145386496;
	setp.lt.u32	%p196, %r413, 1105199104;
	@%p196 bra 	BB0_295;

	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1849;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1850, [retval0+0];
	
	//{
	}// Callseq End 28
	ld.local.u32 	%r491, [%rd198];

BB0_295:
	add.s32 	%r138, %r491, 1;
	and.b32  	%r414, %r138, 1;
	shl.b32 	%r415, %r414, 3;
	setp.eq.b32	%p197, %r414, 1;
	selp.f64	%fd1601, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p197;
	mul.wide.u32 	%rd257, %r415, 8;
	add.s64 	%rd259, %rd257, %rd202;
	ld.const.f64 	%fd1602, [%rd259+8];
	mul.rn.f64 	%fd505, %fd1850, %fd1850;
	fma.rn.f64 	%fd1603, %fd1601, %fd505, %fd1602;
	ld.const.f64 	%fd1604, [%rd259+16];
	fma.rn.f64 	%fd1605, %fd1603, %fd505, %fd1604;
	ld.const.f64 	%fd1606, [%rd259+24];
	fma.rn.f64 	%fd1607, %fd1605, %fd505, %fd1606;
	ld.const.f64 	%fd1608, [%rd259+32];
	fma.rn.f64 	%fd1609, %fd1607, %fd505, %fd1608;
	ld.const.f64 	%fd1610, [%rd259+40];
	fma.rn.f64 	%fd1611, %fd1609, %fd505, %fd1610;
	ld.const.f64 	%fd1612, [%rd259+48];
	fma.rn.f64 	%fd506, %fd1611, %fd505, %fd1612;
	fma.rn.f64 	%fd1851, %fd506, %fd1850, %fd1850;
	setp.eq.s32	%p198, %r414, 0;
	@%p198 bra 	BB0_297;

	mov.f64 	%fd1613, 0d3FF0000000000000;
	fma.rn.f64 	%fd1851, %fd506, %fd505, %fd1613;

BB0_297:
	and.b32  	%r416, %r138, 2;
	setp.eq.s32	%p199, %r416, 0;
	@%p199 bra 	BB0_299;

	mov.f64 	%fd1614, 0d0000000000000000;
	mov.f64 	%fd1615, 0dBFF0000000000000;
	fma.rn.f64 	%fd1851, %fd1851, %fd1615, %fd1614;

BB0_299:
	ld.global.f64 	%fd1852, [%rd2+40];
	abs.f64 	%fd1616, %fd1852;
	setp.neu.f64	%p200, %fd1616, 0d7FF0000000000000;
	@%p200 bra 	BB0_301;

	mov.f64 	%fd1617, 0d0000000000000000;
	mul.rn.f64 	%fd1852, %fd1852, %fd1617;

BB0_301:
	mul.f64 	%fd1618, %fd1852, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r492, %fd1618;
	st.local.u32 	[%rd198], %r492;
	cvt.rn.f64.s32	%fd1619, %r492;
	neg.f64 	%fd1620, %fd1619;
	fma.rn.f64 	%fd1622, %fd1620, %fd1374, %fd1852;
	fma.rn.f64 	%fd1624, %fd1620, %fd1376, %fd1622;
	fma.rn.f64 	%fd1853, %fd1620, %fd1378, %fd1624;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r417}, %fd1852;
	}
	and.b32  	%r418, %r417, 2145386496;
	setp.lt.u32	%p201, %r418, 1105199104;
	@%p201 bra 	BB0_303;

	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1852;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd197;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1853, [retval0+0];
	
	//{
	}// Callseq End 29
	ld.local.u32 	%r492, [%rd198];

BB0_303:
	add.s32 	%r142, %r492, 1;
	and.b32  	%r419, %r142, 1;
	shl.b32 	%r420, %r419, 3;
	setp.eq.b32	%p202, %r419, 1;
	selp.f64	%fd1626, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p202;
	mul.wide.u32 	%rd264, %r420, 8;
	add.s64 	%rd266, %rd264, %rd202;
	ld.const.f64 	%fd1627, [%rd266+8];
	mul.rn.f64 	%fd518, %fd1853, %fd1853;
	fma.rn.f64 	%fd1628, %fd1626, %fd518, %fd1627;
	ld.const.f64 	%fd1629, [%rd266+16];
	fma.rn.f64 	%fd1630, %fd1628, %fd518, %fd1629;
	ld.const.f64 	%fd1631, [%rd266+24];
	fma.rn.f64 	%fd1632, %fd1630, %fd518, %fd1631;
	ld.const.f64 	%fd1633, [%rd266+32];
	fma.rn.f64 	%fd1634, %fd1632, %fd518, %fd1633;
	ld.const.f64 	%fd1635, [%rd266+40];
	fma.rn.f64 	%fd1636, %fd1634, %fd518, %fd1635;
	ld.const.f64 	%fd1637, [%rd266+48];
	fma.rn.f64 	%fd519, %fd1636, %fd518, %fd1637;
	fma.rn.f64 	%fd1854, %fd519, %fd1853, %fd1853;
	setp.eq.s32	%p203, %r419, 0;
	@%p203 bra 	BB0_305;

	mov.f64 	%fd1638, 0d3FF0000000000000;
	fma.rn.f64 	%fd1854, %fd519, %fd518, %fd1638;

BB0_305:
	and.b32  	%r421, %r142, 2;
	setp.eq.s32	%p204, %r421, 0;
	@%p204 bra 	BB0_307;

	mov.f64 	%fd1639, 0d0000000000000000;
	mov.f64 	%fd1640, 0dBFF0000000000000;
	fma.rn.f64 	%fd1854, %fd1854, %fd1640, %fd1639;

BB0_307:
	ld.global.f64 	%fd1643, [%rd2+32];
	add.f64 	%fd1644, %fd1643, %fd1643;
	mul.f64 	%fd1645, %fd1643, %fd1644;
	mul.f64 	%fd1646, %fd1851, %fd1854;
	div.rn.f64 	%fd1647, %fd1646, %fd1645;
	add.f64 	%fd525, %fd498, %fd1647;
	mov.f64 	%fd1857, 0d0000000000000000;
	mov.f64 	%fd1856, %fd1857;
	@%p2 bra 	BB0_313;

	ld.global.f64 	%fd526, [%rd2];
	ld.global.f64 	%fd527, [%rd2+8];
	add.f64 	%fd528, %fd471, %fd471;
	ld.global.f64 	%fd529, [%rd2+16];
	ld.global.f64 	%fd530, [%rd2+48];
	mul.lo.s32 	%r429, %r1, %r2;
	mul.lo.s32 	%r430, %r429, %r2;
	mul.wide.s32 	%rd271, %r430, 4;
	add.s64 	%rd275, %rd1, %rd271;
	mov.f64 	%fd1857, 0d0000000000000000;
	mov.u32 	%r493, 0;
	mov.f64 	%fd1856, %fd1857;

BB0_309:
	rem.s32 	%r431, %r493, %r2;
	cvt.rn.f64.s32	%fd1650, %r431;
	sub.f64 	%fd1651, %fd1650, %fd527;
	mul.f64 	%fd1652, %fd443, %fd1651;
	mul.f64 	%fd1653, %fd1651, %fd1652;
	mul.f64 	%fd1654, %fd528, %fd1651;
	div.s32 	%r432, %r493, %r2;
	cvt.rn.f64.s32	%fd1655, %r432;
	sub.f64 	%fd1656, %fd1655, %fd529;
	mul.f64 	%fd1657, %fd1654, %fd1656;
	sub.f64 	%fd1658, %fd1653, %fd1657;
	mul.f64 	%fd1659, %fd525, %fd1656;
	fma.rn.f64 	%fd533, %fd1656, %fd1659, %fd1658;
	neg.f64 	%fd1660, %fd533;
	mov.f64 	%fd1661, 0d4338000000000000;
	mov.f64 	%fd1662, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1663, %fd1660, %fd1662, %fd1661;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r146, %temp}, %fd1663;
	}
	mov.f64 	%fd1664, 0dC338000000000000;
	add.rn.f64 	%fd1665, %fd1663, %fd1664;
	mov.f64 	%fd1666, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1667, %fd1665, %fd1666, %fd1660;
	mov.f64 	%fd1668, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1669, %fd1665, %fd1668, %fd1667;
	mov.f64 	%fd1670, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1671, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1672, %fd1671, %fd1669, %fd1670;
	mov.f64 	%fd1673, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1674, %fd1672, %fd1669, %fd1673;
	mov.f64 	%fd1675, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1676, %fd1674, %fd1669, %fd1675;
	mov.f64 	%fd1677, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1678, %fd1676, %fd1669, %fd1677;
	mov.f64 	%fd1679, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1680, %fd1678, %fd1669, %fd1679;
	mov.f64 	%fd1681, 0d3F81111111122322;
	fma.rn.f64 	%fd1682, %fd1680, %fd1669, %fd1681;
	mov.f64 	%fd1683, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1684, %fd1682, %fd1669, %fd1683;
	mov.f64 	%fd1685, 0d3FC5555555555511;
	fma.rn.f64 	%fd1686, %fd1684, %fd1669, %fd1685;
	mov.f64 	%fd1687, 0d3FE000000000000B;
	fma.rn.f64 	%fd1688, %fd1686, %fd1669, %fd1687;
	mov.f64 	%fd1689, 0d3FF0000000000000;
	fma.rn.f64 	%fd1690, %fd1688, %fd1669, %fd1689;
	fma.rn.f64 	%fd1691, %fd1690, %fd1669, %fd1689;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r147, %temp}, %fd1691;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r148}, %fd1691;
	}
	shl.b32 	%r433, %r146, 20;
	add.s32 	%r434, %r148, %r433;
	mov.b64 	%fd1855, {%r147, %r434};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r435}, %fd1660;
	}
	mov.b32 	 %f10, %r435;
	abs.f32 	%f5, %f10;
	setp.lt.f32	%p206, %f5, 0f4086232B;
	@%p206 bra 	BB0_312;

	setp.gt.f64	%p207, %fd533, 0d8000000000000000;
	mov.f64 	%fd1692, 0d7FF0000000000000;
	sub.f64 	%fd1693, %fd1692, %fd533;
	selp.f64	%fd1855, 0d0000000000000000, %fd1693, %p207;
	setp.geu.f32	%p208, %f5, 0f40874800;
	@%p208 bra 	BB0_312;

	shr.u32 	%r436, %r146, 31;
	add.s32 	%r437, %r146, %r436;
	shr.s32 	%r438, %r437, 1;
	shl.b32 	%r439, %r438, 20;
	add.s32 	%r440, %r439, %r148;
	mov.b64 	%fd1694, {%r147, %r440};
	sub.s32 	%r441, %r146, %r438;
	shl.b32 	%r442, %r441, 20;
	add.s32 	%r443, %r442, 1072693248;
	mov.u32 	%r444, 0;
	mov.b64 	%fd1695, {%r444, %r443};
	mul.f64 	%fd1855, %fd1694, %fd1695;

BB0_312:
	fma.rn.f64 	%fd1696, %fd526, %fd1855, %fd530;
	add.f64 	%fd1856, %fd1856, %fd1696;
	ld.global.u32 	%r445, [%rd275];
	cvt.rn.f64.s32	%fd1697, %r445;
	sub.f64 	%fd1698, %fd1696, %fd1697;
	fma.rn.f64 	%fd1857, %fd1698, %fd1698, %fd1857;
	add.s64 	%rd275, %rd275, 4;
	add.s32 	%r493, %r493, 1;
	setp.lt.s32	%p209, %r493, %r3;
	@%p209 bra 	BB0_309;

BB0_313:
	st.global.f64 	[%rd2], %fd1856;
	div.rn.f64 	%fd1699, %fd1857, %fd1714;
	mov.f64 	%fd1700, 0d3FF0000000000000;
	sub.f64 	%fd1701, %fd1700, %fd1699;
	st.global.f64 	[%rd2+48], %fd1701;

BB0_314:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot1;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB1_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB1_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd92, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	neg.s32 	%r25, %r24;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd90, %rd45, 120;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r7;

BB1_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r39;
	mov.u64 	%rd7, %rd91;
	ld.const.u64 	%rd48, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd48;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd46, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd92], %rd46;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r26, %r9, %r7;
	mul.wide.s32 	%rd51, %r26, 8;
	add.s64 	%rd92, %rd1, %rd51;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd93, %rd13;
	add.s64 	%rd90, %rd90, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd91, %rd13;
	mov.u32 	%r39, %r9;
	@%p3 bra 	BB1_3;

BB1_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB1_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r10;
	shl.b64 	%rd52, %rd96, %r10;
	shr.u64 	%rd53, %rd95, %r28;
	or.b64  	%rd96, %rd52, %rd53;
	shl.b64 	%rd54, %rd95, %r10;
	ld.local.u64 	%rd55, [%rd1+8];
	shr.u64 	%rd56, %rd55, %r28;
	or.b64  	%rd95, %rd56, %rd54;

BB1_6:
	cvta.to.local.u64 	%rd57, %rd37;
	shr.u64 	%rd58, %rd96, 62;
	cvt.u32.u64	%r29, %rd58;
	shr.u64 	%rd59, %rd95, 62;
	shl.b64 	%rd60, %rd96, 2;
	or.b64  	%rd98, %rd60, %rd59;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd61, %rd96, 61;
	cvt.u32.u64	%r30, %rd61;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.ne.s32	%p5, %r40, 0;
	selp.b32	%r34, %r33, %r32, %p5;
	st.local.u32 	[%rd57], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB1_8;

	mov.u64 	%rd65, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd65;
	mov.b64         {a2,a3}, %rd65;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB1_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB1_10;

	shl.b64 	%rd68, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd69, %rd97, %r36;
	or.b64  	%rd98, %rd69, %rd68;

BB1_10:
	mov.u64 	%rd73, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd73;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd70, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB1_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd70;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd70;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd74, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB1_12:
	cvt.u64.u32	%rd80, %r40;
	shl.b64 	%rd81, %rd80, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd82, %r38;
	shl.b64 	%rd83, %rd82, 52;
	add.s64 	%rd84, %rd99, 1;
	shr.u64 	%rd85, %rd84, 10;
	add.s64 	%rd86, %rd85, 1;
	shr.u64 	%rd87, %rd86, 1;
	add.s64 	%rd88, %rd87, %rd83;
	or.b64  	%rd89, %rd88, %rd81;
	mov.b64 	 %fd4, %rd89;

BB1_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


