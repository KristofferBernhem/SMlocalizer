//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-20732876
// Cuda compilation tools, release 8.0, V8.0.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	gaussFitter
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry gaussFitter(
	.param .u64 gaussFitter_param_0,
	.param .u32 gaussFitter_param_1,
	.param .u64 gaussFitter_param_2,
	.param .u32 gaussFitter_param_3,
	.param .u16 gaussFitter_param_4,
	.param .u64 gaussFitter_param_5,
	.param .u32 gaussFitter_param_6,
	.param .u64 gaussFitter_param_7,
	.param .u32 gaussFitter_param_8,
	.param .f64 gaussFitter_param_9,
	.param .u32 gaussFitter_param_10
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<216>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<513>;
	.reg .f64 	%fd<1873>;
	.reg .b64 	%rd<282>;


	mov.u64 	%rd281, __local_depot0;
	cvta.local.u64 	%SP, %rd281;
	ld.param.u64 	%rd11, [gaussFitter_param_0];
	ld.param.u32 	%r149, [gaussFitter_param_1];
	ld.param.u64 	%rd12, [gaussFitter_param_2];
	ld.param.u16 	%rs5, [gaussFitter_param_4];
	ld.param.u64 	%rd13, [gaussFitter_param_5];
	ld.param.u64 	%rd14, [gaussFitter_param_7];
	cvta.to.global.u64 	%rd1, %rd11;
	mov.u32 	%r150, %nctaid.x;
	mov.u32 	%r151, %ctaid.y;
	mov.u32 	%r152, %ctaid.x;
	mad.lo.s32 	%r1, %r150, %r151, %r152;
	cvt.u32.u16	%r2, %rs5;
	mul.wide.u16 	%r3, %rs5, %rs5;
	div.s32 	%r153, %r149, %r3;
	setp.ge.s32	%p1, %r1, %r153;
	@%p1 bra 	BB0_328;

	mul.lo.s32 	%r4, %r1, 7;
	mul.lo.s32 	%r5, %r1, %r3;
	setp.eq.s32	%p2, %r3, 0;
	mov.f64 	%fd1707, 0d0000000000000000;
	mov.f64 	%fd1704, %fd1707;
	mov.f64 	%fd1701, %fd1707;
	mov.u32 	%r471, 0;
	mov.f64 	%fd1706, %fd1707;
	mov.f64 	%fd1703, %fd1707;
	mov.f64 	%fd1700, %fd1707;
	@%p2 bra 	BB0_3;

BB0_2:
	add.s32 	%r155, %r471, %r5;
	mul.wide.s32 	%rd15, %r155, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ldu.global.u32 	%r156, [%rd16];
	cvt.rn.f64.s32	%fd540, %r156;
	add.f64 	%fd1701, %fd1701, %fd540;
	rem.s32 	%r157, %r471, %r2;
	mul.lo.s32 	%r158, %r157, %r156;
	cvt.rn.f64.s32	%fd541, %r158;
	add.f64 	%fd1707, %fd1707, %fd541;
	div.s32 	%r159, %r471, %r2;
	mul.lo.s32 	%r160, %r156, %r159;
	cvt.rn.f64.s32	%fd542, %r160;
	add.f64 	%fd1704, %fd1704, %fd542;
	add.s32 	%r471, %r471, 1;
	setp.lt.s32	%p3, %r471, %r3;
	mov.f64 	%fd1700, %fd1701;
	mov.f64 	%fd1703, %fd1704;
	mov.f64 	%fd1706, %fd1707;
	@%p3 bra 	BB0_2;

BB0_3:
	cvta.to.global.u64 	%rd17, %rd12;
	mul.wide.s32 	%rd18, %r4, 8;
	add.s64 	%rd2, %rd17, %rd18;
	div.rn.f64 	%fd545, %fd1706, %fd1700;
	st.global.f64 	[%rd2+8], %fd545;
	div.rn.f64 	%fd546, %fd1703, %fd1700;
	st.global.f64 	[%rd2+16], %fd546;
	cvt.rn.f64.s32	%fd547, %r3;
	div.rn.f64 	%fd10, %fd1700, %fd547;
	mov.f64 	%fd1710, 0d0000000000000000;
	mov.u32 	%r472, 0;
	mov.f64 	%fd1709, %fd1710;
	@%p2 bra 	BB0_5;

BB0_4:
	add.s32 	%r162, %r472, %r5;
	mul.wide.s32 	%rd19, %r162, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r163, [%rd20];
	cvt.rn.f64.s32	%fd548, %r163;
	sub.f64 	%fd549, %fd548, %fd10;
	fma.rn.f64 	%fd1710, %fd549, %fd549, %fd1710;
	add.s32 	%r472, %r472, 1;
	setp.lt.s32	%p5, %r472, %r3;
	mov.f64 	%fd1709, %fd1710;
	@%p5 bra 	BB0_4;

BB0_5:
	cvta.to.global.u64 	%rd21, %rd14;
	cvta.to.global.u64 	%rd22, %rd13;
	ld.global.f64 	%fd550, [%rd22];
	ld.global.f64 	%fd551, [%rd2];
	mul.f64 	%fd14, %fd551, %fd550;
	ld.global.f64 	%fd552, [%rd22+8];
	mul.f64 	%fd15, %fd551, %fd552;
	ld.global.f64 	%fd553, [%rd22+96];
	mul.f64 	%fd16, %fd551, %fd553;
	ld.global.f64 	%fd554, [%rd22+104];
	mul.f64 	%fd17, %fd551, %fd554;
	add.s64 	%rd3, %rd21, %rd18;
	ld.global.f64 	%fd555, [%rd3];
	mul.f64 	%fd556, %fd551, %fd555;
	ld.global.f64 	%fd557, [%rd3+48];
	st.global.f64 	[%rd3], %fd556;
	ld.global.f64 	%fd558, [%rd2];
	mul.f64 	%fd559, %fd558, %fd557;
	st.global.f64 	[%rd3+48], %fd559;
	ld.global.f64 	%fd560, [%rd22+56];
	ld.global.f64 	%fd1711, [%rd22+48];
	setp.gtu.f64	%p6, %fd1711, %fd560;
	@%p6 bra 	BB0_18;

	ld.global.f64 	%fd1712, [%rd22+72];
	mov.f64 	%fd1723, 0d3FF0000000000000;

BB0_7:
	mov.f64 	%fd1717, %fd1723;
	mov.f64 	%fd1720, %fd1717;
	add.f64 	%fd562, %fd1711, %fd1711;
	mul.f64 	%fd563, %fd1711, %fd562;
	rcp.rn.f64 	%fd23, %fd563;
	ld.global.f64 	%fd1713, [%rd22+64];
	setp.gtu.f64	%p7, %fd1713, %fd1712;
	mov.f64 	%fd1722, %fd1720;
	@%p7 bra 	BB0_17;

BB0_8:
	mov.f64 	%fd1714, %fd1720;
	mov.f64 	%fd27, %fd1714;
	mov.f64 	%fd1725, 0d0000000000000000;
	@%p2 bra 	BB0_14;

	add.f64 	%fd566, %fd1713, %fd1713;
	mul.f64 	%fd567, %fd1713, %fd566;
	rcp.rn.f64 	%fd28, %fd567;
	ld.global.f64 	%fd29, [%rd2];
	ld.global.f64 	%fd30, [%rd2+8];
	ld.global.f64 	%fd31, [%rd2+16];
	mov.f64 	%fd1725, 0d0000000000000000;
	mov.u32 	%r473, 0;

BB0_10:
	rem.s32 	%r172, %r473, %r2;
	div.s32 	%r173, %r473, %r2;
	cvt.rn.f64.s32	%fd568, %r172;
	sub.f64 	%fd569, %fd568, %fd30;
	mul.f64 	%fd570, %fd23, %fd569;
	cvt.rn.f64.s32	%fd571, %r173;
	sub.f64 	%fd572, %fd571, %fd31;
	mul.f64 	%fd573, %fd28, %fd572;
	mul.f64 	%fd574, %fd572, %fd573;
	fma.rn.f64 	%fd33, %fd569, %fd570, %fd574;
	neg.f64 	%fd575, %fd33;
	mov.f64 	%fd576, 0d4338000000000000;
	mov.f64 	%fd577, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd578, %fd575, %fd577, %fd576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r11, %temp}, %fd578;
	}
	mov.f64 	%fd579, 0dC338000000000000;
	add.rn.f64 	%fd580, %fd578, %fd579;
	mov.f64 	%fd581, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd582, %fd580, %fd581, %fd575;
	mov.f64 	%fd583, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd584, %fd580, %fd583, %fd582;
	mov.f64 	%fd585, 0d3E928AF3FCA213EA;
	mov.f64 	%fd586, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd587, %fd586, %fd584, %fd585;
	mov.f64 	%fd588, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd589, %fd587, %fd584, %fd588;
	mov.f64 	%fd590, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd591, %fd589, %fd584, %fd590;
	mov.f64 	%fd592, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd593, %fd591, %fd584, %fd592;
	mov.f64 	%fd594, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd595, %fd593, %fd584, %fd594;
	mov.f64 	%fd596, 0d3F81111111122322;
	fma.rn.f64 	%fd597, %fd595, %fd584, %fd596;
	mov.f64 	%fd598, 0d3FA55555555502A1;
	fma.rn.f64 	%fd599, %fd597, %fd584, %fd598;
	mov.f64 	%fd600, 0d3FC5555555555511;
	fma.rn.f64 	%fd601, %fd599, %fd584, %fd600;
	mov.f64 	%fd602, 0d3FE000000000000B;
	fma.rn.f64 	%fd603, %fd601, %fd584, %fd602;
	mov.f64 	%fd604, 0d3FF0000000000000;
	fma.rn.f64 	%fd605, %fd603, %fd584, %fd604;
	fma.rn.f64 	%fd606, %fd605, %fd584, %fd604;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd606;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd606;
	}
	shl.b32 	%r174, %r11, 20;
	add.s32 	%r175, %r13, %r174;
	mov.b64 	%fd1724, {%r12, %r175};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd575;
	}
	mov.b32 	 %f6, %r176;
	abs.f32 	%f1, %f6;
	setp.lt.f32	%p9, %f1, 0f4086232B;
	@%p9 bra 	BB0_13;

	setp.gt.f64	%p10, %fd33, 0d8000000000000000;
	mov.f64 	%fd607, 0d7FF0000000000000;
	sub.f64 	%fd608, %fd607, %fd33;
	selp.f64	%fd1724, 0d0000000000000000, %fd608, %p10;
	setp.geu.f32	%p11, %f1, 0f40874800;
	@%p11 bra 	BB0_13;

	shr.u32 	%r177, %r11, 31;
	add.s32 	%r178, %r11, %r177;
	shr.s32 	%r179, %r178, 1;
	shl.b32 	%r180, %r179, 20;
	add.s32 	%r181, %r180, %r13;
	mov.b64 	%fd609, {%r12, %r181};
	sub.s32 	%r182, %r11, %r179;
	shl.b32 	%r183, %r182, 20;
	add.s32 	%r184, %r183, 1072693248;
	mov.u32 	%r185, 0;
	mov.b64 	%fd610, {%r185, %r184};
	mul.f64 	%fd1724, %fd609, %fd610;

BB0_13:
	add.s32 	%r186, %r473, %r5;
	mul.wide.s32 	%rd30, %r186, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.u32 	%r187, [%rd31];
	cvt.rn.f64.s32	%fd611, %r187;
	mul.f64 	%fd612, %fd29, %fd1724;
	sub.f64 	%fd613, %fd612, %fd611;
	fma.rn.f64 	%fd1725, %fd613, %fd613, %fd1725;
	add.s32 	%r473, %r473, 1;
	setp.lt.s32	%p12, %r473, %r3;
	@%p12 bra 	BB0_10;

BB0_14:
	div.rn.f64 	%fd40, %fd1725, %fd1709;
	setp.geu.f64	%p13, %fd40, %fd27;
	mov.f64 	%fd1721, %fd27;
	@%p13 bra 	BB0_16;

	st.global.f64 	[%rd2+24], %fd1711;
	st.global.f64 	[%rd2+32], %fd1713;
	ld.global.f64 	%fd1712, [%rd22+72];
	mov.f64 	%fd1721, %fd40;

BB0_16:
	mov.f64 	%fd1720, %fd1721;
	ld.global.f64 	%fd614, [%rd3+32];
	add.f64 	%fd1713, %fd1713, %fd614;
	setp.le.f64	%p14, %fd1713, %fd1712;
	mov.f64 	%fd1722, %fd1720;
	@%p14 bra 	BB0_8;

BB0_17:
	mov.f64 	%fd1723, %fd1722;
	ld.global.f64 	%fd615, [%rd3+24];
	add.f64 	%fd1711, %fd1711, %fd615;
	ld.global.f64 	%fd616, [%rd22+56];
	setp.le.f64	%p15, %fd1711, %fd616;
	@%p15 bra 	BB0_7;

BB0_18:
	ld.global.f64 	%fd620, [%rd2+24];
	add.f64 	%fd621, %fd620, %fd620;
	mul.f64 	%fd622, %fd620, %fd621;
	rcp.rn.f64 	%fd1728, %fd622;
	ld.global.f64 	%fd623, [%rd2+32];
	add.f64 	%fd624, %fd623, %fd623;
	mul.f64 	%fd625, %fd623, %fd624;
	rcp.rn.f64 	%fd1726, %fd625;
	mov.u16 	%rs8, 1;
	mov.u32 	%r478, 0;
	mov.f64 	%fd1772, 0d3FF0000000000000;
	mov.f64 	%fd1770, %fd1772;
	mov.u32 	%r476, %r478;
	mov.f64 	%fd1727, 0d0000000000000000;
	bra.uni 	BB0_19;

BB0_214:
	neg.f64 	%fd1300, %fd359;
	st.global.f64 	[%rd3+48], %fd1300;
	mov.f64 	%fd1740, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1740;
	bra.uni 	BB0_237;

BB0_204:
	mul.f64 	%fd1245, %fd344, 0dBFE6666666666666;
	st.global.f64 	[%rd3+48], %fd1245;
	mov.f64 	%fd1737, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1737;
	bra.uni 	BB0_237;

BB0_216:
	mul.f64 	%fd1301, %fd359, 0dBFE6666666666666;
	st.global.f64 	[%rd3+48], %fd1301;
	mov.f64 	%fd1741, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1741;
	bra.uni 	BB0_237;

BB0_196:
	neg.f64 	%fd1240, %fd343;
	st.global.f64 	[%rd5], %fd1240;
	mov.f64 	%fd1733, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1733;
	bra.uni 	BB0_237;

BB0_198:
	mul.f64 	%fd1241, %fd343, 0dBFE6666666666666;
	st.global.f64 	[%rd5], %fd1241;
	mov.f64 	%fd1734, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1734;
	bra.uni 	BB0_237;

BB0_19:
	mov.u32 	%r16, %r478;
	mov.f64 	%fd54, %fd1772;
	mov.f64 	%fd53, %fd1770;
	setp.eq.s32	%p16, %r476, 0;
	@%p16 bra 	BB0_218;
	bra.uni 	BB0_20;

BB0_218:
	ld.global.f64 	%fd360, [%rd3];
	ld.global.f64 	%fd1303, [%rd2];
	add.f64 	%fd361, %fd1303, %fd360;
	setp.gt.f64	%p143, %fd361, %fd14;
	setp.lt.f64	%p144, %fd361, %fd15;
	and.pred  	%p145, %p143, %p144;
	@%p145 bra 	BB0_224;
	bra.uni 	BB0_219;

BB0_224:
	st.global.f64 	[%rd2], %fd361;
	mov.f64 	%fd1839, 0d0000000000000000;
	@%p2 bra 	BB0_230;

	ld.global.f64 	%fd362, [%rd2+8];
	add.f64 	%fd363, %fd1727, %fd1727;
	ld.global.f64 	%fd364, [%rd2+16];
	ld.global.f64 	%fd365, [%rd2+48];
	mov.f64 	%fd1839, 0d0000000000000000;
	mov.u32 	%r501, 0;

BB0_226:
	rem.s32 	%r368, %r501, %r2;
	cvt.rn.f64.s32	%fd1309, %r368;
	sub.f64 	%fd1310, %fd1309, %fd362;
	mul.f64 	%fd1311, %fd1728, %fd1310;
	mul.f64 	%fd1312, %fd1310, %fd1311;
	mul.f64 	%fd1313, %fd363, %fd1310;
	div.s32 	%r369, %r501, %r2;
	cvt.rn.f64.s32	%fd1314, %r369;
	sub.f64 	%fd1315, %fd1314, %fd364;
	mul.f64 	%fd1316, %fd1313, %fd1315;
	sub.f64 	%fd1317, %fd1312, %fd1316;
	mul.f64 	%fd1318, %fd1726, %fd1315;
	fma.rn.f64 	%fd367, %fd1315, %fd1318, %fd1317;
	neg.f64 	%fd1319, %fd367;
	mov.f64 	%fd1320, 0d4338000000000000;
	mov.f64 	%fd1321, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1322, %fd1319, %fd1321, %fd1320;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r100, %temp}, %fd1322;
	}
	mov.f64 	%fd1323, 0dC338000000000000;
	add.rn.f64 	%fd1324, %fd1322, %fd1323;
	mov.f64 	%fd1325, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1326, %fd1324, %fd1325, %fd1319;
	mov.f64 	%fd1327, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1328, %fd1324, %fd1327, %fd1326;
	mov.f64 	%fd1329, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1330, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1331, %fd1330, %fd1328, %fd1329;
	mov.f64 	%fd1332, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1333, %fd1331, %fd1328, %fd1332;
	mov.f64 	%fd1334, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1335, %fd1333, %fd1328, %fd1334;
	mov.f64 	%fd1336, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1337, %fd1335, %fd1328, %fd1336;
	mov.f64 	%fd1338, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1339, %fd1337, %fd1328, %fd1338;
	mov.f64 	%fd1340, 0d3F81111111122322;
	fma.rn.f64 	%fd1341, %fd1339, %fd1328, %fd1340;
	mov.f64 	%fd1342, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1343, %fd1341, %fd1328, %fd1342;
	mov.f64 	%fd1344, 0d3FC5555555555511;
	fma.rn.f64 	%fd1345, %fd1343, %fd1328, %fd1344;
	mov.f64 	%fd1346, 0d3FE000000000000B;
	fma.rn.f64 	%fd1347, %fd1345, %fd1328, %fd1346;
	mov.f64 	%fd1348, 0d3FF0000000000000;
	fma.rn.f64 	%fd1349, %fd1347, %fd1328, %fd1348;
	fma.rn.f64 	%fd1350, %fd1349, %fd1328, %fd1348;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r101, %temp}, %fd1350;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r102}, %fd1350;
	}
	shl.b32 	%r370, %r100, 20;
	add.s32 	%r371, %r102, %r370;
	mov.b64 	%fd1838, {%r101, %r371};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r372}, %fd1319;
	}
	mov.b32 	 %f9, %r372;
	abs.f32 	%f4, %f9;
	setp.lt.f32	%p149, %f4, 0f4086232B;
	@%p149 bra 	BB0_229;

	setp.gt.f64	%p150, %fd367, 0d8000000000000000;
	mov.f64 	%fd1351, 0d7FF0000000000000;
	sub.f64 	%fd1352, %fd1351, %fd367;
	selp.f64	%fd1838, 0d0000000000000000, %fd1352, %p150;
	setp.geu.f32	%p151, %f4, 0f40874800;
	@%p151 bra 	BB0_229;

	shr.u32 	%r373, %r100, 31;
	add.s32 	%r374, %r100, %r373;
	shr.s32 	%r375, %r374, 1;
	shl.b32 	%r376, %r375, 20;
	add.s32 	%r377, %r376, %r102;
	mov.b64 	%fd1353, {%r101, %r377};
	sub.s32 	%r378, %r100, %r375;
	shl.b32 	%r379, %r378, 20;
	add.s32 	%r380, %r379, 1072693248;
	mov.u32 	%r381, 0;
	mov.b64 	%fd1354, {%r381, %r380};
	mul.f64 	%fd1838, %fd1353, %fd1354;

BB0_229:
	fma.rn.f64 	%fd1355, %fd361, %fd1838, %fd365;
	add.s32 	%r382, %r501, %r5;
	mul.wide.s32 	%rd197, %r382, 4;
	add.s64 	%rd198, %rd1, %rd197;
	ld.global.u32 	%r383, [%rd198];
	cvt.rn.f64.s32	%fd1356, %r383;
	sub.f64 	%fd1357, %fd1355, %fd1356;
	fma.rn.f64 	%fd1839, %fd1357, %fd1357, %fd1839;
	add.s32 	%r501, %r501, 1;
	setp.lt.s32	%p152, %r501, %r3;
	@%p152 bra 	BB0_226;

BB0_230:
	div.rn.f64 	%fd1773, %fd1839, %fd1709;
	setp.lt.f64	%p153, %fd1773, %fd54;
	mov.f64 	%fd1771, %fd54;
	@%p153 bra 	BB0_237;

	ld.global.f64 	%fd1358, [%rd3];
	sub.f64 	%fd1359, %fd361, %fd1358;
	st.global.f64 	[%rd2], %fd1359;
	ld.global.f64 	%fd375, [%rd3];
	setp.lt.f64	%p154, %fd375, 0d0000000000000000;
	@%p154 bra 	BB0_233;
	bra.uni 	BB0_232;

BB0_233:
	setp.lt.s32	%p155, %r16, 20;
	@%p155 bra 	BB0_235;
	bra.uni 	BB0_234;

BB0_235:
	mul.f64 	%fd1362, %fd375, 0dBFD3333333333333;
	st.global.f64 	[%rd3], %fd1362;
	bra.uni 	BB0_236;

BB0_20:
	setp.eq.s32	%p17, %r476, 6;
	@%p17 bra 	BB0_200;
	bra.uni 	BB0_21;

BB0_200:
	ld.global.f64 	%fd344, [%rd3+48];
	ld.global.f64 	%fd1243, [%rd2+48];
	add.f64 	%fd345, %fd1243, %fd344;
	setp.gt.f64	%p130, %fd345, %fd16;
	setp.lt.f64	%p131, %fd345, %fd17;
	and.pred  	%p132, %p130, %p131;
	@%p132 bra 	BB0_206;
	bra.uni 	BB0_201;

BB0_206:
	st.global.f64 	[%rd2+48], %fd345;
	mov.f64 	%fd1837, 0d0000000000000000;
	@%p2 bra 	BB0_212;

	ld.global.f64 	%fd346, [%rd2];
	ld.global.f64 	%fd347, [%rd2+8];
	add.f64 	%fd348, %fd1727, %fd1727;
	ld.global.f64 	%fd349, [%rd2+16];
	mov.f64 	%fd1837, 0d0000000000000000;
	mov.u32 	%r500, 0;

BB0_208:
	rem.s32 	%r331, %r500, %r2;
	div.s32 	%r332, %r500, %r2;
	cvt.rn.f64.s32	%fd1249, %r331;
	sub.f64 	%fd1250, %fd1249, %fd347;
	mul.f64 	%fd1251, %fd1728, %fd1250;
	mul.f64 	%fd1252, %fd1250, %fd1251;
	mul.f64 	%fd1253, %fd348, %fd1250;
	cvt.rn.f64.s32	%fd1254, %r332;
	sub.f64 	%fd1255, %fd1254, %fd349;
	mul.f64 	%fd1256, %fd1253, %fd1255;
	sub.f64 	%fd1257, %fd1252, %fd1256;
	mul.f64 	%fd1258, %fd1726, %fd1255;
	fma.rn.f64 	%fd351, %fd1255, %fd1258, %fd1257;
	neg.f64 	%fd1259, %fd351;
	mov.f64 	%fd1260, 0d4338000000000000;
	mov.f64 	%fd1261, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1262, %fd1259, %fd1261, %fd1260;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd1262;
	}
	mov.f64 	%fd1263, 0dC338000000000000;
	add.rn.f64 	%fd1264, %fd1262, %fd1263;
	mov.f64 	%fd1265, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1266, %fd1264, %fd1265, %fd1259;
	mov.f64 	%fd1267, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1268, %fd1264, %fd1267, %fd1266;
	mov.f64 	%fd1269, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1270, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1271, %fd1270, %fd1268, %fd1269;
	mov.f64 	%fd1272, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1273, %fd1271, %fd1268, %fd1272;
	mov.f64 	%fd1274, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1275, %fd1273, %fd1268, %fd1274;
	mov.f64 	%fd1276, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1277, %fd1275, %fd1268, %fd1276;
	mov.f64 	%fd1278, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1279, %fd1277, %fd1268, %fd1278;
	mov.f64 	%fd1280, 0d3F81111111122322;
	fma.rn.f64 	%fd1281, %fd1279, %fd1268, %fd1280;
	mov.f64 	%fd1282, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1283, %fd1281, %fd1268, %fd1282;
	mov.f64 	%fd1284, 0d3FC5555555555511;
	fma.rn.f64 	%fd1285, %fd1283, %fd1268, %fd1284;
	mov.f64 	%fd1286, 0d3FE000000000000B;
	fma.rn.f64 	%fd1287, %fd1285, %fd1268, %fd1286;
	mov.f64 	%fd1288, 0d3FF0000000000000;
	fma.rn.f64 	%fd1289, %fd1287, %fd1268, %fd1288;
	fma.rn.f64 	%fd1290, %fd1289, %fd1268, %fd1288;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r93, %temp}, %fd1290;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd1290;
	}
	shl.b32 	%r333, %r92, 20;
	add.s32 	%r334, %r94, %r333;
	mov.b64 	%fd1836, {%r93, %r334};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r335}, %fd1259;
	}
	mov.b32 	 %f8, %r335;
	abs.f32 	%f3, %f8;
	setp.lt.f32	%p136, %f3, 0f4086232B;
	@%p136 bra 	BB0_211;

	setp.gt.f64	%p137, %fd351, 0d8000000000000000;
	mov.f64 	%fd1291, 0d7FF0000000000000;
	sub.f64 	%fd1292, %fd1291, %fd351;
	selp.f64	%fd1836, 0d0000000000000000, %fd1292, %p137;
	setp.geu.f32	%p138, %f3, 0f40874800;
	@%p138 bra 	BB0_211;

	shr.u32 	%r336, %r92, 31;
	add.s32 	%r337, %r92, %r336;
	shr.s32 	%r338, %r337, 1;
	shl.b32 	%r339, %r338, 20;
	add.s32 	%r340, %r339, %r94;
	mov.b64 	%fd1293, {%r93, %r340};
	sub.s32 	%r341, %r92, %r338;
	shl.b32 	%r342, %r341, 20;
	add.s32 	%r343, %r342, 1072693248;
	mov.u32 	%r344, 0;
	mov.b64 	%fd1294, {%r344, %r343};
	mul.f64 	%fd1836, %fd1293, %fd1294;

BB0_211:
	fma.rn.f64 	%fd1295, %fd346, %fd1836, %fd345;
	mad.lo.s32 	%r350, %r1, %r3, %r500;
	mul.wide.s32 	%rd189, %r350, 4;
	add.s64 	%rd190, %rd1, %rd189;
	ld.global.u32 	%r351, [%rd190];
	cvt.rn.f64.s32	%fd1296, %r351;
	sub.f64 	%fd1297, %fd1295, %fd1296;
	fma.rn.f64 	%fd1837, %fd1297, %fd1297, %fd1837;
	add.s32 	%r500, %r500, 1;
	setp.lt.s32	%p139, %r500, %r3;
	@%p139 bra 	BB0_208;

BB0_212:
	div.rn.f64 	%fd1773, %fd1837, %fd1709;
	setp.lt.f64	%p140, %fd1773, %fd54;
	mov.f64 	%fd1739, %fd53;
	mov.f64 	%fd1771, %fd1739;
	@%p140 bra 	BB0_237;

	ld.global.f64 	%fd1298, [%rd3+48];
	sub.f64 	%fd1299, %fd345, %fd1298;
	st.global.f64 	[%rd2+48], %fd1299;
	ld.global.f64 	%fd359, [%rd3+48];
	setp.lt.f64	%p141, %fd359, 0d0000000000000000;
	@%p141 bra 	BB0_215;
	bra.uni 	BB0_214;

BB0_215:
	setp.lt.s32	%p142, %r16, 20;
	@%p142 bra 	BB0_217;
	bra.uni 	BB0_216;

BB0_217:
	mul.f64 	%fd1302, %fd359, 0dBFD3333333333333;
	st.global.f64 	[%rd3+48], %fd1302;
	mov.f64 	%fd1742, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1742;
	bra.uni 	BB0_237;

BB0_219:
	setp.lt.f64	%p146, %fd360, 0d0000000000000000;
	@%p146 bra 	BB0_221;
	bra.uni 	BB0_220;

BB0_221:
	setp.lt.s32	%p147, %r16, 20;
	@%p147 bra 	BB0_223;
	bra.uni 	BB0_222;

BB0_223:
	mul.f64 	%fd1306, %fd360, 0dBFD3333333333333;
	st.global.f64 	[%rd3], %fd1306;
	bra.uni 	BB0_236;

BB0_21:
	mad.lo.s32 	%r195, %r1, 7, %r476;
	mul.wide.s32 	%rd35, %r195, 8;
	add.s64 	%rd4, %rd17, %rd35;
	add.s64 	%rd5, %rd21, %rd35;
	ld.global.f64 	%fd55, [%rd5];
	ld.global.f64 	%fd626, [%rd4];
	add.f64 	%fd56, %fd626, %fd55;
	shl.b32 	%r196, %r476, 1;
	mul.wide.s32 	%rd38, %r196, 8;
	add.s64 	%rd6, %rd22, %rd38;
	ld.global.f64 	%fd627, [%rd6];
	setp.leu.f64	%p18, %fd56, %fd627;
	@%p18 bra 	BB0_23;

	ld.global.f64 	%fd628, [%rd6+8];
	setp.lt.f64	%p19, %fd56, %fd628;
	@%p19 bra 	BB0_28;
	bra.uni 	BB0_23;

BB0_28:
	st.global.f64 	[%rd4], %fd56;
	ld.global.f64 	%fd1774, [%rd2+40];
	abs.f64 	%fd632, %fd1774;
	setp.neu.f64	%p22, %fd632, 0d7FF0000000000000;
	@%p22 bra 	BB0_30;

	mov.f64 	%fd633, 0d0000000000000000;
	mul.rn.f64 	%fd1774, %fd1774, %fd633;

BB0_30:
	mul.f64 	%fd634, %fd1774, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r479, %fd634;
	add.u64 	%rd39, %SP, 0;
	cvta.to.local.u64 	%rd40, %rd39;
	st.local.u32 	[%rd40], %r479;
	cvt.rn.f64.s32	%fd635, %r479;
	neg.f64 	%fd636, %fd635;
	mov.f64 	%fd637, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd638, %fd636, %fd637, %fd1774;
	mov.f64 	%fd639, 0d3C91A62633145C00;
	fma.rn.f64 	%fd640, %fd636, %fd639, %fd638;
	mov.f64 	%fd641, 0d397B839A252049C0;
	fma.rn.f64 	%fd1775, %fd636, %fd641, %fd640;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r197}, %fd1774;
	}
	and.b32  	%r198, %r197, 2145386496;
	setp.lt.u32	%p23, %r198, 1105199104;
	@%p23 bra 	BB0_32;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1774;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1775, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r479, [%rd40];

BB0_32:
	add.s32 	%r20, %r479, 1;
	and.b32  	%r199, %r20, 1;
	shl.b32 	%r200, %r199, 3;
	setp.eq.b32	%p24, %r199, 1;
	selp.f64	%fd642, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p24;
	mul.wide.u32 	%rd43, %r200, 8;
	mov.u64 	%rd44, __cudart_sin_cos_coeffs;
	add.s64 	%rd45, %rd43, %rd44;
	ld.const.f64 	%fd643, [%rd45+8];
	mul.rn.f64 	%fd63, %fd1775, %fd1775;
	fma.rn.f64 	%fd644, %fd642, %fd63, %fd643;
	ld.const.f64 	%fd645, [%rd45+16];
	fma.rn.f64 	%fd646, %fd644, %fd63, %fd645;
	ld.const.f64 	%fd647, [%rd45+24];
	fma.rn.f64 	%fd648, %fd646, %fd63, %fd647;
	ld.const.f64 	%fd649, [%rd45+32];
	fma.rn.f64 	%fd650, %fd648, %fd63, %fd649;
	ld.const.f64 	%fd651, [%rd45+40];
	fma.rn.f64 	%fd652, %fd650, %fd63, %fd651;
	ld.const.f64 	%fd653, [%rd45+48];
	fma.rn.f64 	%fd64, %fd652, %fd63, %fd653;
	fma.rn.f64 	%fd1776, %fd64, %fd1775, %fd1775;
	setp.eq.s32	%p25, %r199, 0;
	@%p25 bra 	BB0_34;

	mov.f64 	%fd654, 0d3FF0000000000000;
	fma.rn.f64 	%fd1776, %fd64, %fd63, %fd654;

BB0_34:
	and.b32  	%r201, %r20, 2;
	setp.eq.s32	%p26, %r201, 0;
	@%p26 bra 	BB0_36;

	mov.f64 	%fd655, 0d0000000000000000;
	mov.f64 	%fd656, 0dBFF0000000000000;
	fma.rn.f64 	%fd1776, %fd1776, %fd656, %fd655;

BB0_36:
	ld.global.f64 	%fd1777, [%rd2+40];
	abs.f64 	%fd657, %fd1777;
	setp.neu.f64	%p27, %fd657, 0d7FF0000000000000;
	@%p27 bra 	BB0_38;

	mov.f64 	%fd658, 0d0000000000000000;
	mul.rn.f64 	%fd1777, %fd1777, %fd658;

BB0_38:
	mul.f64 	%fd659, %fd1777, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r480, %fd659;
	st.local.u32 	[%rd40], %r480;
	cvt.rn.f64.s32	%fd660, %r480;
	neg.f64 	%fd661, %fd660;
	fma.rn.f64 	%fd663, %fd661, %fd637, %fd1777;
	fma.rn.f64 	%fd665, %fd661, %fd639, %fd663;
	fma.rn.f64 	%fd1778, %fd661, %fd641, %fd665;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r202}, %fd1777;
	}
	and.b32  	%r203, %r202, 2145386496;
	setp.lt.u32	%p28, %r203, 1105199104;
	@%p28 bra 	BB0_40;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1777;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1778, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r480, [%rd40];

BB0_40:
	add.s32 	%r24, %r480, 1;
	and.b32  	%r204, %r24, 1;
	shl.b32 	%r205, %r204, 3;
	setp.eq.b32	%p29, %r204, 1;
	selp.f64	%fd667, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p29;
	mul.wide.u32 	%rd50, %r205, 8;
	add.s64 	%rd52, %rd50, %rd44;
	ld.const.f64 	%fd668, [%rd52+8];
	mul.rn.f64 	%fd76, %fd1778, %fd1778;
	fma.rn.f64 	%fd669, %fd667, %fd76, %fd668;
	ld.const.f64 	%fd670, [%rd52+16];
	fma.rn.f64 	%fd671, %fd669, %fd76, %fd670;
	ld.const.f64 	%fd672, [%rd52+24];
	fma.rn.f64 	%fd673, %fd671, %fd76, %fd672;
	ld.const.f64 	%fd674, [%rd52+32];
	fma.rn.f64 	%fd675, %fd673, %fd76, %fd674;
	ld.const.f64 	%fd676, [%rd52+40];
	fma.rn.f64 	%fd677, %fd675, %fd76, %fd676;
	ld.const.f64 	%fd678, [%rd52+48];
	fma.rn.f64 	%fd77, %fd677, %fd76, %fd678;
	fma.rn.f64 	%fd1779, %fd77, %fd1778, %fd1778;
	setp.eq.s32	%p30, %r204, 0;
	@%p30 bra 	BB0_42;

	mov.f64 	%fd679, 0d3FF0000000000000;
	fma.rn.f64 	%fd1779, %fd77, %fd76, %fd679;

BB0_42:
	and.b32  	%r206, %r24, 2;
	setp.eq.s32	%p31, %r206, 0;
	@%p31 bra 	BB0_44;

	mov.f64 	%fd680, 0d0000000000000000;
	mov.f64 	%fd681, 0dBFF0000000000000;
	fma.rn.f64 	%fd1779, %fd1779, %fd681, %fd680;

BB0_44:
	ld.global.f64 	%fd682, [%rd2+24];
	add.f64 	%fd683, %fd682, %fd682;
	mul.f64 	%fd684, %fd682, %fd683;
	mul.f64 	%fd685, %fd1776, %fd1779;
	div.rn.f64 	%fd83, %fd685, %fd684;
	ld.global.f64 	%fd1780, [%rd2+40];
	abs.f64 	%fd686, %fd1780;
	setp.neu.f64	%p32, %fd686, 0d7FF0000000000000;
	@%p32 bra 	BB0_46;

	mov.f64 	%fd687, 0d0000000000000000;
	mul.rn.f64 	%fd1780, %fd1780, %fd687;

BB0_46:
	mul.f64 	%fd688, %fd1780, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r481, %fd688;
	st.local.u32 	[%rd40], %r481;
	cvt.rn.f64.s32	%fd689, %r481;
	neg.f64 	%fd690, %fd689;
	fma.rn.f64 	%fd692, %fd690, %fd637, %fd1780;
	fma.rn.f64 	%fd694, %fd690, %fd639, %fd692;
	fma.rn.f64 	%fd1781, %fd690, %fd641, %fd694;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r207}, %fd1780;
	}
	and.b32  	%r208, %r207, 2145386496;
	setp.lt.u32	%p33, %r208, 1105199104;
	@%p33 bra 	BB0_48;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1780;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1781, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r481, [%rd40];

BB0_48:
	and.b32  	%r209, %r481, 1;
	shl.b32 	%r210, %r209, 3;
	setp.eq.b32	%p34, %r209, 1;
	selp.f64	%fd696, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p34;
	mul.wide.u32 	%rd57, %r210, 8;
	add.s64 	%rd59, %rd57, %rd44;
	ld.const.f64 	%fd697, [%rd59+8];
	mul.rn.f64 	%fd90, %fd1781, %fd1781;
	fma.rn.f64 	%fd698, %fd696, %fd90, %fd697;
	ld.const.f64 	%fd699, [%rd59+16];
	fma.rn.f64 	%fd700, %fd698, %fd90, %fd699;
	ld.const.f64 	%fd701, [%rd59+24];
	fma.rn.f64 	%fd702, %fd700, %fd90, %fd701;
	ld.const.f64 	%fd703, [%rd59+32];
	fma.rn.f64 	%fd704, %fd702, %fd90, %fd703;
	ld.const.f64 	%fd705, [%rd59+40];
	fma.rn.f64 	%fd706, %fd704, %fd90, %fd705;
	ld.const.f64 	%fd707, [%rd59+48];
	fma.rn.f64 	%fd91, %fd706, %fd90, %fd707;
	fma.rn.f64 	%fd1782, %fd91, %fd1781, %fd1781;
	setp.eq.s32	%p35, %r209, 0;
	@%p35 bra 	BB0_50;

	mov.f64 	%fd708, 0d3FF0000000000000;
	fma.rn.f64 	%fd1782, %fd91, %fd90, %fd708;

BB0_50:
	and.b32  	%r211, %r481, 2;
	setp.eq.s32	%p36, %r211, 0;
	@%p36 bra 	BB0_52;

	mov.f64 	%fd709, 0d0000000000000000;
	mov.f64 	%fd710, 0dBFF0000000000000;
	fma.rn.f64 	%fd1782, %fd1782, %fd710, %fd709;

BB0_52:
	ld.global.f64 	%fd1783, [%rd2+40];
	abs.f64 	%fd711, %fd1783;
	setp.neu.f64	%p37, %fd711, 0d7FF0000000000000;
	@%p37 bra 	BB0_54;

	mov.f64 	%fd712, 0d0000000000000000;
	mul.rn.f64 	%fd1783, %fd1783, %fd712;

BB0_54:
	mul.f64 	%fd713, %fd1783, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r482, %fd713;
	st.local.u32 	[%rd40], %r482;
	cvt.rn.f64.s32	%fd714, %r482;
	neg.f64 	%fd715, %fd714;
	fma.rn.f64 	%fd717, %fd715, %fd637, %fd1783;
	fma.rn.f64 	%fd719, %fd715, %fd639, %fd717;
	fma.rn.f64 	%fd1784, %fd715, %fd641, %fd719;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r212}, %fd1783;
	}
	and.b32  	%r213, %r212, 2145386496;
	setp.lt.u32	%p38, %r213, 1105199104;
	@%p38 bra 	BB0_56;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1783;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1784, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r482, [%rd40];

BB0_56:
	and.b32  	%r214, %r482, 1;
	shl.b32 	%r215, %r214, 3;
	setp.eq.b32	%p39, %r214, 1;
	selp.f64	%fd721, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p39;
	mul.wide.u32 	%rd64, %r215, 8;
	add.s64 	%rd66, %rd64, %rd44;
	ld.const.f64 	%fd722, [%rd66+8];
	mul.rn.f64 	%fd103, %fd1784, %fd1784;
	fma.rn.f64 	%fd723, %fd721, %fd103, %fd722;
	ld.const.f64 	%fd724, [%rd66+16];
	fma.rn.f64 	%fd725, %fd723, %fd103, %fd724;
	ld.const.f64 	%fd726, [%rd66+24];
	fma.rn.f64 	%fd727, %fd725, %fd103, %fd726;
	ld.const.f64 	%fd728, [%rd66+32];
	fma.rn.f64 	%fd729, %fd727, %fd103, %fd728;
	ld.const.f64 	%fd730, [%rd66+40];
	fma.rn.f64 	%fd731, %fd729, %fd103, %fd730;
	ld.const.f64 	%fd732, [%rd66+48];
	fma.rn.f64 	%fd104, %fd731, %fd103, %fd732;
	fma.rn.f64 	%fd1785, %fd104, %fd1784, %fd1784;
	setp.eq.s32	%p40, %r214, 0;
	@%p40 bra 	BB0_58;

	mov.f64 	%fd733, 0d3FF0000000000000;
	fma.rn.f64 	%fd1785, %fd104, %fd103, %fd733;

BB0_58:
	and.b32  	%r216, %r482, 2;
	setp.eq.s32	%p41, %r216, 0;
	@%p41 bra 	BB0_60;

	mov.f64 	%fd734, 0d0000000000000000;
	mov.f64 	%fd735, 0dBFF0000000000000;
	fma.rn.f64 	%fd1785, %fd1785, %fd735, %fd734;

BB0_60:
	ld.global.f64 	%fd736, [%rd2+32];
	add.f64 	%fd737, %fd736, %fd736;
	mul.f64 	%fd738, %fd736, %fd737;
	mul.f64 	%fd739, %fd1782, %fd1785;
	div.rn.f64 	%fd740, %fd739, %fd738;
	add.f64 	%fd1728, %fd83, %fd740;
	ld.global.f64 	%fd741, [%rd2+40];
	add.f64 	%fd1786, %fd741, %fd741;
	abs.f64 	%fd742, %fd1786;
	setp.neu.f64	%p42, %fd742, 0d7FF0000000000000;
	@%p42 bra 	BB0_62;

	mov.f64 	%fd743, 0d0000000000000000;
	mul.rn.f64 	%fd1786, %fd1786, %fd743;

BB0_62:
	mul.f64 	%fd744, %fd1786, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r483, %fd744;
	st.local.u32 	[%rd40], %r483;
	cvt.rn.f64.s32	%fd745, %r483;
	neg.f64 	%fd746, %fd745;
	fma.rn.f64 	%fd748, %fd746, %fd637, %fd1786;
	fma.rn.f64 	%fd750, %fd746, %fd639, %fd748;
	fma.rn.f64 	%fd1787, %fd746, %fd641, %fd750;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r217}, %fd1786;
	}
	and.b32  	%r218, %r217, 2145386496;
	setp.lt.u32	%p43, %r218, 1105199104;
	@%p43 bra 	BB0_64;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1786;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1787, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r483, [%rd40];

BB0_64:
	and.b32  	%r219, %r483, 1;
	shl.b32 	%r220, %r219, 3;
	setp.eq.b32	%p44, %r219, 1;
	selp.f64	%fd752, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p44;
	mul.wide.u32 	%rd71, %r220, 8;
	add.s64 	%rd73, %rd71, %rd44;
	ld.const.f64 	%fd753, [%rd73+8];
	mul.rn.f64 	%fd117, %fd1787, %fd1787;
	fma.rn.f64 	%fd754, %fd752, %fd117, %fd753;
	ld.const.f64 	%fd755, [%rd73+16];
	fma.rn.f64 	%fd756, %fd754, %fd117, %fd755;
	ld.const.f64 	%fd757, [%rd73+24];
	fma.rn.f64 	%fd758, %fd756, %fd117, %fd757;
	ld.const.f64 	%fd759, [%rd73+32];
	fma.rn.f64 	%fd760, %fd758, %fd117, %fd759;
	ld.const.f64 	%fd761, [%rd73+40];
	fma.rn.f64 	%fd762, %fd760, %fd117, %fd761;
	ld.const.f64 	%fd763, [%rd73+48];
	fma.rn.f64 	%fd118, %fd762, %fd117, %fd763;
	fma.rn.f64 	%fd1788, %fd118, %fd1787, %fd1787;
	setp.eq.s32	%p45, %r219, 0;
	@%p45 bra 	BB0_66;

	mov.f64 	%fd764, 0d3FF0000000000000;
	fma.rn.f64 	%fd1788, %fd118, %fd117, %fd764;

BB0_66:
	and.b32  	%r221, %r483, 2;
	setp.eq.s32	%p46, %r221, 0;
	@%p46 bra 	BB0_68;

	mov.f64 	%fd765, 0d0000000000000000;
	mov.f64 	%fd766, 0dBFF0000000000000;
	fma.rn.f64 	%fd1788, %fd1788, %fd766, %fd765;

BB0_68:
	ld.global.f64 	%fd767, [%rd2+24];
	mul.f64 	%fd768, %fd767, 0dC010000000000000;
	mul.f64 	%fd769, %fd767, %fd768;
	div.rn.f64 	%fd124, %fd1788, %fd769;
	ld.global.f64 	%fd770, [%rd2+40];
	add.f64 	%fd1789, %fd770, %fd770;
	abs.f64 	%fd771, %fd1789;
	setp.neu.f64	%p47, %fd771, 0d7FF0000000000000;
	@%p47 bra 	BB0_70;

	mov.f64 	%fd772, 0d0000000000000000;
	mul.rn.f64 	%fd1789, %fd1789, %fd772;

BB0_70:
	mul.f64 	%fd773, %fd1789, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r484, %fd773;
	st.local.u32 	[%rd40], %r484;
	cvt.rn.f64.s32	%fd774, %r484;
	neg.f64 	%fd775, %fd774;
	fma.rn.f64 	%fd777, %fd775, %fd637, %fd1789;
	fma.rn.f64 	%fd779, %fd775, %fd639, %fd777;
	fma.rn.f64 	%fd1790, %fd775, %fd641, %fd779;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r222}, %fd1789;
	}
	and.b32  	%r223, %r222, 2145386496;
	setp.lt.u32	%p48, %r223, 1105199104;
	@%p48 bra 	BB0_72;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1789;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1790, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r484, [%rd40];

BB0_72:
	and.b32  	%r224, %r484, 1;
	shl.b32 	%r225, %r224, 3;
	setp.eq.b32	%p49, %r224, 1;
	selp.f64	%fd781, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p49;
	mul.wide.u32 	%rd78, %r225, 8;
	add.s64 	%rd80, %rd78, %rd44;
	ld.const.f64 	%fd782, [%rd80+8];
	mul.rn.f64 	%fd131, %fd1790, %fd1790;
	fma.rn.f64 	%fd783, %fd781, %fd131, %fd782;
	ld.const.f64 	%fd784, [%rd80+16];
	fma.rn.f64 	%fd785, %fd783, %fd131, %fd784;
	ld.const.f64 	%fd786, [%rd80+24];
	fma.rn.f64 	%fd787, %fd785, %fd131, %fd786;
	ld.const.f64 	%fd788, [%rd80+32];
	fma.rn.f64 	%fd789, %fd787, %fd131, %fd788;
	ld.const.f64 	%fd790, [%rd80+40];
	fma.rn.f64 	%fd791, %fd789, %fd131, %fd790;
	ld.const.f64 	%fd792, [%rd80+48];
	fma.rn.f64 	%fd132, %fd791, %fd131, %fd792;
	fma.rn.f64 	%fd1791, %fd132, %fd1790, %fd1790;
	setp.eq.s32	%p50, %r224, 0;
	@%p50 bra 	BB0_74;

	mov.f64 	%fd793, 0d3FF0000000000000;
	fma.rn.f64 	%fd1791, %fd132, %fd131, %fd793;

BB0_74:
	and.b32  	%r226, %r484, 2;
	setp.eq.s32	%p51, %r226, 0;
	@%p51 bra 	BB0_76;

	mov.f64 	%fd794, 0d0000000000000000;
	mov.f64 	%fd795, 0dBFF0000000000000;
	fma.rn.f64 	%fd1791, %fd1791, %fd795, %fd794;

BB0_76:
	ld.global.f64 	%fd796, [%rd2+32];
	mul.f64 	%fd797, %fd796, 0d4010000000000000;
	mul.f64 	%fd798, %fd796, %fd797;
	div.rn.f64 	%fd799, %fd1791, %fd798;
	add.f64 	%fd1727, %fd124, %fd799;
	ld.global.f64 	%fd1792, [%rd2+40];
	abs.f64 	%fd800, %fd1792;
	setp.neu.f64	%p52, %fd800, 0d7FF0000000000000;
	@%p52 bra 	BB0_78;

	mov.f64 	%fd801, 0d0000000000000000;
	mul.rn.f64 	%fd1792, %fd1792, %fd801;

BB0_78:
	mul.f64 	%fd802, %fd1792, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r485, %fd802;
	st.local.u32 	[%rd40], %r485;
	cvt.rn.f64.s32	%fd803, %r485;
	neg.f64 	%fd804, %fd803;
	fma.rn.f64 	%fd806, %fd804, %fd637, %fd1792;
	fma.rn.f64 	%fd808, %fd804, %fd639, %fd806;
	fma.rn.f64 	%fd1793, %fd804, %fd641, %fd808;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r227}, %fd1792;
	}
	and.b32  	%r228, %r227, 2145386496;
	setp.lt.u32	%p53, %r228, 1105199104;
	@%p53 bra 	BB0_80;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1792;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1793, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r485, [%rd40];

BB0_80:
	and.b32  	%r229, %r485, 1;
	shl.b32 	%r230, %r229, 3;
	setp.eq.b32	%p54, %r229, 1;
	selp.f64	%fd810, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p54;
	mul.wide.u32 	%rd85, %r230, 8;
	add.s64 	%rd87, %rd85, %rd44;
	ld.const.f64 	%fd811, [%rd87+8];
	mul.rn.f64 	%fd145, %fd1793, %fd1793;
	fma.rn.f64 	%fd812, %fd810, %fd145, %fd811;
	ld.const.f64 	%fd813, [%rd87+16];
	fma.rn.f64 	%fd814, %fd812, %fd145, %fd813;
	ld.const.f64 	%fd815, [%rd87+24];
	fma.rn.f64 	%fd816, %fd814, %fd145, %fd815;
	ld.const.f64 	%fd817, [%rd87+32];
	fma.rn.f64 	%fd818, %fd816, %fd145, %fd817;
	ld.const.f64 	%fd819, [%rd87+40];
	fma.rn.f64 	%fd820, %fd818, %fd145, %fd819;
	ld.const.f64 	%fd821, [%rd87+48];
	fma.rn.f64 	%fd146, %fd820, %fd145, %fd821;
	fma.rn.f64 	%fd1794, %fd146, %fd1793, %fd1793;
	setp.eq.s32	%p55, %r229, 0;
	@%p55 bra 	BB0_82;

	mov.f64 	%fd822, 0d3FF0000000000000;
	fma.rn.f64 	%fd1794, %fd146, %fd145, %fd822;

BB0_82:
	and.b32  	%r231, %r485, 2;
	setp.eq.s32	%p56, %r231, 0;
	@%p56 bra 	BB0_84;

	mov.f64 	%fd823, 0d0000000000000000;
	mov.f64 	%fd824, 0dBFF0000000000000;
	fma.rn.f64 	%fd1794, %fd1794, %fd824, %fd823;

BB0_84:
	ld.global.f64 	%fd1795, [%rd2+40];
	abs.f64 	%fd825, %fd1795;
	setp.neu.f64	%p57, %fd825, 0d7FF0000000000000;
	@%p57 bra 	BB0_86;

	mov.f64 	%fd826, 0d0000000000000000;
	mul.rn.f64 	%fd1795, %fd1795, %fd826;

BB0_86:
	mul.f64 	%fd827, %fd1795, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r486, %fd827;
	st.local.u32 	[%rd40], %r486;
	cvt.rn.f64.s32	%fd828, %r486;
	neg.f64 	%fd829, %fd828;
	fma.rn.f64 	%fd831, %fd829, %fd637, %fd1795;
	fma.rn.f64 	%fd833, %fd829, %fd639, %fd831;
	fma.rn.f64 	%fd1796, %fd829, %fd641, %fd833;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r232}, %fd1795;
	}
	and.b32  	%r233, %r232, 2145386496;
	setp.lt.u32	%p58, %r233, 1105199104;
	@%p58 bra 	BB0_88;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1795;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1796, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r486, [%rd40];

BB0_88:
	and.b32  	%r234, %r486, 1;
	shl.b32 	%r235, %r234, 3;
	setp.eq.b32	%p59, %r234, 1;
	selp.f64	%fd835, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p59;
	mul.wide.u32 	%rd92, %r235, 8;
	add.s64 	%rd94, %rd92, %rd44;
	ld.const.f64 	%fd836, [%rd94+8];
	mul.rn.f64 	%fd158, %fd1796, %fd1796;
	fma.rn.f64 	%fd837, %fd835, %fd158, %fd836;
	ld.const.f64 	%fd838, [%rd94+16];
	fma.rn.f64 	%fd839, %fd837, %fd158, %fd838;
	ld.const.f64 	%fd840, [%rd94+24];
	fma.rn.f64 	%fd841, %fd839, %fd158, %fd840;
	ld.const.f64 	%fd842, [%rd94+32];
	fma.rn.f64 	%fd843, %fd841, %fd158, %fd842;
	ld.const.f64 	%fd844, [%rd94+40];
	fma.rn.f64 	%fd845, %fd843, %fd158, %fd844;
	ld.const.f64 	%fd846, [%rd94+48];
	fma.rn.f64 	%fd159, %fd845, %fd158, %fd846;
	fma.rn.f64 	%fd1797, %fd159, %fd1796, %fd1796;
	setp.eq.s32	%p60, %r234, 0;
	@%p60 bra 	BB0_90;

	mov.f64 	%fd847, 0d3FF0000000000000;
	fma.rn.f64 	%fd1797, %fd159, %fd158, %fd847;

BB0_90:
	and.b32  	%r236, %r486, 2;
	setp.eq.s32	%p61, %r236, 0;
	@%p61 bra 	BB0_92;

	mov.f64 	%fd848, 0d0000000000000000;
	mov.f64 	%fd849, 0dBFF0000000000000;
	fma.rn.f64 	%fd1797, %fd1797, %fd849, %fd848;

BB0_92:
	ld.global.f64 	%fd850, [%rd2+24];
	add.f64 	%fd851, %fd850, %fd850;
	mul.f64 	%fd852, %fd850, %fd851;
	mul.f64 	%fd853, %fd1794, %fd1797;
	div.rn.f64 	%fd165, %fd853, %fd852;
	ld.global.f64 	%fd1798, [%rd2+40];
	abs.f64 	%fd854, %fd1798;
	setp.neu.f64	%p62, %fd854, 0d7FF0000000000000;
	@%p62 bra 	BB0_94;

	mov.f64 	%fd855, 0d0000000000000000;
	mul.rn.f64 	%fd1798, %fd1798, %fd855;

BB0_94:
	mul.f64 	%fd856, %fd1798, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r487, %fd856;
	st.local.u32 	[%rd40], %r487;
	cvt.rn.f64.s32	%fd857, %r487;
	neg.f64 	%fd858, %fd857;
	fma.rn.f64 	%fd860, %fd858, %fd637, %fd1798;
	fma.rn.f64 	%fd862, %fd858, %fd639, %fd860;
	fma.rn.f64 	%fd1799, %fd858, %fd641, %fd862;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r237}, %fd1798;
	}
	and.b32  	%r238, %r237, 2145386496;
	setp.lt.u32	%p63, %r238, 1105199104;
	@%p63 bra 	BB0_96;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1798;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1799, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r487, [%rd40];

BB0_96:
	add.s32 	%r46, %r487, 1;
	and.b32  	%r239, %r46, 1;
	shl.b32 	%r240, %r239, 3;
	setp.eq.b32	%p64, %r239, 1;
	selp.f64	%fd864, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p64;
	mul.wide.u32 	%rd99, %r240, 8;
	add.s64 	%rd101, %rd99, %rd44;
	ld.const.f64 	%fd865, [%rd101+8];
	mul.rn.f64 	%fd172, %fd1799, %fd1799;
	fma.rn.f64 	%fd866, %fd864, %fd172, %fd865;
	ld.const.f64 	%fd867, [%rd101+16];
	fma.rn.f64 	%fd868, %fd866, %fd172, %fd867;
	ld.const.f64 	%fd869, [%rd101+24];
	fma.rn.f64 	%fd870, %fd868, %fd172, %fd869;
	ld.const.f64 	%fd871, [%rd101+32];
	fma.rn.f64 	%fd872, %fd870, %fd172, %fd871;
	ld.const.f64 	%fd873, [%rd101+40];
	fma.rn.f64 	%fd874, %fd872, %fd172, %fd873;
	ld.const.f64 	%fd875, [%rd101+48];
	fma.rn.f64 	%fd173, %fd874, %fd172, %fd875;
	fma.rn.f64 	%fd1800, %fd173, %fd1799, %fd1799;
	setp.eq.s32	%p65, %r239, 0;
	@%p65 bra 	BB0_98;

	mov.f64 	%fd876, 0d3FF0000000000000;
	fma.rn.f64 	%fd1800, %fd173, %fd172, %fd876;

BB0_98:
	and.b32  	%r241, %r46, 2;
	setp.eq.s32	%p66, %r241, 0;
	@%p66 bra 	BB0_100;

	mov.f64 	%fd877, 0d0000000000000000;
	mov.f64 	%fd878, 0dBFF0000000000000;
	fma.rn.f64 	%fd1800, %fd1800, %fd878, %fd877;

BB0_100:
	ld.global.f64 	%fd1801, [%rd2+40];
	abs.f64 	%fd879, %fd1801;
	setp.neu.f64	%p67, %fd879, 0d7FF0000000000000;
	@%p67 bra 	BB0_102;

	mov.f64 	%fd880, 0d0000000000000000;
	mul.rn.f64 	%fd1801, %fd1801, %fd880;

BB0_102:
	mul.f64 	%fd881, %fd1801, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r488, %fd881;
	st.local.u32 	[%rd40], %r488;
	cvt.rn.f64.s32	%fd882, %r488;
	neg.f64 	%fd883, %fd882;
	fma.rn.f64 	%fd885, %fd883, %fd637, %fd1801;
	fma.rn.f64 	%fd887, %fd883, %fd639, %fd885;
	fma.rn.f64 	%fd1802, %fd883, %fd641, %fd887;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r242}, %fd1801;
	}
	and.b32  	%r243, %r242, 2145386496;
	setp.lt.u32	%p68, %r243, 1105199104;
	@%p68 bra 	BB0_104;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1801;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1802, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r488, [%rd40];

BB0_104:
	add.s32 	%r50, %r488, 1;
	and.b32  	%r244, %r50, 1;
	shl.b32 	%r245, %r244, 3;
	setp.eq.b32	%p69, %r244, 1;
	selp.f64	%fd889, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p69;
	mul.wide.u32 	%rd106, %r245, 8;
	add.s64 	%rd108, %rd106, %rd44;
	ld.const.f64 	%fd890, [%rd108+8];
	mul.rn.f64 	%fd185, %fd1802, %fd1802;
	fma.rn.f64 	%fd891, %fd889, %fd185, %fd890;
	ld.const.f64 	%fd892, [%rd108+16];
	fma.rn.f64 	%fd893, %fd891, %fd185, %fd892;
	ld.const.f64 	%fd894, [%rd108+24];
	fma.rn.f64 	%fd895, %fd893, %fd185, %fd894;
	ld.const.f64 	%fd896, [%rd108+32];
	fma.rn.f64 	%fd897, %fd895, %fd185, %fd896;
	ld.const.f64 	%fd898, [%rd108+40];
	fma.rn.f64 	%fd899, %fd897, %fd185, %fd898;
	ld.const.f64 	%fd900, [%rd108+48];
	fma.rn.f64 	%fd186, %fd899, %fd185, %fd900;
	fma.rn.f64 	%fd1803, %fd186, %fd1802, %fd1802;
	setp.eq.s32	%p70, %r244, 0;
	@%p70 bra 	BB0_106;

	mov.f64 	%fd901, 0d3FF0000000000000;
	fma.rn.f64 	%fd1803, %fd186, %fd185, %fd901;

BB0_106:
	and.b32  	%r246, %r50, 2;
	setp.eq.s32	%p71, %r246, 0;
	@%p71 bra 	BB0_108;

	mov.f64 	%fd902, 0d0000000000000000;
	mov.f64 	%fd903, 0dBFF0000000000000;
	fma.rn.f64 	%fd1803, %fd1803, %fd903, %fd902;

BB0_108:
	ld.global.f64 	%fd905, [%rd2+32];
	add.f64 	%fd906, %fd905, %fd905;
	mul.f64 	%fd907, %fd905, %fd906;
	mul.f64 	%fd908, %fd1800, %fd1803;
	div.rn.f64 	%fd909, %fd908, %fd907;
	add.f64 	%fd1726, %fd165, %fd909;
	mov.f64 	%fd1805, 0d0000000000000000;
	@%p2 bra 	BB0_114;

	ld.global.f64 	%fd193, [%rd2];
	ld.global.f64 	%fd194, [%rd2+8];
	ld.global.f64 	%fd196, [%rd2+16];
	ld.global.f64 	%fd197, [%rd2+48];
	mov.f64 	%fd1805, 0d0000000000000000;
	mov.u32 	%r489, 0;

BB0_110:
	add.f64 	%fd1698, %fd1727, %fd1727;
	rem.s32 	%r256, %r489, %r2;
	div.s32 	%r257, %r489, %r2;
	cvt.rn.f64.s32	%fd911, %r256;
	sub.f64 	%fd912, %fd911, %fd194;
	mul.f64 	%fd913, %fd1728, %fd912;
	mul.f64 	%fd914, %fd912, %fd913;
	mul.f64 	%fd915, %fd1698, %fd912;
	cvt.rn.f64.s32	%fd916, %r257;
	sub.f64 	%fd917, %fd916, %fd196;
	mul.f64 	%fd918, %fd915, %fd917;
	sub.f64 	%fd919, %fd914, %fd918;
	mul.f64 	%fd920, %fd1726, %fd917;
	fma.rn.f64 	%fd199, %fd917, %fd920, %fd919;
	neg.f64 	%fd921, %fd199;
	mov.f64 	%fd922, 0d4338000000000000;
	mov.f64 	%fd923, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd924, %fd921, %fd923, %fd922;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd924;
	}
	mov.f64 	%fd925, 0dC338000000000000;
	add.rn.f64 	%fd926, %fd924, %fd925;
	mov.f64 	%fd927, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd928, %fd926, %fd927, %fd921;
	mov.f64 	%fd929, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd930, %fd926, %fd929, %fd928;
	mov.f64 	%fd931, 0d3E928AF3FCA213EA;
	mov.f64 	%fd932, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd933, %fd932, %fd930, %fd931;
	mov.f64 	%fd934, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd935, %fd933, %fd930, %fd934;
	mov.f64 	%fd936, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd937, %fd935, %fd930, %fd936;
	mov.f64 	%fd938, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd939, %fd937, %fd930, %fd938;
	mov.f64 	%fd940, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd941, %fd939, %fd930, %fd940;
	mov.f64 	%fd942, 0d3F81111111122322;
	fma.rn.f64 	%fd943, %fd941, %fd930, %fd942;
	mov.f64 	%fd944, 0d3FA55555555502A1;
	fma.rn.f64 	%fd945, %fd943, %fd930, %fd944;
	mov.f64 	%fd946, 0d3FC5555555555511;
	fma.rn.f64 	%fd947, %fd945, %fd930, %fd946;
	mov.f64 	%fd948, 0d3FE000000000000B;
	fma.rn.f64 	%fd949, %fd947, %fd930, %fd948;
	mov.f64 	%fd950, 0d3FF0000000000000;
	fma.rn.f64 	%fd951, %fd949, %fd930, %fd950;
	fma.rn.f64 	%fd952, %fd951, %fd930, %fd950;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd952;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd952;
	}
	shl.b32 	%r258, %r53, 20;
	add.s32 	%r259, %r55, %r258;
	mov.b64 	%fd1804, {%r54, %r259};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r260}, %fd921;
	}
	mov.b32 	 %f7, %r260;
	abs.f32 	%f2, %f7;
	setp.lt.f32	%p73, %f2, 0f4086232B;
	@%p73 bra 	BB0_113;

	setp.gt.f64	%p74, %fd199, 0d8000000000000000;
	mov.f64 	%fd953, 0d7FF0000000000000;
	sub.f64 	%fd954, %fd953, %fd199;
	selp.f64	%fd1804, 0d0000000000000000, %fd954, %p74;
	setp.geu.f32	%p75, %f2, 0f40874800;
	@%p75 bra 	BB0_113;

	shr.u32 	%r261, %r53, 31;
	add.s32 	%r262, %r53, %r261;
	shr.s32 	%r263, %r262, 1;
	shl.b32 	%r264, %r263, 20;
	add.s32 	%r265, %r264, %r55;
	mov.b64 	%fd955, {%r54, %r265};
	sub.s32 	%r266, %r53, %r263;
	shl.b32 	%r267, %r266, 20;
	add.s32 	%r268, %r267, 1072693248;
	mov.u32 	%r269, 0;
	mov.b64 	%fd956, {%r269, %r268};
	mul.f64 	%fd1804, %fd955, %fd956;

BB0_113:
	fma.rn.f64 	%fd957, %fd193, %fd1804, %fd197;
	add.s32 	%r270, %r489, %r5;
	mul.wide.s32 	%rd113, %r270, 4;
	add.s64 	%rd114, %rd1, %rd113;
	ld.global.u32 	%r271, [%rd114];
	cvt.rn.f64.s32	%fd958, %r271;
	sub.f64 	%fd959, %fd957, %fd958;
	fma.rn.f64 	%fd1805, %fd959, %fd959, %fd1805;
	add.s32 	%r489, %r489, 1;
	setp.lt.s32	%p76, %r489, %r3;
	@%p76 bra 	BB0_110;

BB0_114:
	div.rn.f64 	%fd1773, %fd1805, %fd1709;
	setp.lt.f64	%p77, %fd1773, %fd54;
	mov.f64 	%fd1771, %fd53;
	@%p77 bra 	BB0_237;

	ld.global.f64 	%fd960, [%rd5];
	ld.global.f64 	%fd961, [%rd4];
	sub.f64 	%fd962, %fd961, %fd960;
	st.global.f64 	[%rd4], %fd962;
	ld.global.f64 	%fd1806, [%rd2+40];
	abs.f64 	%fd963, %fd1806;
	setp.neu.f64	%p78, %fd963, 0d7FF0000000000000;
	@%p78 bra 	BB0_117;

	mov.f64 	%fd964, 0d0000000000000000;
	mul.rn.f64 	%fd1806, %fd1806, %fd964;

BB0_117:
	mul.f64 	%fd965, %fd1806, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r490, %fd965;
	st.local.u32 	[%rd40], %r490;
	cvt.rn.f64.s32	%fd966, %r490;
	neg.f64 	%fd967, %fd966;
	fma.rn.f64 	%fd969, %fd967, %fd637, %fd1806;
	fma.rn.f64 	%fd971, %fd967, %fd639, %fd969;
	fma.rn.f64 	%fd1807, %fd967, %fd641, %fd971;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r273}, %fd1806;
	}
	and.b32  	%r274, %r273, 2145386496;
	setp.lt.u32	%p79, %r274, 1105199104;
	@%p79 bra 	BB0_119;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1806;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1807, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r490, [%rd40];

BB0_119:
	add.s32 	%r60, %r490, 1;
	and.b32  	%r275, %r60, 1;
	shl.b32 	%r276, %r275, 3;
	setp.eq.b32	%p80, %r275, 1;
	selp.f64	%fd973, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p80;
	mul.wide.u32 	%rd119, %r276, 8;
	add.s64 	%rd121, %rd119, %rd44;
	ld.const.f64 	%fd974, [%rd121+8];
	mul.rn.f64 	%fd213, %fd1807, %fd1807;
	fma.rn.f64 	%fd975, %fd973, %fd213, %fd974;
	ld.const.f64 	%fd976, [%rd121+16];
	fma.rn.f64 	%fd977, %fd975, %fd213, %fd976;
	ld.const.f64 	%fd978, [%rd121+24];
	fma.rn.f64 	%fd979, %fd977, %fd213, %fd978;
	ld.const.f64 	%fd980, [%rd121+32];
	fma.rn.f64 	%fd981, %fd979, %fd213, %fd980;
	ld.const.f64 	%fd982, [%rd121+40];
	fma.rn.f64 	%fd983, %fd981, %fd213, %fd982;
	ld.const.f64 	%fd984, [%rd121+48];
	fma.rn.f64 	%fd214, %fd983, %fd213, %fd984;
	fma.rn.f64 	%fd1808, %fd214, %fd1807, %fd1807;
	setp.eq.s32	%p81, %r275, 0;
	@%p81 bra 	BB0_121;

	mov.f64 	%fd985, 0d3FF0000000000000;
	fma.rn.f64 	%fd1808, %fd214, %fd213, %fd985;

BB0_121:
	and.b32  	%r277, %r60, 2;
	setp.eq.s32	%p82, %r277, 0;
	@%p82 bra 	BB0_123;

	mov.f64 	%fd986, 0d0000000000000000;
	mov.f64 	%fd987, 0dBFF0000000000000;
	fma.rn.f64 	%fd1808, %fd1808, %fd987, %fd986;

BB0_123:
	ld.global.f64 	%fd1809, [%rd2+40];
	abs.f64 	%fd988, %fd1809;
	setp.neu.f64	%p83, %fd988, 0d7FF0000000000000;
	@%p83 bra 	BB0_125;

	mov.f64 	%fd989, 0d0000000000000000;
	mul.rn.f64 	%fd1809, %fd1809, %fd989;

BB0_125:
	mul.f64 	%fd990, %fd1809, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r491, %fd990;
	st.local.u32 	[%rd40], %r491;
	cvt.rn.f64.s32	%fd991, %r491;
	neg.f64 	%fd992, %fd991;
	fma.rn.f64 	%fd994, %fd992, %fd637, %fd1809;
	fma.rn.f64 	%fd996, %fd992, %fd639, %fd994;
	fma.rn.f64 	%fd1810, %fd992, %fd641, %fd996;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r278}, %fd1809;
	}
	and.b32  	%r279, %r278, 2145386496;
	setp.lt.u32	%p84, %r279, 1105199104;
	@%p84 bra 	BB0_127;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1809;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1810, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r491, [%rd40];

BB0_127:
	add.s32 	%r64, %r491, 1;
	and.b32  	%r280, %r64, 1;
	shl.b32 	%r281, %r280, 3;
	setp.eq.b32	%p85, %r280, 1;
	selp.f64	%fd998, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p85;
	mul.wide.u32 	%rd126, %r281, 8;
	add.s64 	%rd128, %rd126, %rd44;
	ld.const.f64 	%fd999, [%rd128+8];
	mul.rn.f64 	%fd226, %fd1810, %fd1810;
	fma.rn.f64 	%fd1000, %fd998, %fd226, %fd999;
	ld.const.f64 	%fd1001, [%rd128+16];
	fma.rn.f64 	%fd1002, %fd1000, %fd226, %fd1001;
	ld.const.f64 	%fd1003, [%rd128+24];
	fma.rn.f64 	%fd1004, %fd1002, %fd226, %fd1003;
	ld.const.f64 	%fd1005, [%rd128+32];
	fma.rn.f64 	%fd1006, %fd1004, %fd226, %fd1005;
	ld.const.f64 	%fd1007, [%rd128+40];
	fma.rn.f64 	%fd1008, %fd1006, %fd226, %fd1007;
	ld.const.f64 	%fd1009, [%rd128+48];
	fma.rn.f64 	%fd227, %fd1008, %fd226, %fd1009;
	fma.rn.f64 	%fd1811, %fd227, %fd1810, %fd1810;
	setp.eq.s32	%p86, %r280, 0;
	@%p86 bra 	BB0_129;

	mov.f64 	%fd1010, 0d3FF0000000000000;
	fma.rn.f64 	%fd1811, %fd227, %fd226, %fd1010;

BB0_129:
	and.b32  	%r282, %r64, 2;
	setp.eq.s32	%p87, %r282, 0;
	@%p87 bra 	BB0_131;

	mov.f64 	%fd1011, 0d0000000000000000;
	mov.f64 	%fd1012, 0dBFF0000000000000;
	fma.rn.f64 	%fd1811, %fd1811, %fd1012, %fd1011;

BB0_131:
	ld.global.f64 	%fd1013, [%rd2+24];
	add.f64 	%fd1014, %fd1013, %fd1013;
	mul.f64 	%fd1015, %fd1013, %fd1014;
	mul.f64 	%fd1016, %fd1808, %fd1811;
	div.rn.f64 	%fd233, %fd1016, %fd1015;
	ld.global.f64 	%fd1812, [%rd2+40];
	abs.f64 	%fd1017, %fd1812;
	setp.neu.f64	%p88, %fd1017, 0d7FF0000000000000;
	@%p88 bra 	BB0_133;

	mov.f64 	%fd1018, 0d0000000000000000;
	mul.rn.f64 	%fd1812, %fd1812, %fd1018;

BB0_133:
	mul.f64 	%fd1019, %fd1812, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r492, %fd1019;
	st.local.u32 	[%rd40], %r492;
	cvt.rn.f64.s32	%fd1020, %r492;
	neg.f64 	%fd1021, %fd1020;
	fma.rn.f64 	%fd1023, %fd1021, %fd637, %fd1812;
	fma.rn.f64 	%fd1025, %fd1021, %fd639, %fd1023;
	fma.rn.f64 	%fd1813, %fd1021, %fd641, %fd1025;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r283}, %fd1812;
	}
	and.b32  	%r284, %r283, 2145386496;
	setp.lt.u32	%p89, %r284, 1105199104;
	@%p89 bra 	BB0_135;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1812;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1813, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r492, [%rd40];

BB0_135:
	and.b32  	%r285, %r492, 1;
	shl.b32 	%r286, %r285, 3;
	setp.eq.b32	%p90, %r285, 1;
	selp.f64	%fd1027, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p90;
	mul.wide.u32 	%rd133, %r286, 8;
	add.s64 	%rd135, %rd133, %rd44;
	ld.const.f64 	%fd1028, [%rd135+8];
	mul.rn.f64 	%fd240, %fd1813, %fd1813;
	fma.rn.f64 	%fd1029, %fd1027, %fd240, %fd1028;
	ld.const.f64 	%fd1030, [%rd135+16];
	fma.rn.f64 	%fd1031, %fd1029, %fd240, %fd1030;
	ld.const.f64 	%fd1032, [%rd135+24];
	fma.rn.f64 	%fd1033, %fd1031, %fd240, %fd1032;
	ld.const.f64 	%fd1034, [%rd135+32];
	fma.rn.f64 	%fd1035, %fd1033, %fd240, %fd1034;
	ld.const.f64 	%fd1036, [%rd135+40];
	fma.rn.f64 	%fd1037, %fd1035, %fd240, %fd1036;
	ld.const.f64 	%fd1038, [%rd135+48];
	fma.rn.f64 	%fd241, %fd1037, %fd240, %fd1038;
	fma.rn.f64 	%fd1814, %fd241, %fd1813, %fd1813;
	setp.eq.s32	%p91, %r285, 0;
	@%p91 bra 	BB0_137;

	mov.f64 	%fd1039, 0d3FF0000000000000;
	fma.rn.f64 	%fd1814, %fd241, %fd240, %fd1039;

BB0_137:
	and.b32  	%r287, %r492, 2;
	setp.eq.s32	%p92, %r287, 0;
	@%p92 bra 	BB0_139;

	mov.f64 	%fd1040, 0d0000000000000000;
	mov.f64 	%fd1041, 0dBFF0000000000000;
	fma.rn.f64 	%fd1814, %fd1814, %fd1041, %fd1040;

BB0_139:
	ld.global.f64 	%fd1815, [%rd2+40];
	abs.f64 	%fd1042, %fd1815;
	setp.neu.f64	%p93, %fd1042, 0d7FF0000000000000;
	@%p93 bra 	BB0_141;

	mov.f64 	%fd1043, 0d0000000000000000;
	mul.rn.f64 	%fd1815, %fd1815, %fd1043;

BB0_141:
	mul.f64 	%fd1044, %fd1815, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r493, %fd1044;
	st.local.u32 	[%rd40], %r493;
	cvt.rn.f64.s32	%fd1045, %r493;
	neg.f64 	%fd1046, %fd1045;
	fma.rn.f64 	%fd1048, %fd1046, %fd637, %fd1815;
	fma.rn.f64 	%fd1050, %fd1046, %fd639, %fd1048;
	fma.rn.f64 	%fd1816, %fd1046, %fd641, %fd1050;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r288}, %fd1815;
	}
	and.b32  	%r289, %r288, 2145386496;
	setp.lt.u32	%p94, %r289, 1105199104;
	@%p94 bra 	BB0_143;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1815;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1816, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r493, [%rd40];

BB0_143:
	and.b32  	%r290, %r493, 1;
	shl.b32 	%r291, %r290, 3;
	setp.eq.b32	%p95, %r290, 1;
	selp.f64	%fd1052, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p95;
	mul.wide.u32 	%rd140, %r291, 8;
	add.s64 	%rd142, %rd140, %rd44;
	ld.const.f64 	%fd1053, [%rd142+8];
	mul.rn.f64 	%fd253, %fd1816, %fd1816;
	fma.rn.f64 	%fd1054, %fd1052, %fd253, %fd1053;
	ld.const.f64 	%fd1055, [%rd142+16];
	fma.rn.f64 	%fd1056, %fd1054, %fd253, %fd1055;
	ld.const.f64 	%fd1057, [%rd142+24];
	fma.rn.f64 	%fd1058, %fd1056, %fd253, %fd1057;
	ld.const.f64 	%fd1059, [%rd142+32];
	fma.rn.f64 	%fd1060, %fd1058, %fd253, %fd1059;
	ld.const.f64 	%fd1061, [%rd142+40];
	fma.rn.f64 	%fd1062, %fd1060, %fd253, %fd1061;
	ld.const.f64 	%fd1063, [%rd142+48];
	fma.rn.f64 	%fd254, %fd1062, %fd253, %fd1063;
	fma.rn.f64 	%fd1817, %fd254, %fd1816, %fd1816;
	setp.eq.s32	%p96, %r290, 0;
	@%p96 bra 	BB0_145;

	mov.f64 	%fd1064, 0d3FF0000000000000;
	fma.rn.f64 	%fd1817, %fd254, %fd253, %fd1064;

BB0_145:
	and.b32  	%r292, %r493, 2;
	setp.eq.s32	%p97, %r292, 0;
	@%p97 bra 	BB0_147;

	mov.f64 	%fd1065, 0d0000000000000000;
	mov.f64 	%fd1066, 0dBFF0000000000000;
	fma.rn.f64 	%fd1817, %fd1817, %fd1066, %fd1065;

BB0_147:
	ld.global.f64 	%fd1067, [%rd2+32];
	add.f64 	%fd1068, %fd1067, %fd1067;
	mul.f64 	%fd1069, %fd1067, %fd1068;
	mul.f64 	%fd1070, %fd1814, %fd1817;
	div.rn.f64 	%fd1071, %fd1070, %fd1069;
	add.f64 	%fd1728, %fd233, %fd1071;
	ld.global.f64 	%fd1072, [%rd2+40];
	add.f64 	%fd1818, %fd1072, %fd1072;
	abs.f64 	%fd1073, %fd1818;
	setp.neu.f64	%p98, %fd1073, 0d7FF0000000000000;
	@%p98 bra 	BB0_149;

	mov.f64 	%fd1074, 0d0000000000000000;
	mul.rn.f64 	%fd1818, %fd1818, %fd1074;

BB0_149:
	mul.f64 	%fd1075, %fd1818, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r494, %fd1075;
	st.local.u32 	[%rd40], %r494;
	cvt.rn.f64.s32	%fd1076, %r494;
	neg.f64 	%fd1077, %fd1076;
	fma.rn.f64 	%fd1079, %fd1077, %fd637, %fd1818;
	fma.rn.f64 	%fd1081, %fd1077, %fd639, %fd1079;
	fma.rn.f64 	%fd1819, %fd1077, %fd641, %fd1081;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r293}, %fd1818;
	}
	and.b32  	%r294, %r293, 2145386496;
	setp.lt.u32	%p99, %r294, 1105199104;
	@%p99 bra 	BB0_151;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1818;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1819, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r494, [%rd40];

BB0_151:
	and.b32  	%r295, %r494, 1;
	shl.b32 	%r296, %r295, 3;
	setp.eq.b32	%p100, %r295, 1;
	selp.f64	%fd1083, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p100;
	mul.wide.u32 	%rd147, %r296, 8;
	add.s64 	%rd149, %rd147, %rd44;
	ld.const.f64 	%fd1084, [%rd149+8];
	mul.rn.f64 	%fd267, %fd1819, %fd1819;
	fma.rn.f64 	%fd1085, %fd1083, %fd267, %fd1084;
	ld.const.f64 	%fd1086, [%rd149+16];
	fma.rn.f64 	%fd1087, %fd1085, %fd267, %fd1086;
	ld.const.f64 	%fd1088, [%rd149+24];
	fma.rn.f64 	%fd1089, %fd1087, %fd267, %fd1088;
	ld.const.f64 	%fd1090, [%rd149+32];
	fma.rn.f64 	%fd1091, %fd1089, %fd267, %fd1090;
	ld.const.f64 	%fd1092, [%rd149+40];
	fma.rn.f64 	%fd1093, %fd1091, %fd267, %fd1092;
	ld.const.f64 	%fd1094, [%rd149+48];
	fma.rn.f64 	%fd268, %fd1093, %fd267, %fd1094;
	fma.rn.f64 	%fd1820, %fd268, %fd1819, %fd1819;
	setp.eq.s32	%p101, %r295, 0;
	@%p101 bra 	BB0_153;

	mov.f64 	%fd1095, 0d3FF0000000000000;
	fma.rn.f64 	%fd1820, %fd268, %fd267, %fd1095;

BB0_153:
	and.b32  	%r297, %r494, 2;
	setp.eq.s32	%p102, %r297, 0;
	@%p102 bra 	BB0_155;

	mov.f64 	%fd1096, 0d0000000000000000;
	mov.f64 	%fd1097, 0dBFF0000000000000;
	fma.rn.f64 	%fd1820, %fd1820, %fd1097, %fd1096;

BB0_155:
	ld.global.f64 	%fd1098, [%rd2+24];
	mul.f64 	%fd1099, %fd1098, 0dC010000000000000;
	mul.f64 	%fd1100, %fd1098, %fd1099;
	div.rn.f64 	%fd274, %fd1820, %fd1100;
	ld.global.f64 	%fd1101, [%rd2+40];
	add.f64 	%fd1821, %fd1101, %fd1101;
	abs.f64 	%fd1102, %fd1821;
	setp.neu.f64	%p103, %fd1102, 0d7FF0000000000000;
	@%p103 bra 	BB0_157;

	mov.f64 	%fd1103, 0d0000000000000000;
	mul.rn.f64 	%fd1821, %fd1821, %fd1103;

BB0_157:
	mul.f64 	%fd1104, %fd1821, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r495, %fd1104;
	st.local.u32 	[%rd40], %r495;
	cvt.rn.f64.s32	%fd1105, %r495;
	neg.f64 	%fd1106, %fd1105;
	fma.rn.f64 	%fd1108, %fd1106, %fd637, %fd1821;
	fma.rn.f64 	%fd1110, %fd1106, %fd639, %fd1108;
	fma.rn.f64 	%fd1822, %fd1106, %fd641, %fd1110;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r298}, %fd1821;
	}
	and.b32  	%r299, %r298, 2145386496;
	setp.lt.u32	%p104, %r299, 1105199104;
	@%p104 bra 	BB0_159;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1821;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1822, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r495, [%rd40];

BB0_159:
	and.b32  	%r300, %r495, 1;
	shl.b32 	%r301, %r300, 3;
	setp.eq.b32	%p105, %r300, 1;
	selp.f64	%fd1112, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p105;
	mul.wide.u32 	%rd154, %r301, 8;
	add.s64 	%rd156, %rd154, %rd44;
	ld.const.f64 	%fd1113, [%rd156+8];
	mul.rn.f64 	%fd281, %fd1822, %fd1822;
	fma.rn.f64 	%fd1114, %fd1112, %fd281, %fd1113;
	ld.const.f64 	%fd1115, [%rd156+16];
	fma.rn.f64 	%fd1116, %fd1114, %fd281, %fd1115;
	ld.const.f64 	%fd1117, [%rd156+24];
	fma.rn.f64 	%fd1118, %fd1116, %fd281, %fd1117;
	ld.const.f64 	%fd1119, [%rd156+32];
	fma.rn.f64 	%fd1120, %fd1118, %fd281, %fd1119;
	ld.const.f64 	%fd1121, [%rd156+40];
	fma.rn.f64 	%fd1122, %fd1120, %fd281, %fd1121;
	ld.const.f64 	%fd1123, [%rd156+48];
	fma.rn.f64 	%fd282, %fd1122, %fd281, %fd1123;
	fma.rn.f64 	%fd1823, %fd282, %fd1822, %fd1822;
	setp.eq.s32	%p106, %r300, 0;
	@%p106 bra 	BB0_161;

	mov.f64 	%fd1124, 0d3FF0000000000000;
	fma.rn.f64 	%fd1823, %fd282, %fd281, %fd1124;

BB0_161:
	and.b32  	%r302, %r495, 2;
	setp.eq.s32	%p107, %r302, 0;
	@%p107 bra 	BB0_163;

	mov.f64 	%fd1125, 0d0000000000000000;
	mov.f64 	%fd1126, 0dBFF0000000000000;
	fma.rn.f64 	%fd1823, %fd1823, %fd1126, %fd1125;

BB0_163:
	ld.global.f64 	%fd1127, [%rd2+32];
	mul.f64 	%fd1128, %fd1127, 0d4010000000000000;
	mul.f64 	%fd1129, %fd1127, %fd1128;
	div.rn.f64 	%fd1130, %fd1823, %fd1129;
	add.f64 	%fd1727, %fd274, %fd1130;
	ld.global.f64 	%fd1824, [%rd2+40];
	abs.f64 	%fd1131, %fd1824;
	setp.neu.f64	%p108, %fd1131, 0d7FF0000000000000;
	@%p108 bra 	BB0_165;

	mov.f64 	%fd1132, 0d0000000000000000;
	mul.rn.f64 	%fd1824, %fd1824, %fd1132;

BB0_165:
	mul.f64 	%fd1133, %fd1824, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r496, %fd1133;
	st.local.u32 	[%rd40], %r496;
	cvt.rn.f64.s32	%fd1134, %r496;
	neg.f64 	%fd1135, %fd1134;
	fma.rn.f64 	%fd1137, %fd1135, %fd637, %fd1824;
	fma.rn.f64 	%fd1139, %fd1135, %fd639, %fd1137;
	fma.rn.f64 	%fd1825, %fd1135, %fd641, %fd1139;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r303}, %fd1824;
	}
	and.b32  	%r304, %r303, 2145386496;
	setp.lt.u32	%p109, %r304, 1105199104;
	@%p109 bra 	BB0_167;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1824;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1825, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r496, [%rd40];

BB0_167:
	and.b32  	%r305, %r496, 1;
	shl.b32 	%r306, %r305, 3;
	setp.eq.b32	%p110, %r305, 1;
	selp.f64	%fd1141, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p110;
	mul.wide.u32 	%rd161, %r306, 8;
	add.s64 	%rd163, %rd161, %rd44;
	ld.const.f64 	%fd1142, [%rd163+8];
	mul.rn.f64 	%fd295, %fd1825, %fd1825;
	fma.rn.f64 	%fd1143, %fd1141, %fd295, %fd1142;
	ld.const.f64 	%fd1144, [%rd163+16];
	fma.rn.f64 	%fd1145, %fd1143, %fd295, %fd1144;
	ld.const.f64 	%fd1146, [%rd163+24];
	fma.rn.f64 	%fd1147, %fd1145, %fd295, %fd1146;
	ld.const.f64 	%fd1148, [%rd163+32];
	fma.rn.f64 	%fd1149, %fd1147, %fd295, %fd1148;
	ld.const.f64 	%fd1150, [%rd163+40];
	fma.rn.f64 	%fd1151, %fd1149, %fd295, %fd1150;
	ld.const.f64 	%fd1152, [%rd163+48];
	fma.rn.f64 	%fd296, %fd1151, %fd295, %fd1152;
	fma.rn.f64 	%fd1826, %fd296, %fd1825, %fd1825;
	setp.eq.s32	%p111, %r305, 0;
	@%p111 bra 	BB0_169;

	mov.f64 	%fd1153, 0d3FF0000000000000;
	fma.rn.f64 	%fd1826, %fd296, %fd295, %fd1153;

BB0_169:
	and.b32  	%r307, %r496, 2;
	setp.eq.s32	%p112, %r307, 0;
	@%p112 bra 	BB0_171;

	mov.f64 	%fd1154, 0d0000000000000000;
	mov.f64 	%fd1155, 0dBFF0000000000000;
	fma.rn.f64 	%fd1826, %fd1826, %fd1155, %fd1154;

BB0_171:
	ld.global.f64 	%fd1827, [%rd2+40];
	abs.f64 	%fd1156, %fd1827;
	setp.neu.f64	%p113, %fd1156, 0d7FF0000000000000;
	@%p113 bra 	BB0_173;

	mov.f64 	%fd1157, 0d0000000000000000;
	mul.rn.f64 	%fd1827, %fd1827, %fd1157;

BB0_173:
	mul.f64 	%fd1158, %fd1827, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r497, %fd1158;
	st.local.u32 	[%rd40], %r497;
	cvt.rn.f64.s32	%fd1159, %r497;
	neg.f64 	%fd1160, %fd1159;
	fma.rn.f64 	%fd1162, %fd1160, %fd637, %fd1827;
	fma.rn.f64 	%fd1164, %fd1160, %fd639, %fd1162;
	fma.rn.f64 	%fd1828, %fd1160, %fd641, %fd1164;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r308}, %fd1827;
	}
	and.b32  	%r309, %r308, 2145386496;
	setp.lt.u32	%p114, %r309, 1105199104;
	@%p114 bra 	BB0_175;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1827;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1828, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r497, [%rd40];

BB0_175:
	and.b32  	%r310, %r497, 1;
	shl.b32 	%r311, %r310, 3;
	setp.eq.b32	%p115, %r310, 1;
	selp.f64	%fd1166, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p115;
	mul.wide.u32 	%rd168, %r311, 8;
	add.s64 	%rd170, %rd168, %rd44;
	ld.const.f64 	%fd1167, [%rd170+8];
	mul.rn.f64 	%fd308, %fd1828, %fd1828;
	fma.rn.f64 	%fd1168, %fd1166, %fd308, %fd1167;
	ld.const.f64 	%fd1169, [%rd170+16];
	fma.rn.f64 	%fd1170, %fd1168, %fd308, %fd1169;
	ld.const.f64 	%fd1171, [%rd170+24];
	fma.rn.f64 	%fd1172, %fd1170, %fd308, %fd1171;
	ld.const.f64 	%fd1173, [%rd170+32];
	fma.rn.f64 	%fd1174, %fd1172, %fd308, %fd1173;
	ld.const.f64 	%fd1175, [%rd170+40];
	fma.rn.f64 	%fd1176, %fd1174, %fd308, %fd1175;
	ld.const.f64 	%fd1177, [%rd170+48];
	fma.rn.f64 	%fd309, %fd1176, %fd308, %fd1177;
	fma.rn.f64 	%fd1829, %fd309, %fd1828, %fd1828;
	setp.eq.s32	%p116, %r310, 0;
	@%p116 bra 	BB0_177;

	mov.f64 	%fd1178, 0d3FF0000000000000;
	fma.rn.f64 	%fd1829, %fd309, %fd308, %fd1178;

BB0_177:
	and.b32  	%r312, %r497, 2;
	setp.eq.s32	%p117, %r312, 0;
	@%p117 bra 	BB0_179;

	mov.f64 	%fd1179, 0d0000000000000000;
	mov.f64 	%fd1180, 0dBFF0000000000000;
	fma.rn.f64 	%fd1829, %fd1829, %fd1180, %fd1179;

BB0_179:
	ld.global.f64 	%fd1181, [%rd2+24];
	add.f64 	%fd1182, %fd1181, %fd1181;
	mul.f64 	%fd1183, %fd1181, %fd1182;
	mul.f64 	%fd1184, %fd1826, %fd1829;
	div.rn.f64 	%fd315, %fd1184, %fd1183;
	ld.global.f64 	%fd1830, [%rd2+40];
	abs.f64 	%fd1185, %fd1830;
	setp.neu.f64	%p118, %fd1185, 0d7FF0000000000000;
	@%p118 bra 	BB0_181;

	mov.f64 	%fd1186, 0d0000000000000000;
	mul.rn.f64 	%fd1830, %fd1830, %fd1186;

BB0_181:
	mul.f64 	%fd1187, %fd1830, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r498, %fd1187;
	st.local.u32 	[%rd40], %r498;
	cvt.rn.f64.s32	%fd1188, %r498;
	neg.f64 	%fd1189, %fd1188;
	fma.rn.f64 	%fd1191, %fd1189, %fd637, %fd1830;
	fma.rn.f64 	%fd1193, %fd1189, %fd639, %fd1191;
	fma.rn.f64 	%fd1831, %fd1189, %fd641, %fd1193;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r313}, %fd1830;
	}
	and.b32  	%r314, %r313, 2145386496;
	setp.lt.u32	%p119, %r314, 1105199104;
	@%p119 bra 	BB0_183;

	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1830;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1831, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r498, [%rd40];

BB0_183:
	add.s32 	%r86, %r498, 1;
	and.b32  	%r315, %r86, 1;
	shl.b32 	%r316, %r315, 3;
	setp.eq.b32	%p120, %r315, 1;
	selp.f64	%fd1195, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p120;
	mul.wide.u32 	%rd175, %r316, 8;
	add.s64 	%rd177, %rd175, %rd44;
	ld.const.f64 	%fd1196, [%rd177+8];
	mul.rn.f64 	%fd322, %fd1831, %fd1831;
	fma.rn.f64 	%fd1197, %fd1195, %fd322, %fd1196;
	ld.const.f64 	%fd1198, [%rd177+16];
	fma.rn.f64 	%fd1199, %fd1197, %fd322, %fd1198;
	ld.const.f64 	%fd1200, [%rd177+24];
	fma.rn.f64 	%fd1201, %fd1199, %fd322, %fd1200;
	ld.const.f64 	%fd1202, [%rd177+32];
	fma.rn.f64 	%fd1203, %fd1201, %fd322, %fd1202;
	ld.const.f64 	%fd1204, [%rd177+40];
	fma.rn.f64 	%fd1205, %fd1203, %fd322, %fd1204;
	ld.const.f64 	%fd1206, [%rd177+48];
	fma.rn.f64 	%fd323, %fd1205, %fd322, %fd1206;
	fma.rn.f64 	%fd1832, %fd323, %fd1831, %fd1831;
	setp.eq.s32	%p121, %r315, 0;
	@%p121 bra 	BB0_185;

	mov.f64 	%fd1207, 0d3FF0000000000000;
	fma.rn.f64 	%fd1832, %fd323, %fd322, %fd1207;

BB0_185:
	and.b32  	%r317, %r86, 2;
	setp.eq.s32	%p122, %r317, 0;
	@%p122 bra 	BB0_187;

	mov.f64 	%fd1208, 0d0000000000000000;
	mov.f64 	%fd1209, 0dBFF0000000000000;
	fma.rn.f64 	%fd1832, %fd1832, %fd1209, %fd1208;

BB0_187:
	ld.global.f64 	%fd1833, [%rd2+40];
	abs.f64 	%fd1210, %fd1833;
	setp.neu.f64	%p123, %fd1210, 0d7FF0000000000000;
	@%p123 bra 	BB0_189;

	mov.f64 	%fd1211, 0d0000000000000000;
	mul.rn.f64 	%fd1833, %fd1833, %fd1211;

BB0_189:
	mul.f64 	%fd1212, %fd1833, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r499, %fd1212;
	st.local.u32 	[%rd40], %r499;
	cvt.rn.f64.s32	%fd1213, %r499;
	neg.f64 	%fd1214, %fd1213;
	fma.rn.f64 	%fd1216, %fd1214, %fd637, %fd1833;
	fma.rn.f64 	%fd1218, %fd1214, %fd639, %fd1216;
	fma.rn.f64 	%fd1834, %fd1214, %fd641, %fd1218;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r318}, %fd1833;
	}
	and.b32  	%r319, %r318, 2145386496;
	setp.lt.u32	%p124, %r319, 1105199104;
	@%p124 bra 	BB0_191;

	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1833;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1834, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r499, [%rd40];

BB0_191:
	add.s32 	%r90, %r499, 1;
	and.b32  	%r320, %r90, 1;
	shl.b32 	%r321, %r320, 3;
	setp.eq.b32	%p125, %r320, 1;
	selp.f64	%fd1220, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p125;
	mul.wide.u32 	%rd182, %r321, 8;
	add.s64 	%rd184, %rd182, %rd44;
	ld.const.f64 	%fd1221, [%rd184+8];
	mul.rn.f64 	%fd335, %fd1834, %fd1834;
	fma.rn.f64 	%fd1222, %fd1220, %fd335, %fd1221;
	ld.const.f64 	%fd1223, [%rd184+16];
	fma.rn.f64 	%fd1224, %fd1222, %fd335, %fd1223;
	ld.const.f64 	%fd1225, [%rd184+24];
	fma.rn.f64 	%fd1226, %fd1224, %fd335, %fd1225;
	ld.const.f64 	%fd1227, [%rd184+32];
	fma.rn.f64 	%fd1228, %fd1226, %fd335, %fd1227;
	ld.const.f64 	%fd1229, [%rd184+40];
	fma.rn.f64 	%fd1230, %fd1228, %fd335, %fd1229;
	ld.const.f64 	%fd1231, [%rd184+48];
	fma.rn.f64 	%fd336, %fd1230, %fd335, %fd1231;
	fma.rn.f64 	%fd1835, %fd336, %fd1834, %fd1834;
	setp.eq.s32	%p126, %r320, 0;
	@%p126 bra 	BB0_193;

	mov.f64 	%fd1232, 0d3FF0000000000000;
	fma.rn.f64 	%fd1835, %fd336, %fd335, %fd1232;

BB0_193:
	and.b32  	%r322, %r90, 2;
	setp.eq.s32	%p127, %r322, 0;
	@%p127 bra 	BB0_195;

	mov.f64 	%fd1233, 0d0000000000000000;
	mov.f64 	%fd1234, 0dBFF0000000000000;
	fma.rn.f64 	%fd1835, %fd1835, %fd1234, %fd1233;

BB0_195:
	ld.global.f64 	%fd1235, [%rd2+32];
	add.f64 	%fd1236, %fd1235, %fd1235;
	mul.f64 	%fd1237, %fd1235, %fd1236;
	mul.f64 	%fd1238, %fd1832, %fd1835;
	div.rn.f64 	%fd1239, %fd1238, %fd1237;
	add.f64 	%fd1726, %fd315, %fd1239;
	ld.global.f64 	%fd343, [%rd5];
	setp.lt.f64	%p128, %fd343, 0d0000000000000000;
	@%p128 bra 	BB0_197;
	bra.uni 	BB0_196;

BB0_197:
	setp.lt.s32	%p129, %r16, 20;
	@%p129 bra 	BB0_199;
	bra.uni 	BB0_198;

BB0_199:
	mul.f64 	%fd1242, %fd343, 0dBFD3333333333333;
	st.global.f64 	[%rd5], %fd1242;
	mov.f64 	%fd1735, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1735;
	bra.uni 	BB0_237;

BB0_23:
	setp.lt.f64	%p20, %fd55, 0d0000000000000000;
	@%p20 bra 	BB0_25;
	bra.uni 	BB0_24;

BB0_25:
	setp.lt.s32	%p21, %r16, 20;
	@%p21 bra 	BB0_27;
	bra.uni 	BB0_26;

BB0_27:
	mul.f64 	%fd631, %fd55, 0dBFD3333333333333;
	st.global.f64 	[%rd5], %fd631;
	mov.f64 	%fd1731, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1731;
	bra.uni 	BB0_237;

BB0_201:
	setp.lt.f64	%p133, %fd344, 0d0000000000000000;
	@%p133 bra 	BB0_203;
	bra.uni 	BB0_202;

BB0_203:
	setp.lt.s32	%p134, %r16, 20;
	@%p134 bra 	BB0_205;
	bra.uni 	BB0_204;

BB0_205:
	mul.f64 	%fd1246, %fd344, 0dBFD3333333333333;
	st.global.f64 	[%rd3+48], %fd1246;
	mov.f64 	%fd1738, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1738;
	bra.uni 	BB0_237;

BB0_220:
	neg.f64 	%fd1304, %fd360;
	st.global.f64 	[%rd3], %fd1304;
	bra.uni 	BB0_236;

BB0_24:
	neg.f64 	%fd629, %fd55;
	st.global.f64 	[%rd5], %fd629;
	mov.f64 	%fd1732, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1732;
	bra.uni 	BB0_237;

BB0_232:
	neg.f64 	%fd1360, %fd375;
	st.global.f64 	[%rd3], %fd1360;
	bra.uni 	BB0_236;

BB0_222:
	mul.f64 	%fd1305, %fd360, 0dBFE6666666666666;
	st.global.f64 	[%rd3], %fd1305;
	bra.uni 	BB0_236;

BB0_202:
	neg.f64 	%fd1244, %fd344;
	st.global.f64 	[%rd3+48], %fd1244;
	mov.f64 	%fd1736, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1736;
	bra.uni 	BB0_237;

BB0_26:
	mul.f64 	%fd630, %fd55, 0dBFE6666666666666;
	st.global.f64 	[%rd5], %fd630;
	mov.f64 	%fd1730, %fd53;
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd1730;
	bra.uni 	BB0_237;

BB0_234:
	mul.f64 	%fd1361, %fd375, 0dBFE6666666666666;
	st.global.f64 	[%rd3], %fd1361;

BB0_236:
	mov.f64 	%fd1773, %fd54;
	mov.f64 	%fd1771, %fd54;

BB0_237:
	mov.f64 	%fd1772, %fd1773;
	mov.f64 	%fd1770, %fd1771;
	add.s32 	%r477, %r476, 1;
	setp.lt.s32	%p156, %r476, 6;
	@%p156 bra 	BB0_240;

	mov.u32 	%r477, 0;
	setp.lt.s32	%p157, %r16, 250;
	@%p157 bra 	BB0_240;

	ld.param.f64 	%fd1697, [gaussFitter_param_9];
	sub.f64 	%fd1363, %fd1770, %fd1772;
	setp.lt.f64	%p158, %fd1363, %fd1697;
	selp.b16	%rs8, 0, %rs8, %p158;

BB0_240:
	mov.u32 	%r476, %r477;
	ld.param.u32 	%r470, [gaussFitter_param_10];
	add.s32 	%r478, %r16, 1;
	setp.ge.s32	%p159, %r16, %r470;
	selp.b16	%rs8, 0, %rs8, %p159;
	and.b16  	%rs7, %rs8, 255;
	setp.ne.s16	%p160, %rs7, 0;
	@%p160 bra 	BB0_19;

	ld.global.f64 	%fd1840, [%rd2+40];
	abs.f64 	%fd1364, %fd1840;
	setp.neu.f64	%p161, %fd1364, 0d7FF0000000000000;
	@%p161 bra 	BB0_243;

	mov.f64 	%fd1365, 0d0000000000000000;
	mul.rn.f64 	%fd1840, %fd1840, %fd1365;

BB0_243:
	mul.f64 	%fd1366, %fd1840, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r502, %fd1366;
	add.u64 	%rd202, %SP, 0;
	cvta.to.local.u64 	%rd203, %rd202;
	st.local.u32 	[%rd203], %r502;
	cvt.rn.f64.s32	%fd1367, %r502;
	neg.f64 	%fd1368, %fd1367;
	mov.f64 	%fd1369, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd1370, %fd1368, %fd1369, %fd1840;
	mov.f64 	%fd1371, 0d3C91A62633145C00;
	fma.rn.f64 	%fd1372, %fd1368, %fd1371, %fd1370;
	mov.f64 	%fd1373, 0d397B839A252049C0;
	fma.rn.f64 	%fd1841, %fd1368, %fd1373, %fd1372;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r391}, %fd1840;
	}
	and.b32  	%r392, %r391, 2145386496;
	setp.lt.u32	%p162, %r392, 1105199104;
	@%p162 bra 	BB0_245;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1840;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1841, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r502, [%rd203];

BB0_245:
	add.s32 	%r110, %r502, 1;
	and.b32  	%r393, %r110, 1;
	shl.b32 	%r394, %r393, 3;
	setp.eq.b32	%p163, %r393, 1;
	selp.f64	%fd1374, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p163;
	mul.wide.u32 	%rd206, %r394, 8;
	mov.u64 	%rd207, __cudart_sin_cos_coeffs;
	add.s64 	%rd208, %rd206, %rd207;
	ld.const.f64 	%fd1375, [%rd208+8];
	mul.rn.f64 	%fd387, %fd1841, %fd1841;
	fma.rn.f64 	%fd1376, %fd1374, %fd387, %fd1375;
	ld.const.f64 	%fd1377, [%rd208+16];
	fma.rn.f64 	%fd1378, %fd1376, %fd387, %fd1377;
	ld.const.f64 	%fd1379, [%rd208+24];
	fma.rn.f64 	%fd1380, %fd1378, %fd387, %fd1379;
	ld.const.f64 	%fd1381, [%rd208+32];
	fma.rn.f64 	%fd1382, %fd1380, %fd387, %fd1381;
	ld.const.f64 	%fd1383, [%rd208+40];
	fma.rn.f64 	%fd1384, %fd1382, %fd387, %fd1383;
	ld.const.f64 	%fd1385, [%rd208+48];
	fma.rn.f64 	%fd388, %fd1384, %fd387, %fd1385;
	fma.rn.f64 	%fd1842, %fd388, %fd1841, %fd1841;
	setp.eq.s32	%p164, %r393, 0;
	@%p164 bra 	BB0_247;

	mov.f64 	%fd1386, 0d3FF0000000000000;
	fma.rn.f64 	%fd1842, %fd388, %fd387, %fd1386;

BB0_247:
	and.b32  	%r395, %r110, 2;
	setp.eq.s32	%p165, %r395, 0;
	@%p165 bra 	BB0_249;

	mov.f64 	%fd1387, 0d0000000000000000;
	mov.f64 	%fd1388, 0dBFF0000000000000;
	fma.rn.f64 	%fd1842, %fd1842, %fd1388, %fd1387;

BB0_249:
	ld.global.f64 	%fd1843, [%rd2+40];
	abs.f64 	%fd1389, %fd1843;
	setp.neu.f64	%p166, %fd1389, 0d7FF0000000000000;
	@%p166 bra 	BB0_251;

	mov.f64 	%fd1390, 0d0000000000000000;
	mul.rn.f64 	%fd1843, %fd1843, %fd1390;

BB0_251:
	mul.f64 	%fd1391, %fd1843, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r503, %fd1391;
	st.local.u32 	[%rd203], %r503;
	cvt.rn.f64.s32	%fd1392, %r503;
	neg.f64 	%fd1393, %fd1392;
	fma.rn.f64 	%fd1395, %fd1393, %fd1369, %fd1843;
	fma.rn.f64 	%fd1397, %fd1393, %fd1371, %fd1395;
	fma.rn.f64 	%fd1844, %fd1393, %fd1373, %fd1397;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r396}, %fd1843;
	}
	and.b32  	%r397, %r396, 2145386496;
	setp.lt.u32	%p167, %r397, 1105199104;
	@%p167 bra 	BB0_253;

	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1843;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1844, [retval0+0];
	
	//{
	}// Callseq End 21
	ld.local.u32 	%r503, [%rd203];

BB0_253:
	add.s32 	%r114, %r503, 1;
	and.b32  	%r398, %r114, 1;
	shl.b32 	%r399, %r398, 3;
	setp.eq.b32	%p168, %r398, 1;
	selp.f64	%fd1399, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p168;
	mul.wide.u32 	%rd213, %r399, 8;
	add.s64 	%rd215, %rd213, %rd207;
	ld.const.f64 	%fd1400, [%rd215+8];
	mul.rn.f64 	%fd400, %fd1844, %fd1844;
	fma.rn.f64 	%fd1401, %fd1399, %fd400, %fd1400;
	ld.const.f64 	%fd1402, [%rd215+16];
	fma.rn.f64 	%fd1403, %fd1401, %fd400, %fd1402;
	ld.const.f64 	%fd1404, [%rd215+24];
	fma.rn.f64 	%fd1405, %fd1403, %fd400, %fd1404;
	ld.const.f64 	%fd1406, [%rd215+32];
	fma.rn.f64 	%fd1407, %fd1405, %fd400, %fd1406;
	ld.const.f64 	%fd1408, [%rd215+40];
	fma.rn.f64 	%fd1409, %fd1407, %fd400, %fd1408;
	ld.const.f64 	%fd1410, [%rd215+48];
	fma.rn.f64 	%fd401, %fd1409, %fd400, %fd1410;
	fma.rn.f64 	%fd1845, %fd401, %fd1844, %fd1844;
	setp.eq.s32	%p169, %r398, 0;
	@%p169 bra 	BB0_255;

	mov.f64 	%fd1411, 0d3FF0000000000000;
	fma.rn.f64 	%fd1845, %fd401, %fd400, %fd1411;

BB0_255:
	and.b32  	%r400, %r114, 2;
	setp.eq.s32	%p170, %r400, 0;
	@%p170 bra 	BB0_257;

	mov.f64 	%fd1412, 0d0000000000000000;
	mov.f64 	%fd1413, 0dBFF0000000000000;
	fma.rn.f64 	%fd1845, %fd1845, %fd1413, %fd1412;

BB0_257:
	ld.global.f64 	%fd1414, [%rd2+24];
	add.f64 	%fd1415, %fd1414, %fd1414;
	mul.f64 	%fd1416, %fd1414, %fd1415;
	mul.f64 	%fd1417, %fd1842, %fd1845;
	div.rn.f64 	%fd407, %fd1417, %fd1416;
	ld.global.f64 	%fd1846, [%rd2+40];
	abs.f64 	%fd1418, %fd1846;
	setp.neu.f64	%p171, %fd1418, 0d7FF0000000000000;
	@%p171 bra 	BB0_259;

	mov.f64 	%fd1419, 0d0000000000000000;
	mul.rn.f64 	%fd1846, %fd1846, %fd1419;

BB0_259:
	mul.f64 	%fd1420, %fd1846, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r504, %fd1420;
	st.local.u32 	[%rd203], %r504;
	cvt.rn.f64.s32	%fd1421, %r504;
	neg.f64 	%fd1422, %fd1421;
	fma.rn.f64 	%fd1424, %fd1422, %fd1369, %fd1846;
	fma.rn.f64 	%fd1426, %fd1422, %fd1371, %fd1424;
	fma.rn.f64 	%fd1847, %fd1422, %fd1373, %fd1426;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r401}, %fd1846;
	}
	and.b32  	%r402, %r401, 2145386496;
	setp.lt.u32	%p172, %r402, 1105199104;
	@%p172 bra 	BB0_261;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1846;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1847, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r504, [%rd203];

BB0_261:
	and.b32  	%r403, %r504, 1;
	shl.b32 	%r404, %r403, 3;
	setp.eq.b32	%p173, %r403, 1;
	selp.f64	%fd1428, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p173;
	mul.wide.u32 	%rd220, %r404, 8;
	add.s64 	%rd222, %rd220, %rd207;
	ld.const.f64 	%fd1429, [%rd222+8];
	mul.rn.f64 	%fd414, %fd1847, %fd1847;
	fma.rn.f64 	%fd1430, %fd1428, %fd414, %fd1429;
	ld.const.f64 	%fd1431, [%rd222+16];
	fma.rn.f64 	%fd1432, %fd1430, %fd414, %fd1431;
	ld.const.f64 	%fd1433, [%rd222+24];
	fma.rn.f64 	%fd1434, %fd1432, %fd414, %fd1433;
	ld.const.f64 	%fd1435, [%rd222+32];
	fma.rn.f64 	%fd1436, %fd1434, %fd414, %fd1435;
	ld.const.f64 	%fd1437, [%rd222+40];
	fma.rn.f64 	%fd1438, %fd1436, %fd414, %fd1437;
	ld.const.f64 	%fd1439, [%rd222+48];
	fma.rn.f64 	%fd415, %fd1438, %fd414, %fd1439;
	fma.rn.f64 	%fd1848, %fd415, %fd1847, %fd1847;
	setp.eq.s32	%p174, %r403, 0;
	@%p174 bra 	BB0_263;

	mov.f64 	%fd1440, 0d3FF0000000000000;
	fma.rn.f64 	%fd1848, %fd415, %fd414, %fd1440;

BB0_263:
	and.b32  	%r405, %r504, 2;
	setp.eq.s32	%p175, %r405, 0;
	@%p175 bra 	BB0_265;

	mov.f64 	%fd1441, 0d0000000000000000;
	mov.f64 	%fd1442, 0dBFF0000000000000;
	fma.rn.f64 	%fd1848, %fd1848, %fd1442, %fd1441;

BB0_265:
	ld.global.f64 	%fd1849, [%rd2+40];
	abs.f64 	%fd1443, %fd1849;
	setp.neu.f64	%p176, %fd1443, 0d7FF0000000000000;
	@%p176 bra 	BB0_267;

	mov.f64 	%fd1444, 0d0000000000000000;
	mul.rn.f64 	%fd1849, %fd1849, %fd1444;

BB0_267:
	mul.f64 	%fd1445, %fd1849, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r505, %fd1445;
	st.local.u32 	[%rd203], %r505;
	cvt.rn.f64.s32	%fd1446, %r505;
	neg.f64 	%fd1447, %fd1446;
	fma.rn.f64 	%fd1449, %fd1447, %fd1369, %fd1849;
	fma.rn.f64 	%fd1451, %fd1447, %fd1371, %fd1449;
	fma.rn.f64 	%fd1850, %fd1447, %fd1373, %fd1451;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r406}, %fd1849;
	}
	and.b32  	%r407, %r406, 2145386496;
	setp.lt.u32	%p177, %r407, 1105199104;
	@%p177 bra 	BB0_269;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1849;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1850, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r505, [%rd203];

BB0_269:
	and.b32  	%r408, %r505, 1;
	shl.b32 	%r409, %r408, 3;
	setp.eq.b32	%p178, %r408, 1;
	selp.f64	%fd1453, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p178;
	mul.wide.u32 	%rd227, %r409, 8;
	add.s64 	%rd229, %rd227, %rd207;
	ld.const.f64 	%fd1454, [%rd229+8];
	mul.rn.f64 	%fd427, %fd1850, %fd1850;
	fma.rn.f64 	%fd1455, %fd1453, %fd427, %fd1454;
	ld.const.f64 	%fd1456, [%rd229+16];
	fma.rn.f64 	%fd1457, %fd1455, %fd427, %fd1456;
	ld.const.f64 	%fd1458, [%rd229+24];
	fma.rn.f64 	%fd1459, %fd1457, %fd427, %fd1458;
	ld.const.f64 	%fd1460, [%rd229+32];
	fma.rn.f64 	%fd1461, %fd1459, %fd427, %fd1460;
	ld.const.f64 	%fd1462, [%rd229+40];
	fma.rn.f64 	%fd1463, %fd1461, %fd427, %fd1462;
	ld.const.f64 	%fd1464, [%rd229+48];
	fma.rn.f64 	%fd428, %fd1463, %fd427, %fd1464;
	fma.rn.f64 	%fd1851, %fd428, %fd1850, %fd1850;
	setp.eq.s32	%p179, %r408, 0;
	@%p179 bra 	BB0_271;

	mov.f64 	%fd1465, 0d3FF0000000000000;
	fma.rn.f64 	%fd1851, %fd428, %fd427, %fd1465;

BB0_271:
	and.b32  	%r410, %r505, 2;
	setp.eq.s32	%p180, %r410, 0;
	@%p180 bra 	BB0_273;

	mov.f64 	%fd1466, 0d0000000000000000;
	mov.f64 	%fd1467, 0dBFF0000000000000;
	fma.rn.f64 	%fd1851, %fd1851, %fd1467, %fd1466;

BB0_273:
	ld.global.f64 	%fd1468, [%rd2+32];
	add.f64 	%fd1469, %fd1468, %fd1468;
	mul.f64 	%fd1470, %fd1468, %fd1469;
	mul.f64 	%fd1471, %fd1848, %fd1851;
	div.rn.f64 	%fd1472, %fd1471, %fd1470;
	add.f64 	%fd434, %fd407, %fd1472;
	ld.global.f64 	%fd1473, [%rd2+40];
	add.f64 	%fd1852, %fd1473, %fd1473;
	abs.f64 	%fd1474, %fd1852;
	setp.neu.f64	%p181, %fd1474, 0d7FF0000000000000;
	@%p181 bra 	BB0_275;

	mov.f64 	%fd1475, 0d0000000000000000;
	mul.rn.f64 	%fd1852, %fd1852, %fd1475;

BB0_275:
	mul.f64 	%fd1476, %fd1852, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r506, %fd1476;
	st.local.u32 	[%rd203], %r506;
	cvt.rn.f64.s32	%fd1477, %r506;
	neg.f64 	%fd1478, %fd1477;
	fma.rn.f64 	%fd1480, %fd1478, %fd1369, %fd1852;
	fma.rn.f64 	%fd1482, %fd1478, %fd1371, %fd1480;
	fma.rn.f64 	%fd1853, %fd1478, %fd1373, %fd1482;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r411}, %fd1852;
	}
	and.b32  	%r412, %r411, 2145386496;
	setp.lt.u32	%p182, %r412, 1105199104;
	@%p182 bra 	BB0_277;

	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1852;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1853, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r506, [%rd203];

BB0_277:
	and.b32  	%r413, %r506, 1;
	shl.b32 	%r414, %r413, 3;
	setp.eq.b32	%p183, %r413, 1;
	selp.f64	%fd1484, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p183;
	mul.wide.u32 	%rd234, %r414, 8;
	add.s64 	%rd236, %rd234, %rd207;
	ld.const.f64 	%fd1485, [%rd236+8];
	mul.rn.f64 	%fd441, %fd1853, %fd1853;
	fma.rn.f64 	%fd1486, %fd1484, %fd441, %fd1485;
	ld.const.f64 	%fd1487, [%rd236+16];
	fma.rn.f64 	%fd1488, %fd1486, %fd441, %fd1487;
	ld.const.f64 	%fd1489, [%rd236+24];
	fma.rn.f64 	%fd1490, %fd1488, %fd441, %fd1489;
	ld.const.f64 	%fd1491, [%rd236+32];
	fma.rn.f64 	%fd1492, %fd1490, %fd441, %fd1491;
	ld.const.f64 	%fd1493, [%rd236+40];
	fma.rn.f64 	%fd1494, %fd1492, %fd441, %fd1493;
	ld.const.f64 	%fd1495, [%rd236+48];
	fma.rn.f64 	%fd442, %fd1494, %fd441, %fd1495;
	fma.rn.f64 	%fd1854, %fd442, %fd1853, %fd1853;
	setp.eq.s32	%p184, %r413, 0;
	@%p184 bra 	BB0_279;

	mov.f64 	%fd1496, 0d3FF0000000000000;
	fma.rn.f64 	%fd1854, %fd442, %fd441, %fd1496;

BB0_279:
	and.b32  	%r415, %r506, 2;
	setp.eq.s32	%p185, %r415, 0;
	@%p185 bra 	BB0_281;

	mov.f64 	%fd1497, 0d0000000000000000;
	mov.f64 	%fd1498, 0dBFF0000000000000;
	fma.rn.f64 	%fd1854, %fd1854, %fd1498, %fd1497;

BB0_281:
	ld.global.f64 	%fd1499, [%rd2+24];
	mul.f64 	%fd1500, %fd1499, 0dC010000000000000;
	mul.f64 	%fd1501, %fd1499, %fd1500;
	div.rn.f64 	%fd448, %fd1854, %fd1501;
	ld.global.f64 	%fd1502, [%rd2+40];
	add.f64 	%fd1855, %fd1502, %fd1502;
	abs.f64 	%fd1503, %fd1855;
	setp.neu.f64	%p186, %fd1503, 0d7FF0000000000000;
	@%p186 bra 	BB0_283;

	mov.f64 	%fd1504, 0d0000000000000000;
	mul.rn.f64 	%fd1855, %fd1855, %fd1504;

BB0_283:
	mul.f64 	%fd1505, %fd1855, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r507, %fd1505;
	st.local.u32 	[%rd203], %r507;
	cvt.rn.f64.s32	%fd1506, %r507;
	neg.f64 	%fd1507, %fd1506;
	fma.rn.f64 	%fd1509, %fd1507, %fd1369, %fd1855;
	fma.rn.f64 	%fd1511, %fd1507, %fd1371, %fd1509;
	fma.rn.f64 	%fd1856, %fd1507, %fd1373, %fd1511;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r416}, %fd1855;
	}
	and.b32  	%r417, %r416, 2145386496;
	setp.lt.u32	%p187, %r417, 1105199104;
	@%p187 bra 	BB0_285;

	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1855;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1856, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r507, [%rd203];

BB0_285:
	and.b32  	%r418, %r507, 1;
	shl.b32 	%r419, %r418, 3;
	setp.eq.b32	%p188, %r418, 1;
	selp.f64	%fd1513, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p188;
	mul.wide.u32 	%rd241, %r419, 8;
	add.s64 	%rd243, %rd241, %rd207;
	ld.const.f64 	%fd1514, [%rd243+8];
	mul.rn.f64 	%fd455, %fd1856, %fd1856;
	fma.rn.f64 	%fd1515, %fd1513, %fd455, %fd1514;
	ld.const.f64 	%fd1516, [%rd243+16];
	fma.rn.f64 	%fd1517, %fd1515, %fd455, %fd1516;
	ld.const.f64 	%fd1518, [%rd243+24];
	fma.rn.f64 	%fd1519, %fd1517, %fd455, %fd1518;
	ld.const.f64 	%fd1520, [%rd243+32];
	fma.rn.f64 	%fd1521, %fd1519, %fd455, %fd1520;
	ld.const.f64 	%fd1522, [%rd243+40];
	fma.rn.f64 	%fd1523, %fd1521, %fd455, %fd1522;
	ld.const.f64 	%fd1524, [%rd243+48];
	fma.rn.f64 	%fd456, %fd1523, %fd455, %fd1524;
	fma.rn.f64 	%fd1857, %fd456, %fd1856, %fd1856;
	setp.eq.s32	%p189, %r418, 0;
	@%p189 bra 	BB0_287;

	mov.f64 	%fd1525, 0d3FF0000000000000;
	fma.rn.f64 	%fd1857, %fd456, %fd455, %fd1525;

BB0_287:
	and.b32  	%r420, %r507, 2;
	setp.eq.s32	%p190, %r420, 0;
	@%p190 bra 	BB0_289;

	mov.f64 	%fd1526, 0d0000000000000000;
	mov.f64 	%fd1527, 0dBFF0000000000000;
	fma.rn.f64 	%fd1857, %fd1857, %fd1527, %fd1526;

BB0_289:
	ld.global.f64 	%fd1528, [%rd2+32];
	mul.f64 	%fd1529, %fd1528, 0d4010000000000000;
	mul.f64 	%fd1530, %fd1528, %fd1529;
	div.rn.f64 	%fd1531, %fd1857, %fd1530;
	add.f64 	%fd462, %fd448, %fd1531;
	ld.global.f64 	%fd1858, [%rd2+40];
	abs.f64 	%fd1532, %fd1858;
	setp.neu.f64	%p191, %fd1532, 0d7FF0000000000000;
	@%p191 bra 	BB0_291;

	mov.f64 	%fd1533, 0d0000000000000000;
	mul.rn.f64 	%fd1858, %fd1858, %fd1533;

BB0_291:
	mul.f64 	%fd1534, %fd1858, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r508, %fd1534;
	st.local.u32 	[%rd203], %r508;
	cvt.rn.f64.s32	%fd1535, %r508;
	neg.f64 	%fd1536, %fd1535;
	fma.rn.f64 	%fd1538, %fd1536, %fd1369, %fd1858;
	fma.rn.f64 	%fd1540, %fd1536, %fd1371, %fd1538;
	fma.rn.f64 	%fd1859, %fd1536, %fd1373, %fd1540;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r421}, %fd1858;
	}
	and.b32  	%r422, %r421, 2145386496;
	setp.lt.u32	%p192, %r422, 1105199104;
	@%p192 bra 	BB0_293;

	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1858;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1859, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r508, [%rd203];

BB0_293:
	and.b32  	%r423, %r508, 1;
	shl.b32 	%r424, %r423, 3;
	setp.eq.b32	%p193, %r423, 1;
	selp.f64	%fd1542, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p193;
	mul.wide.u32 	%rd248, %r424, 8;
	add.s64 	%rd250, %rd248, %rd207;
	ld.const.f64 	%fd1543, [%rd250+8];
	mul.rn.f64 	%fd469, %fd1859, %fd1859;
	fma.rn.f64 	%fd1544, %fd1542, %fd469, %fd1543;
	ld.const.f64 	%fd1545, [%rd250+16];
	fma.rn.f64 	%fd1546, %fd1544, %fd469, %fd1545;
	ld.const.f64 	%fd1547, [%rd250+24];
	fma.rn.f64 	%fd1548, %fd1546, %fd469, %fd1547;
	ld.const.f64 	%fd1549, [%rd250+32];
	fma.rn.f64 	%fd1550, %fd1548, %fd469, %fd1549;
	ld.const.f64 	%fd1551, [%rd250+40];
	fma.rn.f64 	%fd1552, %fd1550, %fd469, %fd1551;
	ld.const.f64 	%fd1553, [%rd250+48];
	fma.rn.f64 	%fd470, %fd1552, %fd469, %fd1553;
	fma.rn.f64 	%fd1860, %fd470, %fd1859, %fd1859;
	setp.eq.s32	%p194, %r423, 0;
	@%p194 bra 	BB0_295;

	mov.f64 	%fd1554, 0d3FF0000000000000;
	fma.rn.f64 	%fd1860, %fd470, %fd469, %fd1554;

BB0_295:
	and.b32  	%r425, %r508, 2;
	setp.eq.s32	%p195, %r425, 0;
	@%p195 bra 	BB0_297;

	mov.f64 	%fd1555, 0d0000000000000000;
	mov.f64 	%fd1556, 0dBFF0000000000000;
	fma.rn.f64 	%fd1860, %fd1860, %fd1556, %fd1555;

BB0_297:
	ld.global.f64 	%fd1861, [%rd2+40];
	abs.f64 	%fd1557, %fd1861;
	setp.neu.f64	%p196, %fd1557, 0d7FF0000000000000;
	@%p196 bra 	BB0_299;

	mov.f64 	%fd1558, 0d0000000000000000;
	mul.rn.f64 	%fd1861, %fd1861, %fd1558;

BB0_299:
	mul.f64 	%fd1559, %fd1861, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r509, %fd1559;
	st.local.u32 	[%rd203], %r509;
	cvt.rn.f64.s32	%fd1560, %r509;
	neg.f64 	%fd1561, %fd1560;
	fma.rn.f64 	%fd1563, %fd1561, %fd1369, %fd1861;
	fma.rn.f64 	%fd1565, %fd1561, %fd1371, %fd1563;
	fma.rn.f64 	%fd1862, %fd1561, %fd1373, %fd1565;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r426}, %fd1861;
	}
	and.b32  	%r427, %r426, 2145386496;
	setp.lt.u32	%p197, %r427, 1105199104;
	@%p197 bra 	BB0_301;

	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1861;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1862, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r509, [%rd203];

BB0_301:
	and.b32  	%r428, %r509, 1;
	shl.b32 	%r429, %r428, 3;
	setp.eq.b32	%p198, %r428, 1;
	selp.f64	%fd1567, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p198;
	mul.wide.u32 	%rd255, %r429, 8;
	add.s64 	%rd257, %rd255, %rd207;
	ld.const.f64 	%fd1568, [%rd257+8];
	mul.rn.f64 	%fd482, %fd1862, %fd1862;
	fma.rn.f64 	%fd1569, %fd1567, %fd482, %fd1568;
	ld.const.f64 	%fd1570, [%rd257+16];
	fma.rn.f64 	%fd1571, %fd1569, %fd482, %fd1570;
	ld.const.f64 	%fd1572, [%rd257+24];
	fma.rn.f64 	%fd1573, %fd1571, %fd482, %fd1572;
	ld.const.f64 	%fd1574, [%rd257+32];
	fma.rn.f64 	%fd1575, %fd1573, %fd482, %fd1574;
	ld.const.f64 	%fd1576, [%rd257+40];
	fma.rn.f64 	%fd1577, %fd1575, %fd482, %fd1576;
	ld.const.f64 	%fd1578, [%rd257+48];
	fma.rn.f64 	%fd483, %fd1577, %fd482, %fd1578;
	fma.rn.f64 	%fd1863, %fd483, %fd1862, %fd1862;
	setp.eq.s32	%p199, %r428, 0;
	@%p199 bra 	BB0_303;

	mov.f64 	%fd1579, 0d3FF0000000000000;
	fma.rn.f64 	%fd1863, %fd483, %fd482, %fd1579;

BB0_303:
	and.b32  	%r430, %r509, 2;
	setp.eq.s32	%p200, %r430, 0;
	@%p200 bra 	BB0_305;

	mov.f64 	%fd1580, 0d0000000000000000;
	mov.f64 	%fd1581, 0dBFF0000000000000;
	fma.rn.f64 	%fd1863, %fd1863, %fd1581, %fd1580;

BB0_305:
	ld.global.f64 	%fd1582, [%rd2+24];
	add.f64 	%fd1583, %fd1582, %fd1582;
	mul.f64 	%fd1584, %fd1582, %fd1583;
	mul.f64 	%fd1585, %fd1860, %fd1863;
	div.rn.f64 	%fd489, %fd1585, %fd1584;
	ld.global.f64 	%fd1864, [%rd2+40];
	abs.f64 	%fd1586, %fd1864;
	setp.neu.f64	%p201, %fd1586, 0d7FF0000000000000;
	@%p201 bra 	BB0_307;

	mov.f64 	%fd1587, 0d0000000000000000;
	mul.rn.f64 	%fd1864, %fd1864, %fd1587;

BB0_307:
	mul.f64 	%fd1588, %fd1864, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r510, %fd1588;
	st.local.u32 	[%rd203], %r510;
	cvt.rn.f64.s32	%fd1589, %r510;
	neg.f64 	%fd1590, %fd1589;
	fma.rn.f64 	%fd1592, %fd1590, %fd1369, %fd1864;
	fma.rn.f64 	%fd1594, %fd1590, %fd1371, %fd1592;
	fma.rn.f64 	%fd1865, %fd1590, %fd1373, %fd1594;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r431}, %fd1864;
	}
	and.b32  	%r432, %r431, 2145386496;
	setp.lt.u32	%p202, %r432, 1105199104;
	@%p202 bra 	BB0_309;

	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1864;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1865, [retval0+0];
	
	//{
	}// Callseq End 28
	ld.local.u32 	%r510, [%rd203];

BB0_309:
	add.s32 	%r136, %r510, 1;
	and.b32  	%r433, %r136, 1;
	shl.b32 	%r434, %r433, 3;
	setp.eq.b32	%p203, %r433, 1;
	selp.f64	%fd1596, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p203;
	mul.wide.u32 	%rd262, %r434, 8;
	add.s64 	%rd264, %rd262, %rd207;
	ld.const.f64 	%fd1597, [%rd264+8];
	mul.rn.f64 	%fd496, %fd1865, %fd1865;
	fma.rn.f64 	%fd1598, %fd1596, %fd496, %fd1597;
	ld.const.f64 	%fd1599, [%rd264+16];
	fma.rn.f64 	%fd1600, %fd1598, %fd496, %fd1599;
	ld.const.f64 	%fd1601, [%rd264+24];
	fma.rn.f64 	%fd1602, %fd1600, %fd496, %fd1601;
	ld.const.f64 	%fd1603, [%rd264+32];
	fma.rn.f64 	%fd1604, %fd1602, %fd496, %fd1603;
	ld.const.f64 	%fd1605, [%rd264+40];
	fma.rn.f64 	%fd1606, %fd1604, %fd496, %fd1605;
	ld.const.f64 	%fd1607, [%rd264+48];
	fma.rn.f64 	%fd497, %fd1606, %fd496, %fd1607;
	fma.rn.f64 	%fd1866, %fd497, %fd1865, %fd1865;
	setp.eq.s32	%p204, %r433, 0;
	@%p204 bra 	BB0_311;

	mov.f64 	%fd1608, 0d3FF0000000000000;
	fma.rn.f64 	%fd1866, %fd497, %fd496, %fd1608;

BB0_311:
	and.b32  	%r435, %r136, 2;
	setp.eq.s32	%p205, %r435, 0;
	@%p205 bra 	BB0_313;

	mov.f64 	%fd1609, 0d0000000000000000;
	mov.f64 	%fd1610, 0dBFF0000000000000;
	fma.rn.f64 	%fd1866, %fd1866, %fd1610, %fd1609;

BB0_313:
	ld.global.f64 	%fd1867, [%rd2+40];
	abs.f64 	%fd1611, %fd1867;
	setp.neu.f64	%p206, %fd1611, 0d7FF0000000000000;
	@%p206 bra 	BB0_315;

	mov.f64 	%fd1612, 0d0000000000000000;
	mul.rn.f64 	%fd1867, %fd1867, %fd1612;

BB0_315:
	mul.f64 	%fd1613, %fd1867, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r511, %fd1613;
	st.local.u32 	[%rd203], %r511;
	cvt.rn.f64.s32	%fd1614, %r511;
	neg.f64 	%fd1615, %fd1614;
	fma.rn.f64 	%fd1617, %fd1615, %fd1369, %fd1867;
	fma.rn.f64 	%fd1619, %fd1615, %fd1371, %fd1617;
	fma.rn.f64 	%fd1868, %fd1615, %fd1373, %fd1619;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r436}, %fd1867;
	}
	and.b32  	%r437, %r436, 2145386496;
	setp.lt.u32	%p207, %r437, 1105199104;
	@%p207 bra 	BB0_317;

	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1867;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1868, [retval0+0];
	
	//{
	}// Callseq End 29
	ld.local.u32 	%r511, [%rd203];

BB0_317:
	add.s32 	%r140, %r511, 1;
	and.b32  	%r438, %r140, 1;
	shl.b32 	%r439, %r438, 3;
	setp.eq.b32	%p208, %r438, 1;
	selp.f64	%fd1621, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p208;
	mul.wide.u32 	%rd269, %r439, 8;
	add.s64 	%rd271, %rd269, %rd207;
	ld.const.f64 	%fd1622, [%rd271+8];
	mul.rn.f64 	%fd509, %fd1868, %fd1868;
	fma.rn.f64 	%fd1623, %fd1621, %fd509, %fd1622;
	ld.const.f64 	%fd1624, [%rd271+16];
	fma.rn.f64 	%fd1625, %fd1623, %fd509, %fd1624;
	ld.const.f64 	%fd1626, [%rd271+24];
	fma.rn.f64 	%fd1627, %fd1625, %fd509, %fd1626;
	ld.const.f64 	%fd1628, [%rd271+32];
	fma.rn.f64 	%fd1629, %fd1627, %fd509, %fd1628;
	ld.const.f64 	%fd1630, [%rd271+40];
	fma.rn.f64 	%fd1631, %fd1629, %fd509, %fd1630;
	ld.const.f64 	%fd1632, [%rd271+48];
	fma.rn.f64 	%fd510, %fd1631, %fd509, %fd1632;
	fma.rn.f64 	%fd1869, %fd510, %fd1868, %fd1868;
	setp.eq.s32	%p209, %r438, 0;
	@%p209 bra 	BB0_319;

	mov.f64 	%fd1633, 0d3FF0000000000000;
	fma.rn.f64 	%fd1869, %fd510, %fd509, %fd1633;

BB0_319:
	and.b32  	%r440, %r140, 2;
	setp.eq.s32	%p210, %r440, 0;
	@%p210 bra 	BB0_321;

	mov.f64 	%fd1634, 0d0000000000000000;
	mov.f64 	%fd1635, 0dBFF0000000000000;
	fma.rn.f64 	%fd1869, %fd1869, %fd1635, %fd1634;

BB0_321:
	ld.global.f64 	%fd1638, [%rd2+32];
	add.f64 	%fd1639, %fd1638, %fd1638;
	mul.f64 	%fd1640, %fd1638, %fd1639;
	mul.f64 	%fd1641, %fd1866, %fd1869;
	div.rn.f64 	%fd1642, %fd1641, %fd1640;
	add.f64 	%fd516, %fd489, %fd1642;
	mov.f64 	%fd1872, 0d0000000000000000;
	mov.f64 	%fd1871, %fd1872;
	@%p2 bra 	BB0_327;

	ld.global.f64 	%fd517, [%rd2];
	ld.global.f64 	%fd518, [%rd2+8];
	add.f64 	%fd519, %fd462, %fd462;
	ld.global.f64 	%fd520, [%rd2+16];
	ld.global.f64 	%fd521, [%rd2+48];
	mul.lo.s32 	%r448, %r1, %r2;
	mul.lo.s32 	%r449, %r448, %r2;
	mul.wide.s32 	%rd276, %r449, 4;
	add.s64 	%rd280, %rd1, %rd276;
	mov.f64 	%fd1872, 0d0000000000000000;
	mov.u32 	%r512, 0;
	mov.f64 	%fd1871, %fd1872;

BB0_323:
	rem.s32 	%r450, %r512, %r2;
	cvt.rn.f64.s32	%fd1645, %r450;
	sub.f64 	%fd1646, %fd1645, %fd518;
	mul.f64 	%fd1647, %fd434, %fd1646;
	mul.f64 	%fd1648, %fd1646, %fd1647;
	mul.f64 	%fd1649, %fd519, %fd1646;
	div.s32 	%r451, %r512, %r2;
	cvt.rn.f64.s32	%fd1650, %r451;
	sub.f64 	%fd1651, %fd1650, %fd520;
	mul.f64 	%fd1652, %fd1649, %fd1651;
	sub.f64 	%fd1653, %fd1648, %fd1652;
	mul.f64 	%fd1654, %fd516, %fd1651;
	fma.rn.f64 	%fd524, %fd1651, %fd1654, %fd1653;
	neg.f64 	%fd1655, %fd524;
	mov.f64 	%fd1656, 0d4338000000000000;
	mov.f64 	%fd1657, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1658, %fd1655, %fd1657, %fd1656;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r144, %temp}, %fd1658;
	}
	mov.f64 	%fd1659, 0dC338000000000000;
	add.rn.f64 	%fd1660, %fd1658, %fd1659;
	mov.f64 	%fd1661, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1662, %fd1660, %fd1661, %fd1655;
	mov.f64 	%fd1663, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1664, %fd1660, %fd1663, %fd1662;
	mov.f64 	%fd1665, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1666, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1667, %fd1666, %fd1664, %fd1665;
	mov.f64 	%fd1668, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1669, %fd1667, %fd1664, %fd1668;
	mov.f64 	%fd1670, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1671, %fd1669, %fd1664, %fd1670;
	mov.f64 	%fd1672, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1673, %fd1671, %fd1664, %fd1672;
	mov.f64 	%fd1674, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1675, %fd1673, %fd1664, %fd1674;
	mov.f64 	%fd1676, 0d3F81111111122322;
	fma.rn.f64 	%fd1677, %fd1675, %fd1664, %fd1676;
	mov.f64 	%fd1678, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1679, %fd1677, %fd1664, %fd1678;
	mov.f64 	%fd1680, 0d3FC5555555555511;
	fma.rn.f64 	%fd1681, %fd1679, %fd1664, %fd1680;
	mov.f64 	%fd1682, 0d3FE000000000000B;
	fma.rn.f64 	%fd1683, %fd1681, %fd1664, %fd1682;
	mov.f64 	%fd1684, 0d3FF0000000000000;
	fma.rn.f64 	%fd1685, %fd1683, %fd1664, %fd1684;
	fma.rn.f64 	%fd1686, %fd1685, %fd1664, %fd1684;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r145, %temp}, %fd1686;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r146}, %fd1686;
	}
	shl.b32 	%r452, %r144, 20;
	add.s32 	%r453, %r146, %r452;
	mov.b64 	%fd1870, {%r145, %r453};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r454}, %fd1655;
	}
	mov.b32 	 %f10, %r454;
	abs.f32 	%f5, %f10;
	setp.lt.f32	%p212, %f5, 0f4086232B;
	@%p212 bra 	BB0_326;

	setp.gt.f64	%p213, %fd524, 0d8000000000000000;
	mov.f64 	%fd1687, 0d7FF0000000000000;
	sub.f64 	%fd1688, %fd1687, %fd524;
	selp.f64	%fd1870, 0d0000000000000000, %fd1688, %p213;
	setp.geu.f32	%p214, %f5, 0f40874800;
	@%p214 bra 	BB0_326;

	shr.u32 	%r455, %r144, 31;
	add.s32 	%r456, %r144, %r455;
	shr.s32 	%r457, %r456, 1;
	shl.b32 	%r458, %r457, 20;
	add.s32 	%r459, %r458, %r146;
	mov.b64 	%fd1689, {%r145, %r459};
	sub.s32 	%r460, %r144, %r457;
	shl.b32 	%r461, %r460, 20;
	add.s32 	%r462, %r461, 1072693248;
	mov.u32 	%r463, 0;
	mov.b64 	%fd1690, {%r463, %r462};
	mul.f64 	%fd1870, %fd1689, %fd1690;

BB0_326:
	fma.rn.f64 	%fd1691, %fd517, %fd1870, %fd521;
	add.f64 	%fd1871, %fd1871, %fd1691;
	ld.global.u32 	%r464, [%rd280];
	cvt.rn.f64.s32	%fd1692, %r464;
	sub.f64 	%fd1693, %fd1691, %fd1692;
	fma.rn.f64 	%fd1872, %fd1693, %fd1693, %fd1872;
	add.s64 	%rd280, %rd280, 4;
	add.s32 	%r512, %r512, 1;
	setp.lt.s32	%p215, %r512, %r3;
	@%p215 bra 	BB0_323;

BB0_327:
	st.global.f64 	[%rd2], %fd1871;
	div.rn.f64 	%fd1694, %fd1872, %fd1709;
	mov.f64 	%fd1695, 0d3FF0000000000000;
	sub.f64 	%fd1696, %fd1695, %fd1694;
	st.global.f64 	[%rd2+48], %fd1696;

BB0_328:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot1;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB1_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB1_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd92, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	neg.s32 	%r25, %r24;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd90, %rd45, 120;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r7;

BB1_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r39;
	mov.u64 	%rd7, %rd91;
	ld.const.u64 	%rd48, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd48;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd46, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd92], %rd46;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r26, %r9, %r7;
	mul.wide.s32 	%rd51, %r26, 8;
	add.s64 	%rd92, %rd1, %rd51;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd93, %rd13;
	add.s64 	%rd90, %rd90, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd91, %rd13;
	mov.u32 	%r39, %r9;
	@%p3 bra 	BB1_3;

BB1_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB1_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r10;
	shl.b64 	%rd52, %rd96, %r10;
	shr.u64 	%rd53, %rd95, %r28;
	or.b64  	%rd96, %rd52, %rd53;
	shl.b64 	%rd54, %rd95, %r10;
	ld.local.u64 	%rd55, [%rd1+8];
	shr.u64 	%rd56, %rd55, %r28;
	or.b64  	%rd95, %rd56, %rd54;

BB1_6:
	cvta.to.local.u64 	%rd57, %rd37;
	shr.u64 	%rd58, %rd96, 62;
	cvt.u32.u64	%r29, %rd58;
	shr.u64 	%rd59, %rd95, 62;
	shl.b64 	%rd60, %rd96, 2;
	or.b64  	%rd98, %rd60, %rd59;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd61, %rd96, 61;
	cvt.u32.u64	%r30, %rd61;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.ne.s32	%p5, %r40, 0;
	selp.b32	%r34, %r33, %r32, %p5;
	st.local.u32 	[%rd57], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB1_8;

	mov.u64 	%rd65, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd65;
	mov.b64         {a2,a3}, %rd65;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB1_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB1_10;

	shl.b64 	%rd68, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd69, %rd97, %r36;
	or.b64  	%rd98, %rd69, %rd68;

BB1_10:
	mov.u64 	%rd73, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd73;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd70, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB1_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd70;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd70;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd74, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB1_12:
	cvt.u64.u32	%rd80, %r40;
	shl.b64 	%rd81, %rd80, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd82, %r38;
	shl.b64 	%rd83, %rd82, 52;
	add.s64 	%rd84, %rd99, 1;
	shr.u64 	%rd85, %rd84, 10;
	add.s64 	%rd86, %rd85, 1;
	shr.u64 	%rd87, %rd86, 1;
	add.s64 	%rd88, %rd87, %rd83;
	or.b64  	%rd89, %rd88, %rd81;
	mov.b64 	 %fd4, %rd89;

BB1_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


