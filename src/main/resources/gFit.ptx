//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-20732876
// Cuda compilation tools, release 8.0, V8.0.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	gaussFitter
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry gaussFitter(
	.param .u32 gaussFitter_param_0,
	.param .u64 gaussFitter_param_1,
	.param .u32 gaussFitter_param_2,
	.param .u64 gaussFitter_param_3,
	.param .u32 gaussFitter_param_4,
	.param .u16 gaussFitter_param_5,
	.param .u64 gaussFitter_param_6,
	.param .u32 gaussFitter_param_7,
	.param .u64 gaussFitter_param_8,
	.param .u32 gaussFitter_param_9,
	.param .f64 gaussFitter_param_10,
	.param .u32 gaussFitter_param_11
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<217>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<472>;
	.reg .f64 	%fd<1875>;
	.reg .b64 	%rd<304>;


	mov.u64 	%rd303, __local_depot0;
	cvta.local.u64 	%SP, %rd303;
	ld.param.u32 	%r146, [gaussFitter_param_0];
	ld.param.u64 	%rd10, [gaussFitter_param_1];
	ld.param.u16 	%rs5, [gaussFitter_param_5];
	cvta.to.global.u64 	%rd1, %rd10;
	mov.u32 	%r148, %ntid.x;
	mov.u32 	%r149, %ctaid.x;
	mov.u32 	%r150, %tid.x;
	mad.lo.s32 	%r429, %r148, %r149, %r150;
	setp.ge.s32	%p1, %r429, %r146;
	@%p1 bra 	BB0_329;

	cvt.u32.u16	%r1, %rs5;
	mul.wide.u16 	%r2, %rs5, %rs5;
	cvt.rn.f64.s32	%fd1, %r2;

BB0_2:
	mul.lo.s32 	%r5, %r429, %r2;
	setp.eq.s32	%p2, %r2, 0;
	mov.f64 	%fd1709, 0d0000000000000000;
	mov.f64 	%fd1706, %fd1709;
	mov.f64 	%fd1703, %fd1709;
	mov.u32 	%r430, 0;
	mov.f64 	%fd1708, %fd1709;
	mov.f64 	%fd1705, %fd1709;
	mov.f64 	%fd1702, %fd1709;
	@%p2 bra 	BB0_4;

BB0_3:
	add.s32 	%r156, %r430, %r5;
	mul.wide.s32 	%rd11, %r156, 4;
	add.s64 	%rd12, %rd1, %rd11;
	ld.global.u32 	%r157, [%rd12];
	cvt.rn.f64.s32	%fd546, %r157;
	add.f64 	%fd1703, %fd1703, %fd546;
	rem.s32 	%r158, %r430, %r1;
	mul.lo.s32 	%r159, %r158, %r157;
	cvt.rn.f64.s32	%fd547, %r159;
	add.f64 	%fd1709, %fd1709, %fd547;
	div.s32 	%r160, %r430, %r1;
	mul.lo.s32 	%r161, %r157, %r160;
	cvt.rn.f64.s32	%fd548, %r161;
	add.f64 	%fd1706, %fd1706, %fd548;
	add.s32 	%r430, %r430, 1;
	setp.lt.s32	%p3, %r430, %r2;
	mov.f64 	%fd1702, %fd1703;
	mov.f64 	%fd1705, %fd1706;
	mov.f64 	%fd1708, %fd1709;
	@%p3 bra 	BB0_3;

BB0_4:
	ld.param.u64 	%rd300, [gaussFitter_param_3];
	mul.lo.s32 	%r163, %r429, 7;
	cvta.to.global.u64 	%rd13, %rd300;
	mul.wide.s32 	%rd14, %r163, 8;
	add.s64 	%rd2, %rd13, %rd14;
	div.rn.f64 	%fd551, %fd1708, %fd1702;
	st.global.f64 	[%rd2+8], %fd551;
	div.rn.f64 	%fd552, %fd1705, %fd1702;
	st.global.f64 	[%rd2+16], %fd552;
	div.rn.f64 	%fd11, %fd1702, %fd1;
	mov.f64 	%fd1712, 0d0000000000000000;
	mov.u32 	%r431, 0;
	mov.f64 	%fd1711, %fd1712;
	@%p2 bra 	BB0_6;

BB0_5:
	add.s32 	%r164, %r431, %r5;
	mul.wide.s32 	%rd15, %r164, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u32 	%r165, [%rd16];
	cvt.rn.f64.s32	%fd553, %r165;
	sub.f64 	%fd554, %fd553, %fd11;
	fma.rn.f64 	%fd1712, %fd554, %fd554, %fd1712;
	add.s32 	%r431, %r431, 1;
	setp.lt.s32	%p5, %r431, %r2;
	mov.f64 	%fd1711, %fd1712;
	@%p5 bra 	BB0_5;

BB0_6:
	ld.param.u64 	%rd302, [gaussFitter_param_8];
	ld.param.u64 	%rd301, [gaussFitter_param_6];
	cvta.to.global.u64 	%rd17, %rd301;
	ld.global.f64 	%fd555, [%rd17];
	ld.global.f64 	%fd556, [%rd2];
	mul.f64 	%fd15, %fd556, %fd555;
	ld.global.f64 	%fd557, [%rd17+8];
	mul.f64 	%fd16, %fd556, %fd557;
	ld.global.f64 	%fd558, [%rd17+96];
	mul.f64 	%fd17, %fd556, %fd558;
	ld.global.f64 	%fd559, [%rd17+104];
	mul.f64 	%fd18, %fd556, %fd559;
	cvta.to.global.u64 	%rd18, %rd302;
	add.s64 	%rd3, %rd18, %rd14;
	ld.global.f64 	%fd560, [%rd3];
	mul.f64 	%fd561, %fd556, %fd560;
	ld.global.f64 	%fd562, [%rd3+48];
	st.global.f64 	[%rd3], %fd561;
	ld.global.f64 	%fd563, [%rd2];
	mul.f64 	%fd564, %fd563, %fd562;
	st.global.f64 	[%rd3+48], %fd564;
	ld.global.f64 	%fd565, [%rd17+56];
	ld.global.f64 	%fd1713, [%rd17+48];
	setp.gtu.f64	%p6, %fd1713, %fd565;
	@%p6 bra 	BB0_19;

	ld.global.f64 	%fd1714, [%rd17+72];
	mov.f64 	%fd1725, 0d3FF0000000000000;

BB0_8:
	mov.f64 	%fd1719, %fd1725;
	mov.f64 	%fd1722, %fd1719;
	add.f64 	%fd567, %fd1713, %fd1713;
	mul.f64 	%fd568, %fd1713, %fd567;
	rcp.rn.f64 	%fd24, %fd568;
	ld.global.f64 	%fd1715, [%rd17+64];
	setp.gtu.f64	%p7, %fd1715, %fd1714;
	mov.f64 	%fd1724, %fd1722;
	@%p7 bra 	BB0_18;

BB0_9:
	mov.f64 	%fd1716, %fd1722;
	mov.f64 	%fd28, %fd1716;
	mov.f64 	%fd1727, 0d0000000000000000;
	@%p2 bra 	BB0_15;

	add.f64 	%fd571, %fd1715, %fd1715;
	mul.f64 	%fd572, %fd1715, %fd571;
	rcp.rn.f64 	%fd29, %fd572;
	ld.global.f64 	%fd30, [%rd2];
	ld.global.f64 	%fd31, [%rd2+8];
	ld.global.f64 	%fd32, [%rd2+16];
	mov.f64 	%fd1727, 0d0000000000000000;
	mov.u32 	%r432, 0;

BB0_11:
	rem.s32 	%r167, %r432, %r1;
	cvt.rn.f64.s32	%fd573, %r167;
	sub.f64 	%fd574, %fd573, %fd31;
	mul.f64 	%fd575, %fd24, %fd574;
	div.s32 	%r168, %r432, %r1;
	cvt.rn.f64.s32	%fd576, %r168;
	sub.f64 	%fd577, %fd576, %fd32;
	mul.f64 	%fd578, %fd29, %fd577;
	mul.f64 	%fd579, %fd577, %fd578;
	fma.rn.f64 	%fd34, %fd574, %fd575, %fd579;
	neg.f64 	%fd580, %fd34;
	mov.f64 	%fd581, 0d4338000000000000;
	mov.f64 	%fd582, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd583, %fd580, %fd582, %fd581;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd583;
	}
	mov.f64 	%fd584, 0dC338000000000000;
	add.rn.f64 	%fd585, %fd583, %fd584;
	mov.f64 	%fd586, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd587, %fd585, %fd586, %fd580;
	mov.f64 	%fd588, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd589, %fd585, %fd588, %fd587;
	mov.f64 	%fd590, 0d3E928AF3FCA213EA;
	mov.f64 	%fd591, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd592, %fd591, %fd589, %fd590;
	mov.f64 	%fd593, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd594, %fd592, %fd589, %fd593;
	mov.f64 	%fd595, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd596, %fd594, %fd589, %fd595;
	mov.f64 	%fd597, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd598, %fd596, %fd589, %fd597;
	mov.f64 	%fd599, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd600, %fd598, %fd589, %fd599;
	mov.f64 	%fd601, 0d3F81111111122322;
	fma.rn.f64 	%fd602, %fd600, %fd589, %fd601;
	mov.f64 	%fd603, 0d3FA55555555502A1;
	fma.rn.f64 	%fd604, %fd602, %fd589, %fd603;
	mov.f64 	%fd605, 0d3FC5555555555511;
	fma.rn.f64 	%fd606, %fd604, %fd589, %fd605;
	mov.f64 	%fd607, 0d3FE000000000000B;
	fma.rn.f64 	%fd608, %fd606, %fd589, %fd607;
	mov.f64 	%fd609, 0d3FF0000000000000;
	fma.rn.f64 	%fd610, %fd608, %fd589, %fd609;
	fma.rn.f64 	%fd611, %fd610, %fd589, %fd609;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd611;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r14}, %fd611;
	}
	shl.b32 	%r169, %r12, 20;
	add.s32 	%r170, %r14, %r169;
	mov.b64 	%fd1726, {%r13, %r170};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r171}, %fd580;
	}
	mov.b32 	 %f6, %r171;
	abs.f32 	%f1, %f6;
	setp.lt.f32	%p9, %f1, 0f4086232B;
	@%p9 bra 	BB0_14;

	setp.gt.f64	%p10, %fd34, 0d8000000000000000;
	mov.f64 	%fd612, 0d7FF0000000000000;
	sub.f64 	%fd613, %fd612, %fd34;
	selp.f64	%fd1726, 0d0000000000000000, %fd613, %p10;
	setp.geu.f32	%p11, %f1, 0f40874800;
	@%p11 bra 	BB0_14;

	shr.u32 	%r172, %r12, 31;
	add.s32 	%r173, %r12, %r172;
	shr.s32 	%r174, %r173, 1;
	shl.b32 	%r175, %r174, 20;
	add.s32 	%r176, %r175, %r14;
	mov.b64 	%fd614, {%r13, %r176};
	sub.s32 	%r177, %r12, %r174;
	shl.b32 	%r178, %r177, 20;
	add.s32 	%r179, %r178, 1072693248;
	mov.u32 	%r180, 0;
	mov.b64 	%fd615, {%r180, %r179};
	mul.f64 	%fd1726, %fd614, %fd615;

BB0_14:
	add.s32 	%r181, %r432, %r5;
	mul.wide.s32 	%rd23, %r181, 4;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u32 	%r182, [%rd24];
	cvt.rn.f64.s32	%fd616, %r182;
	mul.f64 	%fd617, %fd30, %fd1726;
	sub.f64 	%fd618, %fd617, %fd616;
	fma.rn.f64 	%fd1727, %fd618, %fd618, %fd1727;
	add.s32 	%r432, %r432, 1;
	setp.lt.s32	%p12, %r432, %r2;
	@%p12 bra 	BB0_11;

BB0_15:
	div.rn.f64 	%fd41, %fd1727, %fd1711;
	setp.geu.f64	%p13, %fd41, %fd28;
	mov.f64 	%fd1723, %fd28;
	@%p13 bra 	BB0_17;

	st.global.f64 	[%rd2+24], %fd1713;
	st.global.f64 	[%rd2+32], %fd1715;
	ld.global.f64 	%fd1714, [%rd17+72];
	mov.f64 	%fd1723, %fd41;

BB0_17:
	mov.f64 	%fd1722, %fd1723;
	ld.global.f64 	%fd619, [%rd3+32];
	add.f64 	%fd1715, %fd1715, %fd619;
	setp.le.f64	%p14, %fd1715, %fd1714;
	mov.f64 	%fd1724, %fd1722;
	@%p14 bra 	BB0_9;

BB0_18:
	mov.f64 	%fd1725, %fd1724;
	ld.global.f64 	%fd620, [%rd3+24];
	add.f64 	%fd1713, %fd1713, %fd620;
	ld.global.f64 	%fd621, [%rd17+56];
	setp.le.f64	%p15, %fd1713, %fd621;
	@%p15 bra 	BB0_8;

BB0_19:
	ld.global.f64 	%fd625, [%rd2+24];
	add.f64 	%fd626, %fd625, %fd625;
	mul.f64 	%fd627, %fd625, %fd626;
	rcp.rn.f64 	%fd1730, %fd627;
	ld.global.f64 	%fd628, [%rd2+32];
	add.f64 	%fd629, %fd628, %fd628;
	mul.f64 	%fd630, %fd628, %fd629;
	rcp.rn.f64 	%fd1728, %fd630;
	mov.u16 	%rs8, 1;
	mov.u32 	%r437, 0;
	mov.f64 	%fd1774, 0d3FF0000000000000;
	mov.f64 	%fd1772, %fd1774;
	mov.u32 	%r435, %r437;
	mov.f64 	%fd1729, 0d0000000000000000;
	bra.uni 	BB0_20;

BB0_215:
	neg.f64 	%fd1305, %fd360;
	st.global.f64 	[%rd3+48], %fd1305;
	mov.f64 	%fd1742, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1742;
	bra.uni 	BB0_238;

BB0_205:
	mul.f64 	%fd1250, %fd345, 0dBFE6666666666666;
	st.global.f64 	[%rd3+48], %fd1250;
	mov.f64 	%fd1739, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1739;
	bra.uni 	BB0_238;

BB0_217:
	mul.f64 	%fd1306, %fd360, 0dBFE6666666666666;
	st.global.f64 	[%rd3+48], %fd1306;
	mov.f64 	%fd1743, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1743;
	bra.uni 	BB0_238;

BB0_197:
	neg.f64 	%fd1245, %fd344;
	st.global.f64 	[%rd122], %fd1245;
	mov.f64 	%fd1735, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1735;
	bra.uni 	BB0_238;

BB0_199:
	mul.f64 	%fd1246, %fd344, 0dBFE6666666666666;
	st.global.f64 	[%rd122], %fd1246;
	mov.f64 	%fd1736, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1736;
	bra.uni 	BB0_238;

BB0_20:
	mov.u32 	%r17, %r437;
	mov.f64 	%fd55, %fd1774;
	mov.f64 	%fd54, %fd1772;
	setp.eq.s32	%p16, %r435, 0;
	@%p16 bra 	BB0_219;
	bra.uni 	BB0_21;

BB0_219:
	ld.global.f64 	%fd361, [%rd3];
	ld.global.f64 	%fd1308, [%rd2];
	add.f64 	%fd362, %fd1308, %fd361;
	setp.gt.f64	%p143, %fd362, %fd15;
	setp.lt.f64	%p144, %fd362, %fd16;
	and.pred  	%p145, %p143, %p144;
	@%p145 bra 	BB0_225;
	bra.uni 	BB0_220;

BB0_225:
	st.global.f64 	[%rd2], %fd362;
	mov.f64 	%fd1841, 0d0000000000000000;
	@%p2 bra 	BB0_231;

	ld.global.f64 	%fd363, [%rd2+8];
	ld.global.f64 	%fd365, [%rd2+16];
	ld.global.f64 	%fd366, [%rd2+48];
	mov.f64 	%fd1841, 0d0000000000000000;
	mov.u32 	%r460, 0;

BB0_227:
	add.f64 	%fd1700, %fd1729, %fd1729;
	rem.s32 	%r335, %r460, %r1;
	cvt.rn.f64.s32	%fd1314, %r335;
	sub.f64 	%fd1315, %fd1314, %fd363;
	mul.f64 	%fd1316, %fd1730, %fd1315;
	mul.f64 	%fd1317, %fd1315, %fd1316;
	mul.f64 	%fd1318, %fd1700, %fd1315;
	div.s32 	%r336, %r460, %r1;
	cvt.rn.f64.s32	%fd1319, %r336;
	sub.f64 	%fd1320, %fd1319, %fd365;
	mul.f64 	%fd1321, %fd1318, %fd1320;
	sub.f64 	%fd1322, %fd1317, %fd1321;
	mul.f64 	%fd1323, %fd1728, %fd1320;
	fma.rn.f64 	%fd368, %fd1320, %fd1323, %fd1322;
	neg.f64 	%fd1324, %fd368;
	mov.f64 	%fd1325, 0d4338000000000000;
	mov.f64 	%fd1326, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1327, %fd1324, %fd1326, %fd1325;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r98, %temp}, %fd1327;
	}
	mov.f64 	%fd1328, 0dC338000000000000;
	add.rn.f64 	%fd1329, %fd1327, %fd1328;
	mov.f64 	%fd1330, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1331, %fd1329, %fd1330, %fd1324;
	mov.f64 	%fd1332, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1333, %fd1329, %fd1332, %fd1331;
	mov.f64 	%fd1334, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1335, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1336, %fd1335, %fd1333, %fd1334;
	mov.f64 	%fd1337, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1338, %fd1336, %fd1333, %fd1337;
	mov.f64 	%fd1339, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1340, %fd1338, %fd1333, %fd1339;
	mov.f64 	%fd1341, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1342, %fd1340, %fd1333, %fd1341;
	mov.f64 	%fd1343, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1344, %fd1342, %fd1333, %fd1343;
	mov.f64 	%fd1345, 0d3F81111111122322;
	fma.rn.f64 	%fd1346, %fd1344, %fd1333, %fd1345;
	mov.f64 	%fd1347, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1348, %fd1346, %fd1333, %fd1347;
	mov.f64 	%fd1349, 0d3FC5555555555511;
	fma.rn.f64 	%fd1350, %fd1348, %fd1333, %fd1349;
	mov.f64 	%fd1351, 0d3FE000000000000B;
	fma.rn.f64 	%fd1352, %fd1350, %fd1333, %fd1351;
	mov.f64 	%fd1353, 0d3FF0000000000000;
	fma.rn.f64 	%fd1354, %fd1352, %fd1333, %fd1353;
	fma.rn.f64 	%fd1355, %fd1354, %fd1333, %fd1353;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r99, %temp}, %fd1355;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r100}, %fd1355;
	}
	shl.b32 	%r337, %r98, 20;
	add.s32 	%r338, %r100, %r337;
	mov.b64 	%fd1840, {%r99, %r338};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r339}, %fd1324;
	}
	mov.b32 	 %f9, %r339;
	abs.f32 	%f4, %f9;
	setp.lt.f32	%p149, %f4, 0f4086232B;
	@%p149 bra 	BB0_230;

	setp.gt.f64	%p150, %fd368, 0d8000000000000000;
	mov.f64 	%fd1356, 0d7FF0000000000000;
	sub.f64 	%fd1357, %fd1356, %fd368;
	selp.f64	%fd1840, 0d0000000000000000, %fd1357, %p150;
	setp.geu.f32	%p151, %f4, 0f40874800;
	@%p151 bra 	BB0_230;

	shr.u32 	%r340, %r98, 31;
	add.s32 	%r341, %r98, %r340;
	shr.s32 	%r342, %r341, 1;
	shl.b32 	%r343, %r342, 20;
	add.s32 	%r344, %r343, %r100;
	mov.b64 	%fd1358, {%r99, %r344};
	sub.s32 	%r345, %r98, %r342;
	shl.b32 	%r346, %r345, 20;
	add.s32 	%r347, %r346, 1072693248;
	mov.u32 	%r348, 0;
	mov.b64 	%fd1359, {%r348, %r347};
	mul.f64 	%fd1840, %fd1358, %fd1359;

BB0_230:
	fma.rn.f64 	%fd1360, %fd362, %fd1840, %fd366;
	add.s32 	%r349, %r460, %r5;
	mul.wide.s32 	%rd218, %r349, 4;
	add.s64 	%rd219, %rd1, %rd218;
	ld.global.u32 	%r350, [%rd219];
	cvt.rn.f64.s32	%fd1361, %r350;
	sub.f64 	%fd1362, %fd1360, %fd1361;
	fma.rn.f64 	%fd1841, %fd1362, %fd1362, %fd1841;
	add.s32 	%r460, %r460, 1;
	setp.lt.s32	%p152, %r460, %r2;
	@%p152 bra 	BB0_227;

BB0_231:
	div.rn.f64 	%fd1775, %fd1841, %fd1711;
	setp.lt.f64	%p153, %fd1775, %fd55;
	mov.f64 	%fd1773, %fd55;
	@%p153 bra 	BB0_238;

	ld.global.f64 	%fd1363, [%rd3];
	sub.f64 	%fd1364, %fd362, %fd1363;
	st.global.f64 	[%rd2], %fd1364;
	ld.global.f64 	%fd376, [%rd3];
	setp.lt.f64	%p154, %fd376, 0d0000000000000000;
	@%p154 bra 	BB0_234;
	bra.uni 	BB0_233;

BB0_234:
	setp.lt.s32	%p155, %r17, 20;
	@%p155 bra 	BB0_236;
	bra.uni 	BB0_235;

BB0_236:
	mul.f64 	%fd1367, %fd376, 0dBFD3333333333333;
	st.global.f64 	[%rd3], %fd1367;
	bra.uni 	BB0_237;

BB0_21:
	setp.eq.s32	%p17, %r435, 6;
	@%p17 bra 	BB0_201;
	bra.uni 	BB0_22;

BB0_201:
	ld.global.f64 	%fd345, [%rd3+48];
	ld.global.f64 	%fd1248, [%rd2+48];
	add.f64 	%fd346, %fd1248, %fd345;
	setp.gt.f64	%p130, %fd346, %fd17;
	setp.lt.f64	%p131, %fd346, %fd18;
	and.pred  	%p132, %p130, %p131;
	@%p132 bra 	BB0_207;
	bra.uni 	BB0_202;

BB0_207:
	st.global.f64 	[%rd2+48], %fd346;
	mov.f64 	%fd1839, 0d0000000000000000;
	@%p2 bra 	BB0_213;

	ld.global.f64 	%fd347, [%rd2];
	ld.global.f64 	%fd348, [%rd2+8];
	ld.global.f64 	%fd350, [%rd2+16];
	mov.f64 	%fd1839, 0d0000000000000000;
	mov.u32 	%r459, 0;

BB0_209:
	add.f64 	%fd1699, %fd1729, %fd1729;
	rem.s32 	%r316, %r459, %r1;
	cvt.rn.f64.s32	%fd1254, %r316;
	sub.f64 	%fd1255, %fd1254, %fd348;
	mul.f64 	%fd1256, %fd1730, %fd1255;
	mul.f64 	%fd1257, %fd1255, %fd1256;
	mul.f64 	%fd1258, %fd1699, %fd1255;
	div.s32 	%r317, %r459, %r1;
	cvt.rn.f64.s32	%fd1259, %r317;
	sub.f64 	%fd1260, %fd1259, %fd350;
	mul.f64 	%fd1261, %fd1258, %fd1260;
	sub.f64 	%fd1262, %fd1257, %fd1261;
	mul.f64 	%fd1263, %fd1728, %fd1260;
	fma.rn.f64 	%fd352, %fd1260, %fd1263, %fd1262;
	neg.f64 	%fd1264, %fd352;
	mov.f64 	%fd1265, 0d4338000000000000;
	mov.f64 	%fd1266, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1267, %fd1264, %fd1266, %fd1265;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r93, %temp}, %fd1267;
	}
	mov.f64 	%fd1268, 0dC338000000000000;
	add.rn.f64 	%fd1269, %fd1267, %fd1268;
	mov.f64 	%fd1270, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1271, %fd1269, %fd1270, %fd1264;
	mov.f64 	%fd1272, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1273, %fd1269, %fd1272, %fd1271;
	mov.f64 	%fd1274, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1275, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1276, %fd1275, %fd1273, %fd1274;
	mov.f64 	%fd1277, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1278, %fd1276, %fd1273, %fd1277;
	mov.f64 	%fd1279, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1280, %fd1278, %fd1273, %fd1279;
	mov.f64 	%fd1281, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1282, %fd1280, %fd1273, %fd1281;
	mov.f64 	%fd1283, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1284, %fd1282, %fd1273, %fd1283;
	mov.f64 	%fd1285, 0d3F81111111122322;
	fma.rn.f64 	%fd1286, %fd1284, %fd1273, %fd1285;
	mov.f64 	%fd1287, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1288, %fd1286, %fd1273, %fd1287;
	mov.f64 	%fd1289, 0d3FC5555555555511;
	fma.rn.f64 	%fd1290, %fd1288, %fd1273, %fd1289;
	mov.f64 	%fd1291, 0d3FE000000000000B;
	fma.rn.f64 	%fd1292, %fd1290, %fd1273, %fd1291;
	mov.f64 	%fd1293, 0d3FF0000000000000;
	fma.rn.f64 	%fd1294, %fd1292, %fd1273, %fd1293;
	fma.rn.f64 	%fd1295, %fd1294, %fd1273, %fd1293;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r94, %temp}, %fd1295;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r95}, %fd1295;
	}
	shl.b32 	%r318, %r93, 20;
	add.s32 	%r319, %r95, %r318;
	mov.b64 	%fd1838, {%r94, %r319};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r320}, %fd1264;
	}
	mov.b32 	 %f8, %r320;
	abs.f32 	%f3, %f8;
	setp.lt.f32	%p136, %f3, 0f4086232B;
	@%p136 bra 	BB0_212;

	setp.gt.f64	%p137, %fd352, 0d8000000000000000;
	mov.f64 	%fd1296, 0d7FF0000000000000;
	sub.f64 	%fd1297, %fd1296, %fd352;
	selp.f64	%fd1838, 0d0000000000000000, %fd1297, %p137;
	setp.geu.f32	%p138, %f3, 0f40874800;
	@%p138 bra 	BB0_212;

	shr.u32 	%r321, %r93, 31;
	add.s32 	%r322, %r93, %r321;
	shr.s32 	%r323, %r322, 1;
	shl.b32 	%r324, %r323, 20;
	add.s32 	%r325, %r324, %r95;
	mov.b64 	%fd1298, {%r94, %r325};
	sub.s32 	%r326, %r93, %r323;
	shl.b32 	%r327, %r326, 20;
	add.s32 	%r328, %r327, 1072693248;
	mov.u32 	%r329, 0;
	mov.b64 	%fd1299, {%r329, %r328};
	mul.f64 	%fd1838, %fd1298, %fd1299;

BB0_212:
	fma.rn.f64 	%fd1300, %fd347, %fd1838, %fd346;
	add.s32 	%r330, %r459, %r5;
	mul.wide.s32 	%rd210, %r330, 4;
	add.s64 	%rd211, %rd1, %rd210;
	ld.global.u32 	%r331, [%rd211];
	cvt.rn.f64.s32	%fd1301, %r331;
	sub.f64 	%fd1302, %fd1300, %fd1301;
	fma.rn.f64 	%fd1839, %fd1302, %fd1302, %fd1839;
	add.s32 	%r459, %r459, 1;
	setp.lt.s32	%p139, %r459, %r2;
	@%p139 bra 	BB0_209;

BB0_213:
	div.rn.f64 	%fd1775, %fd1839, %fd1711;
	setp.lt.f64	%p140, %fd1775, %fd55;
	mov.f64 	%fd1741, %fd54;
	mov.f64 	%fd1773, %fd1741;
	@%p140 bra 	BB0_238;

	ld.global.f64 	%fd1303, [%rd3+48];
	sub.f64 	%fd1304, %fd346, %fd1303;
	st.global.f64 	[%rd2+48], %fd1304;
	ld.global.f64 	%fd360, [%rd3+48];
	setp.lt.f64	%p141, %fd360, 0d0000000000000000;
	@%p141 bra 	BB0_216;
	bra.uni 	BB0_215;

BB0_216:
	setp.lt.s32	%p142, %r17, 20;
	@%p142 bra 	BB0_218;
	bra.uni 	BB0_217;

BB0_218:
	mul.f64 	%fd1307, %fd360, 0dBFD3333333333333;
	st.global.f64 	[%rd3+48], %fd1307;
	mov.f64 	%fd1744, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1744;
	bra.uni 	BB0_238;

BB0_220:
	setp.lt.f64	%p146, %fd361, 0d0000000000000000;
	@%p146 bra 	BB0_222;
	bra.uni 	BB0_221;

BB0_222:
	setp.lt.s32	%p147, %r17, 20;
	@%p147 bra 	BB0_224;
	bra.uni 	BB0_223;

BB0_224:
	mul.f64 	%fd1311, %fd361, 0dBFD3333333333333;
	st.global.f64 	[%rd3], %fd1311;
	bra.uni 	BB0_237;

BB0_22:
	mul.lo.s32 	%r426, %r429, 7;
	add.s32 	%r185, %r435, %r426;
	mul.wide.s32 	%rd27, %r185, 8;
	add.s64 	%rd28, %rd13, %rd27;
	add.s64 	%rd30, %rd18, %rd27;
	ld.global.f64 	%fd56, [%rd30];
	ld.global.f64 	%fd631, [%rd28];
	add.f64 	%fd57, %fd631, %fd56;
	shl.b32 	%r186, %r435, 1;
	mul.wide.s32 	%rd32, %r186, 8;
	add.s64 	%rd5, %rd17, %rd32;
	ld.global.f64 	%fd632, [%rd5];
	setp.leu.f64	%p18, %fd57, %fd632;
	@%p18 bra 	BB0_24;

	ld.global.f64 	%fd633, [%rd5+8];
	setp.lt.f64	%p19, %fd57, %fd633;
	@%p19 bra 	BB0_29;
	bra.uni 	BB0_24;

BB0_29:
	st.global.f64 	[%rd28], %fd57;
	ld.global.f64 	%fd1776, [%rd2+40];
	abs.f64 	%fd637, %fd1776;
	setp.neu.f64	%p22, %fd637, 0d7FF0000000000000;
	@%p22 bra 	BB0_31;

	mov.f64 	%fd638, 0d0000000000000000;
	mul.rn.f64 	%fd1776, %fd1776, %fd638;

BB0_31:
	mul.f64 	%fd639, %fd1776, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r438, %fd639;
	add.u64 	%rd45, %SP, 0;
	cvta.to.local.u64 	%rd46, %rd45;
	st.local.u32 	[%rd46], %r438;
	cvt.rn.f64.s32	%fd640, %r438;
	neg.f64 	%fd641, %fd640;
	mov.f64 	%fd642, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd643, %fd641, %fd642, %fd1776;
	mov.f64 	%fd644, 0d3C91A62633145C00;
	fma.rn.f64 	%fd645, %fd641, %fd644, %fd643;
	mov.f64 	%fd646, 0d397B839A252049C0;
	fma.rn.f64 	%fd1777, %fd641, %fd646, %fd645;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r191}, %fd1776;
	}
	and.b32  	%r192, %r191, 2145386496;
	setp.lt.u32	%p23, %r192, 1105199104;
	@%p23 bra 	BB0_33;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1776;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1777, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r438, [%rd46];

BB0_33:
	add.s32 	%r22, %r438, 1;
	and.b32  	%r193, %r22, 1;
	shl.b32 	%r194, %r193, 3;
	setp.eq.b32	%p24, %r193, 1;
	selp.f64	%fd647, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p24;
	mul.wide.u32 	%rd49, %r194, 8;
	mov.u64 	%rd50, __cudart_sin_cos_coeffs;
	add.s64 	%rd51, %rd49, %rd50;
	ld.const.f64 	%fd648, [%rd51+8];
	mul.rn.f64 	%fd64, %fd1777, %fd1777;
	fma.rn.f64 	%fd649, %fd647, %fd64, %fd648;
	ld.const.f64 	%fd650, [%rd51+16];
	fma.rn.f64 	%fd651, %fd649, %fd64, %fd650;
	ld.const.f64 	%fd652, [%rd51+24];
	fma.rn.f64 	%fd653, %fd651, %fd64, %fd652;
	ld.const.f64 	%fd654, [%rd51+32];
	fma.rn.f64 	%fd655, %fd653, %fd64, %fd654;
	ld.const.f64 	%fd656, [%rd51+40];
	fma.rn.f64 	%fd657, %fd655, %fd64, %fd656;
	ld.const.f64 	%fd658, [%rd51+48];
	fma.rn.f64 	%fd65, %fd657, %fd64, %fd658;
	fma.rn.f64 	%fd1778, %fd65, %fd1777, %fd1777;
	setp.eq.s32	%p25, %r193, 0;
	@%p25 bra 	BB0_35;

	mov.f64 	%fd659, 0d3FF0000000000000;
	fma.rn.f64 	%fd1778, %fd65, %fd64, %fd659;

BB0_35:
	and.b32  	%r195, %r22, 2;
	setp.eq.s32	%p26, %r195, 0;
	@%p26 bra 	BB0_37;

	mov.f64 	%fd660, 0d0000000000000000;
	mov.f64 	%fd661, 0dBFF0000000000000;
	fma.rn.f64 	%fd1778, %fd1778, %fd661, %fd660;

BB0_37:
	ld.global.f64 	%fd1779, [%rd2+40];
	abs.f64 	%fd662, %fd1779;
	setp.neu.f64	%p27, %fd662, 0d7FF0000000000000;
	@%p27 bra 	BB0_39;

	mov.f64 	%fd663, 0d0000000000000000;
	mul.rn.f64 	%fd1779, %fd1779, %fd663;

BB0_39:
	mul.f64 	%fd664, %fd1779, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r439, %fd664;
	st.local.u32 	[%rd46], %r439;
	cvt.rn.f64.s32	%fd665, %r439;
	neg.f64 	%fd666, %fd665;
	fma.rn.f64 	%fd668, %fd666, %fd642, %fd1779;
	fma.rn.f64 	%fd670, %fd666, %fd644, %fd668;
	fma.rn.f64 	%fd1780, %fd666, %fd646, %fd670;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r196}, %fd1779;
	}
	and.b32  	%r197, %r196, 2145386496;
	setp.lt.u32	%p28, %r197, 1105199104;
	@%p28 bra 	BB0_41;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1779;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1780, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r439, [%rd46];

BB0_41:
	add.s32 	%r26, %r439, 1;
	and.b32  	%r198, %r26, 1;
	shl.b32 	%r199, %r198, 3;
	setp.eq.b32	%p29, %r198, 1;
	selp.f64	%fd672, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p29;
	mul.wide.u32 	%rd56, %r199, 8;
	add.s64 	%rd58, %rd56, %rd50;
	ld.const.f64 	%fd673, [%rd58+8];
	mul.rn.f64 	%fd77, %fd1780, %fd1780;
	fma.rn.f64 	%fd674, %fd672, %fd77, %fd673;
	ld.const.f64 	%fd675, [%rd58+16];
	fma.rn.f64 	%fd676, %fd674, %fd77, %fd675;
	ld.const.f64 	%fd677, [%rd58+24];
	fma.rn.f64 	%fd678, %fd676, %fd77, %fd677;
	ld.const.f64 	%fd679, [%rd58+32];
	fma.rn.f64 	%fd680, %fd678, %fd77, %fd679;
	ld.const.f64 	%fd681, [%rd58+40];
	fma.rn.f64 	%fd682, %fd680, %fd77, %fd681;
	ld.const.f64 	%fd683, [%rd58+48];
	fma.rn.f64 	%fd78, %fd682, %fd77, %fd683;
	fma.rn.f64 	%fd1781, %fd78, %fd1780, %fd1780;
	setp.eq.s32	%p30, %r198, 0;
	@%p30 bra 	BB0_43;

	mov.f64 	%fd684, 0d3FF0000000000000;
	fma.rn.f64 	%fd1781, %fd78, %fd77, %fd684;

BB0_43:
	and.b32  	%r200, %r26, 2;
	setp.eq.s32	%p31, %r200, 0;
	@%p31 bra 	BB0_45;

	mov.f64 	%fd685, 0d0000000000000000;
	mov.f64 	%fd686, 0dBFF0000000000000;
	fma.rn.f64 	%fd1781, %fd1781, %fd686, %fd685;

BB0_45:
	ld.global.f64 	%fd687, [%rd2+24];
	add.f64 	%fd688, %fd687, %fd687;
	mul.f64 	%fd689, %fd687, %fd688;
	mul.f64 	%fd690, %fd1778, %fd1781;
	div.rn.f64 	%fd84, %fd690, %fd689;
	ld.global.f64 	%fd1782, [%rd2+40];
	abs.f64 	%fd691, %fd1782;
	setp.neu.f64	%p32, %fd691, 0d7FF0000000000000;
	@%p32 bra 	BB0_47;

	mov.f64 	%fd692, 0d0000000000000000;
	mul.rn.f64 	%fd1782, %fd1782, %fd692;

BB0_47:
	mul.f64 	%fd693, %fd1782, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r440, %fd693;
	st.local.u32 	[%rd46], %r440;
	cvt.rn.f64.s32	%fd694, %r440;
	neg.f64 	%fd695, %fd694;
	fma.rn.f64 	%fd697, %fd695, %fd642, %fd1782;
	fma.rn.f64 	%fd699, %fd695, %fd644, %fd697;
	fma.rn.f64 	%fd1783, %fd695, %fd646, %fd699;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r201}, %fd1782;
	}
	and.b32  	%r202, %r201, 2145386496;
	setp.lt.u32	%p33, %r202, 1105199104;
	@%p33 bra 	BB0_49;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1782;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1783, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r440, [%rd46];

BB0_49:
	and.b32  	%r203, %r440, 1;
	shl.b32 	%r204, %r203, 3;
	setp.eq.b32	%p34, %r203, 1;
	selp.f64	%fd701, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p34;
	mul.wide.u32 	%rd63, %r204, 8;
	add.s64 	%rd65, %rd63, %rd50;
	ld.const.f64 	%fd702, [%rd65+8];
	mul.rn.f64 	%fd91, %fd1783, %fd1783;
	fma.rn.f64 	%fd703, %fd701, %fd91, %fd702;
	ld.const.f64 	%fd704, [%rd65+16];
	fma.rn.f64 	%fd705, %fd703, %fd91, %fd704;
	ld.const.f64 	%fd706, [%rd65+24];
	fma.rn.f64 	%fd707, %fd705, %fd91, %fd706;
	ld.const.f64 	%fd708, [%rd65+32];
	fma.rn.f64 	%fd709, %fd707, %fd91, %fd708;
	ld.const.f64 	%fd710, [%rd65+40];
	fma.rn.f64 	%fd711, %fd709, %fd91, %fd710;
	ld.const.f64 	%fd712, [%rd65+48];
	fma.rn.f64 	%fd92, %fd711, %fd91, %fd712;
	fma.rn.f64 	%fd1784, %fd92, %fd1783, %fd1783;
	setp.eq.s32	%p35, %r203, 0;
	@%p35 bra 	BB0_51;

	mov.f64 	%fd713, 0d3FF0000000000000;
	fma.rn.f64 	%fd1784, %fd92, %fd91, %fd713;

BB0_51:
	and.b32  	%r205, %r440, 2;
	setp.eq.s32	%p36, %r205, 0;
	@%p36 bra 	BB0_53;

	mov.f64 	%fd714, 0d0000000000000000;
	mov.f64 	%fd715, 0dBFF0000000000000;
	fma.rn.f64 	%fd1784, %fd1784, %fd715, %fd714;

BB0_53:
	ld.global.f64 	%fd1785, [%rd2+40];
	abs.f64 	%fd716, %fd1785;
	setp.neu.f64	%p37, %fd716, 0d7FF0000000000000;
	@%p37 bra 	BB0_55;

	mov.f64 	%fd717, 0d0000000000000000;
	mul.rn.f64 	%fd1785, %fd1785, %fd717;

BB0_55:
	mul.f64 	%fd718, %fd1785, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r441, %fd718;
	st.local.u32 	[%rd46], %r441;
	cvt.rn.f64.s32	%fd719, %r441;
	neg.f64 	%fd720, %fd719;
	fma.rn.f64 	%fd722, %fd720, %fd642, %fd1785;
	fma.rn.f64 	%fd724, %fd720, %fd644, %fd722;
	fma.rn.f64 	%fd1786, %fd720, %fd646, %fd724;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r206}, %fd1785;
	}
	and.b32  	%r207, %r206, 2145386496;
	setp.lt.u32	%p38, %r207, 1105199104;
	@%p38 bra 	BB0_57;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1785;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1786, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r441, [%rd46];

BB0_57:
	and.b32  	%r208, %r441, 1;
	shl.b32 	%r209, %r208, 3;
	setp.eq.b32	%p39, %r208, 1;
	selp.f64	%fd726, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p39;
	mul.wide.u32 	%rd70, %r209, 8;
	add.s64 	%rd72, %rd70, %rd50;
	ld.const.f64 	%fd727, [%rd72+8];
	mul.rn.f64 	%fd104, %fd1786, %fd1786;
	fma.rn.f64 	%fd728, %fd726, %fd104, %fd727;
	ld.const.f64 	%fd729, [%rd72+16];
	fma.rn.f64 	%fd730, %fd728, %fd104, %fd729;
	ld.const.f64 	%fd731, [%rd72+24];
	fma.rn.f64 	%fd732, %fd730, %fd104, %fd731;
	ld.const.f64 	%fd733, [%rd72+32];
	fma.rn.f64 	%fd734, %fd732, %fd104, %fd733;
	ld.const.f64 	%fd735, [%rd72+40];
	fma.rn.f64 	%fd736, %fd734, %fd104, %fd735;
	ld.const.f64 	%fd737, [%rd72+48];
	fma.rn.f64 	%fd105, %fd736, %fd104, %fd737;
	fma.rn.f64 	%fd1787, %fd105, %fd1786, %fd1786;
	setp.eq.s32	%p40, %r208, 0;
	@%p40 bra 	BB0_59;

	mov.f64 	%fd738, 0d3FF0000000000000;
	fma.rn.f64 	%fd1787, %fd105, %fd104, %fd738;

BB0_59:
	and.b32  	%r210, %r441, 2;
	setp.eq.s32	%p41, %r210, 0;
	@%p41 bra 	BB0_61;

	mov.f64 	%fd739, 0d0000000000000000;
	mov.f64 	%fd740, 0dBFF0000000000000;
	fma.rn.f64 	%fd1787, %fd1787, %fd740, %fd739;

BB0_61:
	ld.global.f64 	%fd741, [%rd2+32];
	add.f64 	%fd742, %fd741, %fd741;
	mul.f64 	%fd743, %fd741, %fd742;
	mul.f64 	%fd744, %fd1784, %fd1787;
	div.rn.f64 	%fd745, %fd744, %fd743;
	add.f64 	%fd1730, %fd84, %fd745;
	ld.global.f64 	%fd746, [%rd2+40];
	add.f64 	%fd1788, %fd746, %fd746;
	abs.f64 	%fd747, %fd1788;
	setp.neu.f64	%p42, %fd747, 0d7FF0000000000000;
	@%p42 bra 	BB0_63;

	mov.f64 	%fd748, 0d0000000000000000;
	mul.rn.f64 	%fd1788, %fd1788, %fd748;

BB0_63:
	mul.f64 	%fd749, %fd1788, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r442, %fd749;
	st.local.u32 	[%rd46], %r442;
	cvt.rn.f64.s32	%fd750, %r442;
	neg.f64 	%fd751, %fd750;
	fma.rn.f64 	%fd753, %fd751, %fd642, %fd1788;
	fma.rn.f64 	%fd755, %fd751, %fd644, %fd753;
	fma.rn.f64 	%fd1789, %fd751, %fd646, %fd755;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r211}, %fd1788;
	}
	and.b32  	%r212, %r211, 2145386496;
	setp.lt.u32	%p43, %r212, 1105199104;
	@%p43 bra 	BB0_65;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1788;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1789, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r442, [%rd46];

BB0_65:
	and.b32  	%r213, %r442, 1;
	shl.b32 	%r214, %r213, 3;
	setp.eq.b32	%p44, %r213, 1;
	selp.f64	%fd757, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p44;
	mul.wide.u32 	%rd77, %r214, 8;
	add.s64 	%rd79, %rd77, %rd50;
	ld.const.f64 	%fd758, [%rd79+8];
	mul.rn.f64 	%fd118, %fd1789, %fd1789;
	fma.rn.f64 	%fd759, %fd757, %fd118, %fd758;
	ld.const.f64 	%fd760, [%rd79+16];
	fma.rn.f64 	%fd761, %fd759, %fd118, %fd760;
	ld.const.f64 	%fd762, [%rd79+24];
	fma.rn.f64 	%fd763, %fd761, %fd118, %fd762;
	ld.const.f64 	%fd764, [%rd79+32];
	fma.rn.f64 	%fd765, %fd763, %fd118, %fd764;
	ld.const.f64 	%fd766, [%rd79+40];
	fma.rn.f64 	%fd767, %fd765, %fd118, %fd766;
	ld.const.f64 	%fd768, [%rd79+48];
	fma.rn.f64 	%fd119, %fd767, %fd118, %fd768;
	fma.rn.f64 	%fd1790, %fd119, %fd1789, %fd1789;
	setp.eq.s32	%p45, %r213, 0;
	@%p45 bra 	BB0_67;

	mov.f64 	%fd769, 0d3FF0000000000000;
	fma.rn.f64 	%fd1790, %fd119, %fd118, %fd769;

BB0_67:
	and.b32  	%r215, %r442, 2;
	setp.eq.s32	%p46, %r215, 0;
	@%p46 bra 	BB0_69;

	mov.f64 	%fd770, 0d0000000000000000;
	mov.f64 	%fd771, 0dBFF0000000000000;
	fma.rn.f64 	%fd1790, %fd1790, %fd771, %fd770;

BB0_69:
	ld.global.f64 	%fd772, [%rd2+24];
	mul.f64 	%fd773, %fd772, 0dC010000000000000;
	mul.f64 	%fd774, %fd772, %fd773;
	div.rn.f64 	%fd125, %fd1790, %fd774;
	ld.global.f64 	%fd775, [%rd2+40];
	add.f64 	%fd1791, %fd775, %fd775;
	abs.f64 	%fd776, %fd1791;
	setp.neu.f64	%p47, %fd776, 0d7FF0000000000000;
	@%p47 bra 	BB0_71;

	mov.f64 	%fd777, 0d0000000000000000;
	mul.rn.f64 	%fd1791, %fd1791, %fd777;

BB0_71:
	mul.f64 	%fd778, %fd1791, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r443, %fd778;
	st.local.u32 	[%rd46], %r443;
	cvt.rn.f64.s32	%fd779, %r443;
	neg.f64 	%fd780, %fd779;
	fma.rn.f64 	%fd782, %fd780, %fd642, %fd1791;
	fma.rn.f64 	%fd784, %fd780, %fd644, %fd782;
	fma.rn.f64 	%fd1792, %fd780, %fd646, %fd784;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r216}, %fd1791;
	}
	and.b32  	%r217, %r216, 2145386496;
	setp.lt.u32	%p48, %r217, 1105199104;
	@%p48 bra 	BB0_73;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1791;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1792, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r443, [%rd46];

BB0_73:
	and.b32  	%r218, %r443, 1;
	shl.b32 	%r219, %r218, 3;
	setp.eq.b32	%p49, %r218, 1;
	selp.f64	%fd786, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p49;
	mul.wide.u32 	%rd84, %r219, 8;
	add.s64 	%rd86, %rd84, %rd50;
	ld.const.f64 	%fd787, [%rd86+8];
	mul.rn.f64 	%fd132, %fd1792, %fd1792;
	fma.rn.f64 	%fd788, %fd786, %fd132, %fd787;
	ld.const.f64 	%fd789, [%rd86+16];
	fma.rn.f64 	%fd790, %fd788, %fd132, %fd789;
	ld.const.f64 	%fd791, [%rd86+24];
	fma.rn.f64 	%fd792, %fd790, %fd132, %fd791;
	ld.const.f64 	%fd793, [%rd86+32];
	fma.rn.f64 	%fd794, %fd792, %fd132, %fd793;
	ld.const.f64 	%fd795, [%rd86+40];
	fma.rn.f64 	%fd796, %fd794, %fd132, %fd795;
	ld.const.f64 	%fd797, [%rd86+48];
	fma.rn.f64 	%fd133, %fd796, %fd132, %fd797;
	fma.rn.f64 	%fd1793, %fd133, %fd1792, %fd1792;
	setp.eq.s32	%p50, %r218, 0;
	@%p50 bra 	BB0_75;

	mov.f64 	%fd798, 0d3FF0000000000000;
	fma.rn.f64 	%fd1793, %fd133, %fd132, %fd798;

BB0_75:
	and.b32  	%r220, %r443, 2;
	setp.eq.s32	%p51, %r220, 0;
	@%p51 bra 	BB0_77;

	mov.f64 	%fd799, 0d0000000000000000;
	mov.f64 	%fd800, 0dBFF0000000000000;
	fma.rn.f64 	%fd1793, %fd1793, %fd800, %fd799;

BB0_77:
	ld.global.f64 	%fd801, [%rd2+32];
	mul.f64 	%fd802, %fd801, 0d4010000000000000;
	mul.f64 	%fd803, %fd801, %fd802;
	div.rn.f64 	%fd804, %fd1793, %fd803;
	add.f64 	%fd1729, %fd125, %fd804;
	ld.global.f64 	%fd1794, [%rd2+40];
	abs.f64 	%fd805, %fd1794;
	setp.neu.f64	%p52, %fd805, 0d7FF0000000000000;
	@%p52 bra 	BB0_79;

	mov.f64 	%fd806, 0d0000000000000000;
	mul.rn.f64 	%fd1794, %fd1794, %fd806;

BB0_79:
	mul.f64 	%fd807, %fd1794, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r444, %fd807;
	st.local.u32 	[%rd46], %r444;
	cvt.rn.f64.s32	%fd808, %r444;
	neg.f64 	%fd809, %fd808;
	fma.rn.f64 	%fd811, %fd809, %fd642, %fd1794;
	fma.rn.f64 	%fd813, %fd809, %fd644, %fd811;
	fma.rn.f64 	%fd1795, %fd809, %fd646, %fd813;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r221}, %fd1794;
	}
	and.b32  	%r222, %r221, 2145386496;
	setp.lt.u32	%p53, %r222, 1105199104;
	@%p53 bra 	BB0_81;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1794;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1795, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r444, [%rd46];

BB0_81:
	and.b32  	%r223, %r444, 1;
	shl.b32 	%r224, %r223, 3;
	setp.eq.b32	%p54, %r223, 1;
	selp.f64	%fd815, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p54;
	mul.wide.u32 	%rd91, %r224, 8;
	add.s64 	%rd93, %rd91, %rd50;
	ld.const.f64 	%fd816, [%rd93+8];
	mul.rn.f64 	%fd146, %fd1795, %fd1795;
	fma.rn.f64 	%fd817, %fd815, %fd146, %fd816;
	ld.const.f64 	%fd818, [%rd93+16];
	fma.rn.f64 	%fd819, %fd817, %fd146, %fd818;
	ld.const.f64 	%fd820, [%rd93+24];
	fma.rn.f64 	%fd821, %fd819, %fd146, %fd820;
	ld.const.f64 	%fd822, [%rd93+32];
	fma.rn.f64 	%fd823, %fd821, %fd146, %fd822;
	ld.const.f64 	%fd824, [%rd93+40];
	fma.rn.f64 	%fd825, %fd823, %fd146, %fd824;
	ld.const.f64 	%fd826, [%rd93+48];
	fma.rn.f64 	%fd147, %fd825, %fd146, %fd826;
	fma.rn.f64 	%fd1796, %fd147, %fd1795, %fd1795;
	setp.eq.s32	%p55, %r223, 0;
	@%p55 bra 	BB0_83;

	mov.f64 	%fd827, 0d3FF0000000000000;
	fma.rn.f64 	%fd1796, %fd147, %fd146, %fd827;

BB0_83:
	and.b32  	%r225, %r444, 2;
	setp.eq.s32	%p56, %r225, 0;
	@%p56 bra 	BB0_85;

	mov.f64 	%fd828, 0d0000000000000000;
	mov.f64 	%fd829, 0dBFF0000000000000;
	fma.rn.f64 	%fd1796, %fd1796, %fd829, %fd828;

BB0_85:
	ld.global.f64 	%fd1797, [%rd2+40];
	abs.f64 	%fd830, %fd1797;
	setp.neu.f64	%p57, %fd830, 0d7FF0000000000000;
	@%p57 bra 	BB0_87;

	mov.f64 	%fd831, 0d0000000000000000;
	mul.rn.f64 	%fd1797, %fd1797, %fd831;

BB0_87:
	mul.f64 	%fd832, %fd1797, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r445, %fd832;
	st.local.u32 	[%rd46], %r445;
	cvt.rn.f64.s32	%fd833, %r445;
	neg.f64 	%fd834, %fd833;
	fma.rn.f64 	%fd836, %fd834, %fd642, %fd1797;
	fma.rn.f64 	%fd838, %fd834, %fd644, %fd836;
	fma.rn.f64 	%fd1798, %fd834, %fd646, %fd838;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r226}, %fd1797;
	}
	and.b32  	%r227, %r226, 2145386496;
	setp.lt.u32	%p58, %r227, 1105199104;
	@%p58 bra 	BB0_89;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1797;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1798, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r445, [%rd46];

BB0_89:
	and.b32  	%r228, %r445, 1;
	shl.b32 	%r229, %r228, 3;
	setp.eq.b32	%p59, %r228, 1;
	selp.f64	%fd840, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p59;
	mul.wide.u32 	%rd98, %r229, 8;
	add.s64 	%rd100, %rd98, %rd50;
	ld.const.f64 	%fd841, [%rd100+8];
	mul.rn.f64 	%fd159, %fd1798, %fd1798;
	fma.rn.f64 	%fd842, %fd840, %fd159, %fd841;
	ld.const.f64 	%fd843, [%rd100+16];
	fma.rn.f64 	%fd844, %fd842, %fd159, %fd843;
	ld.const.f64 	%fd845, [%rd100+24];
	fma.rn.f64 	%fd846, %fd844, %fd159, %fd845;
	ld.const.f64 	%fd847, [%rd100+32];
	fma.rn.f64 	%fd848, %fd846, %fd159, %fd847;
	ld.const.f64 	%fd849, [%rd100+40];
	fma.rn.f64 	%fd850, %fd848, %fd159, %fd849;
	ld.const.f64 	%fd851, [%rd100+48];
	fma.rn.f64 	%fd160, %fd850, %fd159, %fd851;
	fma.rn.f64 	%fd1799, %fd160, %fd1798, %fd1798;
	setp.eq.s32	%p60, %r228, 0;
	@%p60 bra 	BB0_91;

	mov.f64 	%fd852, 0d3FF0000000000000;
	fma.rn.f64 	%fd1799, %fd160, %fd159, %fd852;

BB0_91:
	and.b32  	%r230, %r445, 2;
	setp.eq.s32	%p61, %r230, 0;
	@%p61 bra 	BB0_93;

	mov.f64 	%fd853, 0d0000000000000000;
	mov.f64 	%fd854, 0dBFF0000000000000;
	fma.rn.f64 	%fd1799, %fd1799, %fd854, %fd853;

BB0_93:
	ld.global.f64 	%fd855, [%rd2+24];
	add.f64 	%fd856, %fd855, %fd855;
	mul.f64 	%fd857, %fd855, %fd856;
	mul.f64 	%fd858, %fd1796, %fd1799;
	div.rn.f64 	%fd166, %fd858, %fd857;
	ld.global.f64 	%fd1800, [%rd2+40];
	abs.f64 	%fd859, %fd1800;
	setp.neu.f64	%p62, %fd859, 0d7FF0000000000000;
	@%p62 bra 	BB0_95;

	mov.f64 	%fd860, 0d0000000000000000;
	mul.rn.f64 	%fd1800, %fd1800, %fd860;

BB0_95:
	mul.f64 	%fd861, %fd1800, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r446, %fd861;
	st.local.u32 	[%rd46], %r446;
	cvt.rn.f64.s32	%fd862, %r446;
	neg.f64 	%fd863, %fd862;
	fma.rn.f64 	%fd865, %fd863, %fd642, %fd1800;
	fma.rn.f64 	%fd867, %fd863, %fd644, %fd865;
	fma.rn.f64 	%fd1801, %fd863, %fd646, %fd867;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r231}, %fd1800;
	}
	and.b32  	%r232, %r231, 2145386496;
	setp.lt.u32	%p63, %r232, 1105199104;
	@%p63 bra 	BB0_97;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1800;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1801, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r446, [%rd46];

BB0_97:
	add.s32 	%r48, %r446, 1;
	and.b32  	%r233, %r48, 1;
	shl.b32 	%r234, %r233, 3;
	setp.eq.b32	%p64, %r233, 1;
	selp.f64	%fd869, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p64;
	mul.wide.u32 	%rd105, %r234, 8;
	add.s64 	%rd107, %rd105, %rd50;
	ld.const.f64 	%fd870, [%rd107+8];
	mul.rn.f64 	%fd173, %fd1801, %fd1801;
	fma.rn.f64 	%fd871, %fd869, %fd173, %fd870;
	ld.const.f64 	%fd872, [%rd107+16];
	fma.rn.f64 	%fd873, %fd871, %fd173, %fd872;
	ld.const.f64 	%fd874, [%rd107+24];
	fma.rn.f64 	%fd875, %fd873, %fd173, %fd874;
	ld.const.f64 	%fd876, [%rd107+32];
	fma.rn.f64 	%fd877, %fd875, %fd173, %fd876;
	ld.const.f64 	%fd878, [%rd107+40];
	fma.rn.f64 	%fd879, %fd877, %fd173, %fd878;
	ld.const.f64 	%fd880, [%rd107+48];
	fma.rn.f64 	%fd174, %fd879, %fd173, %fd880;
	fma.rn.f64 	%fd1802, %fd174, %fd1801, %fd1801;
	setp.eq.s32	%p65, %r233, 0;
	@%p65 bra 	BB0_99;

	mov.f64 	%fd881, 0d3FF0000000000000;
	fma.rn.f64 	%fd1802, %fd174, %fd173, %fd881;

BB0_99:
	and.b32  	%r235, %r48, 2;
	setp.eq.s32	%p66, %r235, 0;
	@%p66 bra 	BB0_101;

	mov.f64 	%fd882, 0d0000000000000000;
	mov.f64 	%fd883, 0dBFF0000000000000;
	fma.rn.f64 	%fd1802, %fd1802, %fd883, %fd882;

BB0_101:
	ld.global.f64 	%fd1803, [%rd2+40];
	abs.f64 	%fd884, %fd1803;
	setp.neu.f64	%p67, %fd884, 0d7FF0000000000000;
	@%p67 bra 	BB0_103;

	mov.f64 	%fd885, 0d0000000000000000;
	mul.rn.f64 	%fd1803, %fd1803, %fd885;

BB0_103:
	mul.f64 	%fd886, %fd1803, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r447, %fd886;
	st.local.u32 	[%rd46], %r447;
	cvt.rn.f64.s32	%fd887, %r447;
	neg.f64 	%fd888, %fd887;
	fma.rn.f64 	%fd890, %fd888, %fd642, %fd1803;
	fma.rn.f64 	%fd892, %fd888, %fd644, %fd890;
	fma.rn.f64 	%fd1804, %fd888, %fd646, %fd892;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r236}, %fd1803;
	}
	and.b32  	%r237, %r236, 2145386496;
	setp.lt.u32	%p68, %r237, 1105199104;
	@%p68 bra 	BB0_105;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1803;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1804, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r447, [%rd46];

BB0_105:
	add.s32 	%r52, %r447, 1;
	and.b32  	%r238, %r52, 1;
	shl.b32 	%r239, %r238, 3;
	setp.eq.b32	%p69, %r238, 1;
	selp.f64	%fd894, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p69;
	mul.wide.u32 	%rd112, %r239, 8;
	add.s64 	%rd114, %rd112, %rd50;
	ld.const.f64 	%fd895, [%rd114+8];
	mul.rn.f64 	%fd186, %fd1804, %fd1804;
	fma.rn.f64 	%fd896, %fd894, %fd186, %fd895;
	ld.const.f64 	%fd897, [%rd114+16];
	fma.rn.f64 	%fd898, %fd896, %fd186, %fd897;
	ld.const.f64 	%fd899, [%rd114+24];
	fma.rn.f64 	%fd900, %fd898, %fd186, %fd899;
	ld.const.f64 	%fd901, [%rd114+32];
	fma.rn.f64 	%fd902, %fd900, %fd186, %fd901;
	ld.const.f64 	%fd903, [%rd114+40];
	fma.rn.f64 	%fd904, %fd902, %fd186, %fd903;
	ld.const.f64 	%fd905, [%rd114+48];
	fma.rn.f64 	%fd187, %fd904, %fd186, %fd905;
	fma.rn.f64 	%fd1805, %fd187, %fd1804, %fd1804;
	setp.eq.s32	%p70, %r238, 0;
	@%p70 bra 	BB0_107;

	mov.f64 	%fd906, 0d3FF0000000000000;
	fma.rn.f64 	%fd1805, %fd187, %fd186, %fd906;

BB0_107:
	and.b32  	%r240, %r52, 2;
	setp.eq.s32	%p71, %r240, 0;
	@%p71 bra 	BB0_109;

	mov.f64 	%fd907, 0d0000000000000000;
	mov.f64 	%fd908, 0dBFF0000000000000;
	fma.rn.f64 	%fd1805, %fd1805, %fd908, %fd907;

BB0_109:
	ld.global.f64 	%fd910, [%rd2+32];
	add.f64 	%fd911, %fd910, %fd910;
	mul.f64 	%fd912, %fd910, %fd911;
	mul.f64 	%fd913, %fd1802, %fd1805;
	div.rn.f64 	%fd914, %fd913, %fd912;
	add.f64 	%fd1728, %fd166, %fd914;
	mov.f64 	%fd1807, 0d0000000000000000;
	@%p2 bra 	BB0_115;

	ld.global.f64 	%fd194, [%rd2];
	ld.global.f64 	%fd195, [%rd2+8];
	ld.global.f64 	%fd197, [%rd2+16];
	ld.global.f64 	%fd198, [%rd2+48];
	mov.f64 	%fd1807, 0d0000000000000000;
	mov.u32 	%r448, 0;

BB0_111:
	add.f64 	%fd1698, %fd1729, %fd1729;
	rem.s32 	%r243, %r448, %r1;
	cvt.rn.f64.s32	%fd916, %r243;
	sub.f64 	%fd917, %fd916, %fd195;
	mul.f64 	%fd918, %fd1730, %fd917;
	mul.f64 	%fd919, %fd917, %fd918;
	mul.f64 	%fd920, %fd1698, %fd917;
	div.s32 	%r244, %r448, %r1;
	cvt.rn.f64.s32	%fd921, %r244;
	sub.f64 	%fd922, %fd921, %fd197;
	mul.f64 	%fd923, %fd920, %fd922;
	sub.f64 	%fd924, %fd919, %fd923;
	mul.f64 	%fd925, %fd1728, %fd922;
	fma.rn.f64 	%fd200, %fd922, %fd925, %fd924;
	neg.f64 	%fd926, %fd200;
	mov.f64 	%fd927, 0d4338000000000000;
	mov.f64 	%fd928, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd929, %fd926, %fd928, %fd927;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd929;
	}
	mov.f64 	%fd930, 0dC338000000000000;
	add.rn.f64 	%fd931, %fd929, %fd930;
	mov.f64 	%fd932, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd933, %fd931, %fd932, %fd926;
	mov.f64 	%fd934, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd935, %fd931, %fd934, %fd933;
	mov.f64 	%fd936, 0d3E928AF3FCA213EA;
	mov.f64 	%fd937, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd938, %fd937, %fd935, %fd936;
	mov.f64 	%fd939, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd940, %fd938, %fd935, %fd939;
	mov.f64 	%fd941, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd942, %fd940, %fd935, %fd941;
	mov.f64 	%fd943, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd944, %fd942, %fd935, %fd943;
	mov.f64 	%fd945, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd946, %fd944, %fd935, %fd945;
	mov.f64 	%fd947, 0d3F81111111122322;
	fma.rn.f64 	%fd948, %fd946, %fd935, %fd947;
	mov.f64 	%fd949, 0d3FA55555555502A1;
	fma.rn.f64 	%fd950, %fd948, %fd935, %fd949;
	mov.f64 	%fd951, 0d3FC5555555555511;
	fma.rn.f64 	%fd952, %fd950, %fd935, %fd951;
	mov.f64 	%fd953, 0d3FE000000000000B;
	fma.rn.f64 	%fd954, %fd952, %fd935, %fd953;
	mov.f64 	%fd955, 0d3FF0000000000000;
	fma.rn.f64 	%fd956, %fd954, %fd935, %fd955;
	fma.rn.f64 	%fd957, %fd956, %fd935, %fd955;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r55, %temp}, %fd957;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r56}, %fd957;
	}
	shl.b32 	%r245, %r54, 20;
	add.s32 	%r246, %r56, %r245;
	mov.b64 	%fd1806, {%r55, %r246};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r247}, %fd926;
	}
	mov.b32 	 %f7, %r247;
	abs.f32 	%f2, %f7;
	setp.lt.f32	%p73, %f2, 0f4086232B;
	@%p73 bra 	BB0_114;

	setp.gt.f64	%p74, %fd200, 0d8000000000000000;
	mov.f64 	%fd958, 0d7FF0000000000000;
	sub.f64 	%fd959, %fd958, %fd200;
	selp.f64	%fd1806, 0d0000000000000000, %fd959, %p74;
	setp.geu.f32	%p75, %f2, 0f40874800;
	@%p75 bra 	BB0_114;

	shr.u32 	%r248, %r54, 31;
	add.s32 	%r249, %r54, %r248;
	shr.s32 	%r250, %r249, 1;
	shl.b32 	%r251, %r250, 20;
	add.s32 	%r252, %r251, %r56;
	mov.b64 	%fd960, {%r55, %r252};
	sub.s32 	%r253, %r54, %r250;
	shl.b32 	%r254, %r253, 20;
	add.s32 	%r255, %r254, 1072693248;
	mov.u32 	%r256, 0;
	mov.b64 	%fd961, {%r256, %r255};
	mul.f64 	%fd1806, %fd960, %fd961;

BB0_114:
	fma.rn.f64 	%fd962, %fd194, %fd1806, %fd198;
	add.s32 	%r257, %r448, %r5;
	mul.wide.s32 	%rd118, %r257, 4;
	add.s64 	%rd119, %rd1, %rd118;
	ld.global.u32 	%r258, [%rd119];
	cvt.rn.f64.s32	%fd963, %r258;
	sub.f64 	%fd964, %fd962, %fd963;
	fma.rn.f64 	%fd1807, %fd964, %fd964, %fd1807;
	add.s32 	%r448, %r448, 1;
	setp.lt.s32	%p76, %r448, %r2;
	@%p76 bra 	BB0_111;

BB0_115:
	div.rn.f64 	%fd1775, %fd1807, %fd1711;
	setp.lt.f64	%p77, %fd1775, %fd55;
	mov.f64 	%fd1773, %fd54;
	@%p77 bra 	BB0_238;

	mad.lo.s32 	%r259, %r429, 7, %r435;
	mul.wide.s32 	%rd121, %r259, 8;
	add.s64 	%rd122, %rd18, %rd121;
	add.s64 	%rd124, %rd13, %rd121;
	ld.global.f64 	%fd965, [%rd124];
	ld.global.f64 	%fd966, [%rd122];
	sub.f64 	%fd967, %fd965, %fd966;
	st.global.f64 	[%rd124], %fd967;
	ld.global.f64 	%fd1808, [%rd2+40];
	abs.f64 	%fd968, %fd1808;
	setp.neu.f64	%p78, %fd968, 0d7FF0000000000000;
	@%p78 bra 	BB0_118;

	mov.f64 	%fd969, 0d0000000000000000;
	mul.rn.f64 	%fd1808, %fd1808, %fd969;

BB0_118:
	mul.f64 	%fd970, %fd1808, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r449, %fd970;
	st.local.u32 	[%rd46], %r449;
	cvt.rn.f64.s32	%fd971, %r449;
	neg.f64 	%fd972, %fd971;
	fma.rn.f64 	%fd974, %fd972, %fd642, %fd1808;
	fma.rn.f64 	%fd976, %fd972, %fd644, %fd974;
	fma.rn.f64 	%fd1809, %fd972, %fd646, %fd976;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r260}, %fd1808;
	}
	and.b32  	%r261, %r260, 2145386496;
	setp.lt.u32	%p79, %r261, 1105199104;
	@%p79 bra 	BB0_120;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1808;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1809, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r449, [%rd46];

BB0_120:
	add.s32 	%r61, %r449, 1;
	and.b32  	%r262, %r61, 1;
	shl.b32 	%r263, %r262, 3;
	setp.eq.b32	%p80, %r262, 1;
	selp.f64	%fd978, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p80;
	mul.wide.u32 	%rd129, %r263, 8;
	add.s64 	%rd131, %rd129, %rd50;
	ld.const.f64 	%fd979, [%rd131+8];
	mul.rn.f64 	%fd214, %fd1809, %fd1809;
	fma.rn.f64 	%fd980, %fd978, %fd214, %fd979;
	ld.const.f64 	%fd981, [%rd131+16];
	fma.rn.f64 	%fd982, %fd980, %fd214, %fd981;
	ld.const.f64 	%fd983, [%rd131+24];
	fma.rn.f64 	%fd984, %fd982, %fd214, %fd983;
	ld.const.f64 	%fd985, [%rd131+32];
	fma.rn.f64 	%fd986, %fd984, %fd214, %fd985;
	ld.const.f64 	%fd987, [%rd131+40];
	fma.rn.f64 	%fd988, %fd986, %fd214, %fd987;
	ld.const.f64 	%fd989, [%rd131+48];
	fma.rn.f64 	%fd215, %fd988, %fd214, %fd989;
	fma.rn.f64 	%fd1810, %fd215, %fd1809, %fd1809;
	setp.eq.s32	%p81, %r262, 0;
	@%p81 bra 	BB0_122;

	mov.f64 	%fd990, 0d3FF0000000000000;
	fma.rn.f64 	%fd1810, %fd215, %fd214, %fd990;

BB0_122:
	and.b32  	%r264, %r61, 2;
	setp.eq.s32	%p82, %r264, 0;
	@%p82 bra 	BB0_124;

	mov.f64 	%fd991, 0d0000000000000000;
	mov.f64 	%fd992, 0dBFF0000000000000;
	fma.rn.f64 	%fd1810, %fd1810, %fd992, %fd991;

BB0_124:
	ld.global.f64 	%fd1811, [%rd2+40];
	abs.f64 	%fd993, %fd1811;
	setp.neu.f64	%p83, %fd993, 0d7FF0000000000000;
	@%p83 bra 	BB0_126;

	mov.f64 	%fd994, 0d0000000000000000;
	mul.rn.f64 	%fd1811, %fd1811, %fd994;

BB0_126:
	mul.f64 	%fd995, %fd1811, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r450, %fd995;
	st.local.u32 	[%rd46], %r450;
	cvt.rn.f64.s32	%fd996, %r450;
	neg.f64 	%fd997, %fd996;
	fma.rn.f64 	%fd999, %fd997, %fd642, %fd1811;
	fma.rn.f64 	%fd1001, %fd997, %fd644, %fd999;
	fma.rn.f64 	%fd1812, %fd997, %fd646, %fd1001;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r265}, %fd1811;
	}
	and.b32  	%r266, %r265, 2145386496;
	setp.lt.u32	%p84, %r266, 1105199104;
	@%p84 bra 	BB0_128;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1811;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1812, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r450, [%rd46];

BB0_128:
	add.s32 	%r65, %r450, 1;
	and.b32  	%r267, %r65, 1;
	shl.b32 	%r268, %r267, 3;
	setp.eq.b32	%p85, %r267, 1;
	selp.f64	%fd1003, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p85;
	mul.wide.u32 	%rd136, %r268, 8;
	add.s64 	%rd138, %rd136, %rd50;
	ld.const.f64 	%fd1004, [%rd138+8];
	mul.rn.f64 	%fd227, %fd1812, %fd1812;
	fma.rn.f64 	%fd1005, %fd1003, %fd227, %fd1004;
	ld.const.f64 	%fd1006, [%rd138+16];
	fma.rn.f64 	%fd1007, %fd1005, %fd227, %fd1006;
	ld.const.f64 	%fd1008, [%rd138+24];
	fma.rn.f64 	%fd1009, %fd1007, %fd227, %fd1008;
	ld.const.f64 	%fd1010, [%rd138+32];
	fma.rn.f64 	%fd1011, %fd1009, %fd227, %fd1010;
	ld.const.f64 	%fd1012, [%rd138+40];
	fma.rn.f64 	%fd1013, %fd1011, %fd227, %fd1012;
	ld.const.f64 	%fd1014, [%rd138+48];
	fma.rn.f64 	%fd228, %fd1013, %fd227, %fd1014;
	fma.rn.f64 	%fd1813, %fd228, %fd1812, %fd1812;
	setp.eq.s32	%p86, %r267, 0;
	@%p86 bra 	BB0_130;

	mov.f64 	%fd1015, 0d3FF0000000000000;
	fma.rn.f64 	%fd1813, %fd228, %fd227, %fd1015;

BB0_130:
	and.b32  	%r269, %r65, 2;
	setp.eq.s32	%p87, %r269, 0;
	@%p87 bra 	BB0_132;

	mov.f64 	%fd1016, 0d0000000000000000;
	mov.f64 	%fd1017, 0dBFF0000000000000;
	fma.rn.f64 	%fd1813, %fd1813, %fd1017, %fd1016;

BB0_132:
	ld.global.f64 	%fd1018, [%rd2+24];
	add.f64 	%fd1019, %fd1018, %fd1018;
	mul.f64 	%fd1020, %fd1018, %fd1019;
	mul.f64 	%fd1021, %fd1810, %fd1813;
	div.rn.f64 	%fd234, %fd1021, %fd1020;
	ld.global.f64 	%fd1814, [%rd2+40];
	abs.f64 	%fd1022, %fd1814;
	setp.neu.f64	%p88, %fd1022, 0d7FF0000000000000;
	@%p88 bra 	BB0_134;

	mov.f64 	%fd1023, 0d0000000000000000;
	mul.rn.f64 	%fd1814, %fd1814, %fd1023;

BB0_134:
	mul.f64 	%fd1024, %fd1814, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r451, %fd1024;
	st.local.u32 	[%rd46], %r451;
	cvt.rn.f64.s32	%fd1025, %r451;
	neg.f64 	%fd1026, %fd1025;
	fma.rn.f64 	%fd1028, %fd1026, %fd642, %fd1814;
	fma.rn.f64 	%fd1030, %fd1026, %fd644, %fd1028;
	fma.rn.f64 	%fd1815, %fd1026, %fd646, %fd1030;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r270}, %fd1814;
	}
	and.b32  	%r271, %r270, 2145386496;
	setp.lt.u32	%p89, %r271, 1105199104;
	@%p89 bra 	BB0_136;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1814;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1815, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r451, [%rd46];

BB0_136:
	and.b32  	%r272, %r451, 1;
	shl.b32 	%r273, %r272, 3;
	setp.eq.b32	%p90, %r272, 1;
	selp.f64	%fd1032, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p90;
	mul.wide.u32 	%rd143, %r273, 8;
	add.s64 	%rd145, %rd143, %rd50;
	ld.const.f64 	%fd1033, [%rd145+8];
	mul.rn.f64 	%fd241, %fd1815, %fd1815;
	fma.rn.f64 	%fd1034, %fd1032, %fd241, %fd1033;
	ld.const.f64 	%fd1035, [%rd145+16];
	fma.rn.f64 	%fd1036, %fd1034, %fd241, %fd1035;
	ld.const.f64 	%fd1037, [%rd145+24];
	fma.rn.f64 	%fd1038, %fd1036, %fd241, %fd1037;
	ld.const.f64 	%fd1039, [%rd145+32];
	fma.rn.f64 	%fd1040, %fd1038, %fd241, %fd1039;
	ld.const.f64 	%fd1041, [%rd145+40];
	fma.rn.f64 	%fd1042, %fd1040, %fd241, %fd1041;
	ld.const.f64 	%fd1043, [%rd145+48];
	fma.rn.f64 	%fd242, %fd1042, %fd241, %fd1043;
	fma.rn.f64 	%fd1816, %fd242, %fd1815, %fd1815;
	setp.eq.s32	%p91, %r272, 0;
	@%p91 bra 	BB0_138;

	mov.f64 	%fd1044, 0d3FF0000000000000;
	fma.rn.f64 	%fd1816, %fd242, %fd241, %fd1044;

BB0_138:
	and.b32  	%r274, %r451, 2;
	setp.eq.s32	%p92, %r274, 0;
	@%p92 bra 	BB0_140;

	mov.f64 	%fd1045, 0d0000000000000000;
	mov.f64 	%fd1046, 0dBFF0000000000000;
	fma.rn.f64 	%fd1816, %fd1816, %fd1046, %fd1045;

BB0_140:
	ld.global.f64 	%fd1817, [%rd2+40];
	abs.f64 	%fd1047, %fd1817;
	setp.neu.f64	%p93, %fd1047, 0d7FF0000000000000;
	@%p93 bra 	BB0_142;

	mov.f64 	%fd1048, 0d0000000000000000;
	mul.rn.f64 	%fd1817, %fd1817, %fd1048;

BB0_142:
	mul.f64 	%fd1049, %fd1817, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r452, %fd1049;
	st.local.u32 	[%rd46], %r452;
	cvt.rn.f64.s32	%fd1050, %r452;
	neg.f64 	%fd1051, %fd1050;
	fma.rn.f64 	%fd1053, %fd1051, %fd642, %fd1817;
	fma.rn.f64 	%fd1055, %fd1051, %fd644, %fd1053;
	fma.rn.f64 	%fd1818, %fd1051, %fd646, %fd1055;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r275}, %fd1817;
	}
	and.b32  	%r276, %r275, 2145386496;
	setp.lt.u32	%p94, %r276, 1105199104;
	@%p94 bra 	BB0_144;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1817;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1818, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r452, [%rd46];

BB0_144:
	and.b32  	%r277, %r452, 1;
	shl.b32 	%r278, %r277, 3;
	setp.eq.b32	%p95, %r277, 1;
	selp.f64	%fd1057, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p95;
	mul.wide.u32 	%rd150, %r278, 8;
	add.s64 	%rd152, %rd150, %rd50;
	ld.const.f64 	%fd1058, [%rd152+8];
	mul.rn.f64 	%fd254, %fd1818, %fd1818;
	fma.rn.f64 	%fd1059, %fd1057, %fd254, %fd1058;
	ld.const.f64 	%fd1060, [%rd152+16];
	fma.rn.f64 	%fd1061, %fd1059, %fd254, %fd1060;
	ld.const.f64 	%fd1062, [%rd152+24];
	fma.rn.f64 	%fd1063, %fd1061, %fd254, %fd1062;
	ld.const.f64 	%fd1064, [%rd152+32];
	fma.rn.f64 	%fd1065, %fd1063, %fd254, %fd1064;
	ld.const.f64 	%fd1066, [%rd152+40];
	fma.rn.f64 	%fd1067, %fd1065, %fd254, %fd1066;
	ld.const.f64 	%fd1068, [%rd152+48];
	fma.rn.f64 	%fd255, %fd1067, %fd254, %fd1068;
	fma.rn.f64 	%fd1819, %fd255, %fd1818, %fd1818;
	setp.eq.s32	%p96, %r277, 0;
	@%p96 bra 	BB0_146;

	mov.f64 	%fd1069, 0d3FF0000000000000;
	fma.rn.f64 	%fd1819, %fd255, %fd254, %fd1069;

BB0_146:
	and.b32  	%r279, %r452, 2;
	setp.eq.s32	%p97, %r279, 0;
	@%p97 bra 	BB0_148;

	mov.f64 	%fd1070, 0d0000000000000000;
	mov.f64 	%fd1071, 0dBFF0000000000000;
	fma.rn.f64 	%fd1819, %fd1819, %fd1071, %fd1070;

BB0_148:
	ld.global.f64 	%fd1072, [%rd2+32];
	add.f64 	%fd1073, %fd1072, %fd1072;
	mul.f64 	%fd1074, %fd1072, %fd1073;
	mul.f64 	%fd1075, %fd1816, %fd1819;
	div.rn.f64 	%fd1076, %fd1075, %fd1074;
	add.f64 	%fd1730, %fd234, %fd1076;
	ld.global.f64 	%fd1077, [%rd2+40];
	add.f64 	%fd1820, %fd1077, %fd1077;
	abs.f64 	%fd1078, %fd1820;
	setp.neu.f64	%p98, %fd1078, 0d7FF0000000000000;
	@%p98 bra 	BB0_150;

	mov.f64 	%fd1079, 0d0000000000000000;
	mul.rn.f64 	%fd1820, %fd1820, %fd1079;

BB0_150:
	mul.f64 	%fd1080, %fd1820, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r453, %fd1080;
	st.local.u32 	[%rd46], %r453;
	cvt.rn.f64.s32	%fd1081, %r453;
	neg.f64 	%fd1082, %fd1081;
	fma.rn.f64 	%fd1084, %fd1082, %fd642, %fd1820;
	fma.rn.f64 	%fd1086, %fd1082, %fd644, %fd1084;
	fma.rn.f64 	%fd1821, %fd1082, %fd646, %fd1086;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r280}, %fd1820;
	}
	and.b32  	%r281, %r280, 2145386496;
	setp.lt.u32	%p99, %r281, 1105199104;
	@%p99 bra 	BB0_152;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1820;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1821, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r453, [%rd46];

BB0_152:
	and.b32  	%r282, %r453, 1;
	shl.b32 	%r283, %r282, 3;
	setp.eq.b32	%p100, %r282, 1;
	selp.f64	%fd1088, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p100;
	mul.wide.u32 	%rd157, %r283, 8;
	add.s64 	%rd159, %rd157, %rd50;
	ld.const.f64 	%fd1089, [%rd159+8];
	mul.rn.f64 	%fd268, %fd1821, %fd1821;
	fma.rn.f64 	%fd1090, %fd1088, %fd268, %fd1089;
	ld.const.f64 	%fd1091, [%rd159+16];
	fma.rn.f64 	%fd1092, %fd1090, %fd268, %fd1091;
	ld.const.f64 	%fd1093, [%rd159+24];
	fma.rn.f64 	%fd1094, %fd1092, %fd268, %fd1093;
	ld.const.f64 	%fd1095, [%rd159+32];
	fma.rn.f64 	%fd1096, %fd1094, %fd268, %fd1095;
	ld.const.f64 	%fd1097, [%rd159+40];
	fma.rn.f64 	%fd1098, %fd1096, %fd268, %fd1097;
	ld.const.f64 	%fd1099, [%rd159+48];
	fma.rn.f64 	%fd269, %fd1098, %fd268, %fd1099;
	fma.rn.f64 	%fd1822, %fd269, %fd1821, %fd1821;
	setp.eq.s32	%p101, %r282, 0;
	@%p101 bra 	BB0_154;

	mov.f64 	%fd1100, 0d3FF0000000000000;
	fma.rn.f64 	%fd1822, %fd269, %fd268, %fd1100;

BB0_154:
	and.b32  	%r284, %r453, 2;
	setp.eq.s32	%p102, %r284, 0;
	@%p102 bra 	BB0_156;

	mov.f64 	%fd1101, 0d0000000000000000;
	mov.f64 	%fd1102, 0dBFF0000000000000;
	fma.rn.f64 	%fd1822, %fd1822, %fd1102, %fd1101;

BB0_156:
	ld.global.f64 	%fd1103, [%rd2+24];
	mul.f64 	%fd1104, %fd1103, 0dC010000000000000;
	mul.f64 	%fd1105, %fd1103, %fd1104;
	div.rn.f64 	%fd275, %fd1822, %fd1105;
	ld.global.f64 	%fd1106, [%rd2+40];
	add.f64 	%fd1823, %fd1106, %fd1106;
	abs.f64 	%fd1107, %fd1823;
	setp.neu.f64	%p103, %fd1107, 0d7FF0000000000000;
	@%p103 bra 	BB0_158;

	mov.f64 	%fd1108, 0d0000000000000000;
	mul.rn.f64 	%fd1823, %fd1823, %fd1108;

BB0_158:
	mul.f64 	%fd1109, %fd1823, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r454, %fd1109;
	st.local.u32 	[%rd46], %r454;
	cvt.rn.f64.s32	%fd1110, %r454;
	neg.f64 	%fd1111, %fd1110;
	fma.rn.f64 	%fd1113, %fd1111, %fd642, %fd1823;
	fma.rn.f64 	%fd1115, %fd1111, %fd644, %fd1113;
	fma.rn.f64 	%fd1824, %fd1111, %fd646, %fd1115;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r285}, %fd1823;
	}
	and.b32  	%r286, %r285, 2145386496;
	setp.lt.u32	%p104, %r286, 1105199104;
	@%p104 bra 	BB0_160;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1823;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1824, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r454, [%rd46];

BB0_160:
	and.b32  	%r287, %r454, 1;
	shl.b32 	%r288, %r287, 3;
	setp.eq.b32	%p105, %r287, 1;
	selp.f64	%fd1117, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p105;
	mul.wide.u32 	%rd164, %r288, 8;
	add.s64 	%rd166, %rd164, %rd50;
	ld.const.f64 	%fd1118, [%rd166+8];
	mul.rn.f64 	%fd282, %fd1824, %fd1824;
	fma.rn.f64 	%fd1119, %fd1117, %fd282, %fd1118;
	ld.const.f64 	%fd1120, [%rd166+16];
	fma.rn.f64 	%fd1121, %fd1119, %fd282, %fd1120;
	ld.const.f64 	%fd1122, [%rd166+24];
	fma.rn.f64 	%fd1123, %fd1121, %fd282, %fd1122;
	ld.const.f64 	%fd1124, [%rd166+32];
	fma.rn.f64 	%fd1125, %fd1123, %fd282, %fd1124;
	ld.const.f64 	%fd1126, [%rd166+40];
	fma.rn.f64 	%fd1127, %fd1125, %fd282, %fd1126;
	ld.const.f64 	%fd1128, [%rd166+48];
	fma.rn.f64 	%fd283, %fd1127, %fd282, %fd1128;
	fma.rn.f64 	%fd1825, %fd283, %fd1824, %fd1824;
	setp.eq.s32	%p106, %r287, 0;
	@%p106 bra 	BB0_162;

	mov.f64 	%fd1129, 0d3FF0000000000000;
	fma.rn.f64 	%fd1825, %fd283, %fd282, %fd1129;

BB0_162:
	and.b32  	%r289, %r454, 2;
	setp.eq.s32	%p107, %r289, 0;
	@%p107 bra 	BB0_164;

	mov.f64 	%fd1130, 0d0000000000000000;
	mov.f64 	%fd1131, 0dBFF0000000000000;
	fma.rn.f64 	%fd1825, %fd1825, %fd1131, %fd1130;

BB0_164:
	ld.global.f64 	%fd1132, [%rd2+32];
	mul.f64 	%fd1133, %fd1132, 0d4010000000000000;
	mul.f64 	%fd1134, %fd1132, %fd1133;
	div.rn.f64 	%fd1135, %fd1825, %fd1134;
	add.f64 	%fd1729, %fd275, %fd1135;
	ld.global.f64 	%fd1826, [%rd2+40];
	abs.f64 	%fd1136, %fd1826;
	setp.neu.f64	%p108, %fd1136, 0d7FF0000000000000;
	@%p108 bra 	BB0_166;

	mov.f64 	%fd1137, 0d0000000000000000;
	mul.rn.f64 	%fd1826, %fd1826, %fd1137;

BB0_166:
	mul.f64 	%fd1138, %fd1826, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r455, %fd1138;
	st.local.u32 	[%rd46], %r455;
	cvt.rn.f64.s32	%fd1139, %r455;
	neg.f64 	%fd1140, %fd1139;
	fma.rn.f64 	%fd1142, %fd1140, %fd642, %fd1826;
	fma.rn.f64 	%fd1144, %fd1140, %fd644, %fd1142;
	fma.rn.f64 	%fd1827, %fd1140, %fd646, %fd1144;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r290}, %fd1826;
	}
	and.b32  	%r291, %r290, 2145386496;
	setp.lt.u32	%p109, %r291, 1105199104;
	@%p109 bra 	BB0_168;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1826;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1827, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r455, [%rd46];

BB0_168:
	and.b32  	%r292, %r455, 1;
	shl.b32 	%r293, %r292, 3;
	setp.eq.b32	%p110, %r292, 1;
	selp.f64	%fd1146, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p110;
	mul.wide.u32 	%rd171, %r293, 8;
	add.s64 	%rd173, %rd171, %rd50;
	ld.const.f64 	%fd1147, [%rd173+8];
	mul.rn.f64 	%fd296, %fd1827, %fd1827;
	fma.rn.f64 	%fd1148, %fd1146, %fd296, %fd1147;
	ld.const.f64 	%fd1149, [%rd173+16];
	fma.rn.f64 	%fd1150, %fd1148, %fd296, %fd1149;
	ld.const.f64 	%fd1151, [%rd173+24];
	fma.rn.f64 	%fd1152, %fd1150, %fd296, %fd1151;
	ld.const.f64 	%fd1153, [%rd173+32];
	fma.rn.f64 	%fd1154, %fd1152, %fd296, %fd1153;
	ld.const.f64 	%fd1155, [%rd173+40];
	fma.rn.f64 	%fd1156, %fd1154, %fd296, %fd1155;
	ld.const.f64 	%fd1157, [%rd173+48];
	fma.rn.f64 	%fd297, %fd1156, %fd296, %fd1157;
	fma.rn.f64 	%fd1828, %fd297, %fd1827, %fd1827;
	setp.eq.s32	%p111, %r292, 0;
	@%p111 bra 	BB0_170;

	mov.f64 	%fd1158, 0d3FF0000000000000;
	fma.rn.f64 	%fd1828, %fd297, %fd296, %fd1158;

BB0_170:
	and.b32  	%r294, %r455, 2;
	setp.eq.s32	%p112, %r294, 0;
	@%p112 bra 	BB0_172;

	mov.f64 	%fd1159, 0d0000000000000000;
	mov.f64 	%fd1160, 0dBFF0000000000000;
	fma.rn.f64 	%fd1828, %fd1828, %fd1160, %fd1159;

BB0_172:
	ld.global.f64 	%fd1829, [%rd2+40];
	abs.f64 	%fd1161, %fd1829;
	setp.neu.f64	%p113, %fd1161, 0d7FF0000000000000;
	@%p113 bra 	BB0_174;

	mov.f64 	%fd1162, 0d0000000000000000;
	mul.rn.f64 	%fd1829, %fd1829, %fd1162;

BB0_174:
	mul.f64 	%fd1163, %fd1829, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r456, %fd1163;
	st.local.u32 	[%rd46], %r456;
	cvt.rn.f64.s32	%fd1164, %r456;
	neg.f64 	%fd1165, %fd1164;
	fma.rn.f64 	%fd1167, %fd1165, %fd642, %fd1829;
	fma.rn.f64 	%fd1169, %fd1165, %fd644, %fd1167;
	fma.rn.f64 	%fd1830, %fd1165, %fd646, %fd1169;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r295}, %fd1829;
	}
	and.b32  	%r296, %r295, 2145386496;
	setp.lt.u32	%p114, %r296, 1105199104;
	@%p114 bra 	BB0_176;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1829;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1830, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r456, [%rd46];

BB0_176:
	and.b32  	%r297, %r456, 1;
	shl.b32 	%r298, %r297, 3;
	setp.eq.b32	%p115, %r297, 1;
	selp.f64	%fd1171, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p115;
	mul.wide.u32 	%rd178, %r298, 8;
	add.s64 	%rd180, %rd178, %rd50;
	ld.const.f64 	%fd1172, [%rd180+8];
	mul.rn.f64 	%fd309, %fd1830, %fd1830;
	fma.rn.f64 	%fd1173, %fd1171, %fd309, %fd1172;
	ld.const.f64 	%fd1174, [%rd180+16];
	fma.rn.f64 	%fd1175, %fd1173, %fd309, %fd1174;
	ld.const.f64 	%fd1176, [%rd180+24];
	fma.rn.f64 	%fd1177, %fd1175, %fd309, %fd1176;
	ld.const.f64 	%fd1178, [%rd180+32];
	fma.rn.f64 	%fd1179, %fd1177, %fd309, %fd1178;
	ld.const.f64 	%fd1180, [%rd180+40];
	fma.rn.f64 	%fd1181, %fd1179, %fd309, %fd1180;
	ld.const.f64 	%fd1182, [%rd180+48];
	fma.rn.f64 	%fd310, %fd1181, %fd309, %fd1182;
	fma.rn.f64 	%fd1831, %fd310, %fd1830, %fd1830;
	setp.eq.s32	%p116, %r297, 0;
	@%p116 bra 	BB0_178;

	mov.f64 	%fd1183, 0d3FF0000000000000;
	fma.rn.f64 	%fd1831, %fd310, %fd309, %fd1183;

BB0_178:
	and.b32  	%r299, %r456, 2;
	setp.eq.s32	%p117, %r299, 0;
	@%p117 bra 	BB0_180;

	mov.f64 	%fd1184, 0d0000000000000000;
	mov.f64 	%fd1185, 0dBFF0000000000000;
	fma.rn.f64 	%fd1831, %fd1831, %fd1185, %fd1184;

BB0_180:
	ld.global.f64 	%fd1186, [%rd2+24];
	add.f64 	%fd1187, %fd1186, %fd1186;
	mul.f64 	%fd1188, %fd1186, %fd1187;
	mul.f64 	%fd1189, %fd1828, %fd1831;
	div.rn.f64 	%fd316, %fd1189, %fd1188;
	ld.global.f64 	%fd1832, [%rd2+40];
	abs.f64 	%fd1190, %fd1832;
	setp.neu.f64	%p118, %fd1190, 0d7FF0000000000000;
	@%p118 bra 	BB0_182;

	mov.f64 	%fd1191, 0d0000000000000000;
	mul.rn.f64 	%fd1832, %fd1832, %fd1191;

BB0_182:
	mul.f64 	%fd1192, %fd1832, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r457, %fd1192;
	st.local.u32 	[%rd46], %r457;
	cvt.rn.f64.s32	%fd1193, %r457;
	neg.f64 	%fd1194, %fd1193;
	fma.rn.f64 	%fd1196, %fd1194, %fd642, %fd1832;
	fma.rn.f64 	%fd1198, %fd1194, %fd644, %fd1196;
	fma.rn.f64 	%fd1833, %fd1194, %fd646, %fd1198;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r300}, %fd1832;
	}
	and.b32  	%r301, %r300, 2145386496;
	setp.lt.u32	%p119, %r301, 1105199104;
	@%p119 bra 	BB0_184;

	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1832;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1833, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r457, [%rd46];

BB0_184:
	add.s32 	%r87, %r457, 1;
	and.b32  	%r302, %r87, 1;
	shl.b32 	%r303, %r302, 3;
	setp.eq.b32	%p120, %r302, 1;
	selp.f64	%fd1200, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p120;
	mul.wide.u32 	%rd185, %r303, 8;
	add.s64 	%rd187, %rd185, %rd50;
	ld.const.f64 	%fd1201, [%rd187+8];
	mul.rn.f64 	%fd323, %fd1833, %fd1833;
	fma.rn.f64 	%fd1202, %fd1200, %fd323, %fd1201;
	ld.const.f64 	%fd1203, [%rd187+16];
	fma.rn.f64 	%fd1204, %fd1202, %fd323, %fd1203;
	ld.const.f64 	%fd1205, [%rd187+24];
	fma.rn.f64 	%fd1206, %fd1204, %fd323, %fd1205;
	ld.const.f64 	%fd1207, [%rd187+32];
	fma.rn.f64 	%fd1208, %fd1206, %fd323, %fd1207;
	ld.const.f64 	%fd1209, [%rd187+40];
	fma.rn.f64 	%fd1210, %fd1208, %fd323, %fd1209;
	ld.const.f64 	%fd1211, [%rd187+48];
	fma.rn.f64 	%fd324, %fd1210, %fd323, %fd1211;
	fma.rn.f64 	%fd1834, %fd324, %fd1833, %fd1833;
	setp.eq.s32	%p121, %r302, 0;
	@%p121 bra 	BB0_186;

	mov.f64 	%fd1212, 0d3FF0000000000000;
	fma.rn.f64 	%fd1834, %fd324, %fd323, %fd1212;

BB0_186:
	and.b32  	%r304, %r87, 2;
	setp.eq.s32	%p122, %r304, 0;
	@%p122 bra 	BB0_188;

	mov.f64 	%fd1213, 0d0000000000000000;
	mov.f64 	%fd1214, 0dBFF0000000000000;
	fma.rn.f64 	%fd1834, %fd1834, %fd1214, %fd1213;

BB0_188:
	ld.global.f64 	%fd1835, [%rd2+40];
	abs.f64 	%fd1215, %fd1835;
	setp.neu.f64	%p123, %fd1215, 0d7FF0000000000000;
	@%p123 bra 	BB0_190;

	mov.f64 	%fd1216, 0d0000000000000000;
	mul.rn.f64 	%fd1835, %fd1835, %fd1216;

BB0_190:
	mul.f64 	%fd1217, %fd1835, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r458, %fd1217;
	st.local.u32 	[%rd46], %r458;
	cvt.rn.f64.s32	%fd1218, %r458;
	neg.f64 	%fd1219, %fd1218;
	fma.rn.f64 	%fd1221, %fd1219, %fd642, %fd1835;
	fma.rn.f64 	%fd1223, %fd1219, %fd644, %fd1221;
	fma.rn.f64 	%fd1836, %fd1219, %fd646, %fd1223;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r305}, %fd1835;
	}
	and.b32  	%r306, %r305, 2145386496;
	setp.lt.u32	%p124, %r306, 1105199104;
	@%p124 bra 	BB0_192;

	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1835;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd45;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1836, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r458, [%rd46];

BB0_192:
	add.s32 	%r91, %r458, 1;
	and.b32  	%r307, %r91, 1;
	shl.b32 	%r308, %r307, 3;
	setp.eq.b32	%p125, %r307, 1;
	selp.f64	%fd1225, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p125;
	mul.wide.u32 	%rd192, %r308, 8;
	add.s64 	%rd194, %rd192, %rd50;
	ld.const.f64 	%fd1226, [%rd194+8];
	mul.rn.f64 	%fd336, %fd1836, %fd1836;
	fma.rn.f64 	%fd1227, %fd1225, %fd336, %fd1226;
	ld.const.f64 	%fd1228, [%rd194+16];
	fma.rn.f64 	%fd1229, %fd1227, %fd336, %fd1228;
	ld.const.f64 	%fd1230, [%rd194+24];
	fma.rn.f64 	%fd1231, %fd1229, %fd336, %fd1230;
	ld.const.f64 	%fd1232, [%rd194+32];
	fma.rn.f64 	%fd1233, %fd1231, %fd336, %fd1232;
	ld.const.f64 	%fd1234, [%rd194+40];
	fma.rn.f64 	%fd1235, %fd1233, %fd336, %fd1234;
	ld.const.f64 	%fd1236, [%rd194+48];
	fma.rn.f64 	%fd337, %fd1235, %fd336, %fd1236;
	fma.rn.f64 	%fd1837, %fd337, %fd1836, %fd1836;
	setp.eq.s32	%p126, %r307, 0;
	@%p126 bra 	BB0_194;

	mov.f64 	%fd1237, 0d3FF0000000000000;
	fma.rn.f64 	%fd1837, %fd337, %fd336, %fd1237;

BB0_194:
	and.b32  	%r309, %r91, 2;
	setp.eq.s32	%p127, %r309, 0;
	@%p127 bra 	BB0_196;

	mov.f64 	%fd1238, 0d0000000000000000;
	mov.f64 	%fd1239, 0dBFF0000000000000;
	fma.rn.f64 	%fd1837, %fd1837, %fd1239, %fd1238;

BB0_196:
	ld.global.f64 	%fd1240, [%rd2+32];
	add.f64 	%fd1241, %fd1240, %fd1240;
	mul.f64 	%fd1242, %fd1240, %fd1241;
	mul.f64 	%fd1243, %fd1834, %fd1837;
	div.rn.f64 	%fd1244, %fd1243, %fd1242;
	add.f64 	%fd1728, %fd316, %fd1244;
	ld.global.f64 	%fd344, [%rd122];
	setp.lt.f64	%p128, %fd344, 0d0000000000000000;
	@%p128 bra 	BB0_198;
	bra.uni 	BB0_197;

BB0_198:
	setp.lt.s32	%p129, %r17, 20;
	@%p129 bra 	BB0_200;
	bra.uni 	BB0_199;

BB0_200:
	mul.f64 	%fd1247, %fd344, 0dBFD3333333333333;
	st.global.f64 	[%rd122], %fd1247;
	mov.f64 	%fd1737, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1737;
	bra.uni 	BB0_238;

BB0_24:
	setp.lt.f64	%p20, %fd56, 0d0000000000000000;
	@%p20 bra 	BB0_26;
	bra.uni 	BB0_25;

BB0_26:
	setp.lt.s32	%p21, %r17, 20;
	@%p21 bra 	BB0_28;
	bra.uni 	BB0_27;

BB0_28:
	mul.f64 	%fd636, %fd56, 0dBFD3333333333333;
	st.global.f64 	[%rd30], %fd636;
	mov.f64 	%fd1733, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1733;
	bra.uni 	BB0_238;

BB0_202:
	setp.lt.f64	%p133, %fd345, 0d0000000000000000;
	@%p133 bra 	BB0_204;
	bra.uni 	BB0_203;

BB0_204:
	setp.lt.s32	%p134, %r17, 20;
	@%p134 bra 	BB0_206;
	bra.uni 	BB0_205;

BB0_206:
	mul.f64 	%fd1251, %fd345, 0dBFD3333333333333;
	st.global.f64 	[%rd3+48], %fd1251;
	mov.f64 	%fd1740, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1740;
	bra.uni 	BB0_238;

BB0_221:
	neg.f64 	%fd1309, %fd361;
	st.global.f64 	[%rd3], %fd1309;
	bra.uni 	BB0_237;

BB0_25:
	neg.f64 	%fd634, %fd56;
	st.global.f64 	[%rd30], %fd634;
	mov.f64 	%fd1734, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1734;
	bra.uni 	BB0_238;

BB0_233:
	neg.f64 	%fd1365, %fd376;
	st.global.f64 	[%rd3], %fd1365;
	bra.uni 	BB0_237;

BB0_223:
	mul.f64 	%fd1310, %fd361, 0dBFE6666666666666;
	st.global.f64 	[%rd3], %fd1310;
	bra.uni 	BB0_237;

BB0_203:
	neg.f64 	%fd1249, %fd345;
	st.global.f64 	[%rd3+48], %fd1249;
	mov.f64 	%fd1738, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1738;
	bra.uni 	BB0_238;

BB0_27:
	mul.f64 	%fd635, %fd56, 0dBFE6666666666666;
	st.global.f64 	[%rd30], %fd635;
	mov.f64 	%fd1732, %fd54;
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd1732;
	bra.uni 	BB0_238;

BB0_235:
	mul.f64 	%fd1366, %fd376, 0dBFE6666666666666;
	st.global.f64 	[%rd3], %fd1366;

BB0_237:
	mov.f64 	%fd1775, %fd55;
	mov.f64 	%fd1773, %fd55;

BB0_238:
	mov.f64 	%fd1774, %fd1775;
	mov.f64 	%fd1772, %fd1773;
	add.s32 	%r436, %r435, 1;
	setp.lt.s32	%p156, %r435, 6;
	@%p156 bra 	BB0_241;

	mov.u32 	%r436, 0;
	setp.lt.s32	%p157, %r17, 250;
	@%p157 bra 	BB0_241;

	ld.param.f64 	%fd1697, [gaussFitter_param_10];
	sub.f64 	%fd1368, %fd1772, %fd1774;
	setp.lt.f64	%p158, %fd1368, %fd1697;
	selp.b16	%rs8, 0, %rs8, %p158;

BB0_241:
	mov.u32 	%r435, %r436;
	ld.param.u32 	%r425, [gaussFitter_param_11];
	add.s32 	%r437, %r17, 1;
	setp.ge.s32	%p159, %r17, %r425;
	selp.b16	%rs8, 0, %rs8, %p159;
	and.b16  	%rs7, %rs8, 255;
	setp.ne.s16	%p160, %rs7, 0;
	@%p160 bra 	BB0_20;

	ld.global.f64 	%fd1842, [%rd2+40];
	abs.f64 	%fd1369, %fd1842;
	setp.neu.f64	%p161, %fd1369, 0d7FF0000000000000;
	@%p161 bra 	BB0_244;

	mov.f64 	%fd1370, 0d0000000000000000;
	mul.rn.f64 	%fd1842, %fd1842, %fd1370;

BB0_244:
	add.u64 	%rd223, %SP, 0;
	cvta.to.local.u64 	%rd224, %rd223;
	mul.f64 	%fd1371, %fd1842, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r461, %fd1371;
	st.local.u32 	[%rd224], %r461;
	cvt.rn.f64.s32	%fd1372, %r461;
	neg.f64 	%fd1373, %fd1372;
	mov.f64 	%fd1374, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd1375, %fd1373, %fd1374, %fd1842;
	mov.f64 	%fd1376, 0d3C91A62633145C00;
	fma.rn.f64 	%fd1377, %fd1373, %fd1376, %fd1375;
	mov.f64 	%fd1378, 0d397B839A252049C0;
	fma.rn.f64 	%fd1843, %fd1373, %fd1378, %fd1377;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r354}, %fd1842;
	}
	and.b32  	%r355, %r354, 2145386496;
	setp.lt.u32	%p162, %r355, 1105199104;
	@%p162 bra 	BB0_246;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1842;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1843, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r461, [%rd224];

BB0_246:
	add.s32 	%r108, %r461, 1;
	and.b32  	%r356, %r108, 1;
	shl.b32 	%r357, %r356, 3;
	setp.eq.b32	%p163, %r356, 1;
	selp.f64	%fd1379, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p163;
	mul.wide.u32 	%rd227, %r357, 8;
	mov.u64 	%rd228, __cudart_sin_cos_coeffs;
	add.s64 	%rd229, %rd227, %rd228;
	ld.const.f64 	%fd1380, [%rd229+8];
	mul.rn.f64 	%fd388, %fd1843, %fd1843;
	fma.rn.f64 	%fd1381, %fd1379, %fd388, %fd1380;
	ld.const.f64 	%fd1382, [%rd229+16];
	fma.rn.f64 	%fd1383, %fd1381, %fd388, %fd1382;
	ld.const.f64 	%fd1384, [%rd229+24];
	fma.rn.f64 	%fd1385, %fd1383, %fd388, %fd1384;
	ld.const.f64 	%fd1386, [%rd229+32];
	fma.rn.f64 	%fd1387, %fd1385, %fd388, %fd1386;
	ld.const.f64 	%fd1388, [%rd229+40];
	fma.rn.f64 	%fd1389, %fd1387, %fd388, %fd1388;
	ld.const.f64 	%fd1390, [%rd229+48];
	fma.rn.f64 	%fd389, %fd1389, %fd388, %fd1390;
	fma.rn.f64 	%fd1844, %fd389, %fd1843, %fd1843;
	setp.eq.s32	%p164, %r356, 0;
	@%p164 bra 	BB0_248;

	mov.f64 	%fd1391, 0d3FF0000000000000;
	fma.rn.f64 	%fd1844, %fd389, %fd388, %fd1391;

BB0_248:
	and.b32  	%r358, %r108, 2;
	setp.eq.s32	%p165, %r358, 0;
	@%p165 bra 	BB0_250;

	mov.f64 	%fd1392, 0d0000000000000000;
	mov.f64 	%fd1393, 0dBFF0000000000000;
	fma.rn.f64 	%fd1844, %fd1844, %fd1393, %fd1392;

BB0_250:
	ld.global.f64 	%fd1845, [%rd2+40];
	abs.f64 	%fd1394, %fd1845;
	setp.neu.f64	%p166, %fd1394, 0d7FF0000000000000;
	@%p166 bra 	BB0_252;

	mov.f64 	%fd1395, 0d0000000000000000;
	mul.rn.f64 	%fd1845, %fd1845, %fd1395;

BB0_252:
	mul.f64 	%fd1396, %fd1845, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r462, %fd1396;
	st.local.u32 	[%rd224], %r462;
	cvt.rn.f64.s32	%fd1397, %r462;
	neg.f64 	%fd1398, %fd1397;
	fma.rn.f64 	%fd1400, %fd1398, %fd1374, %fd1845;
	fma.rn.f64 	%fd1402, %fd1398, %fd1376, %fd1400;
	fma.rn.f64 	%fd1846, %fd1398, %fd1378, %fd1402;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r359}, %fd1845;
	}
	and.b32  	%r360, %r359, 2145386496;
	setp.lt.u32	%p167, %r360, 1105199104;
	@%p167 bra 	BB0_254;

	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1845;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1846, [retval0+0];
	
	//{
	}// Callseq End 21
	ld.local.u32 	%r462, [%rd224];

BB0_254:
	add.s32 	%r112, %r462, 1;
	and.b32  	%r361, %r112, 1;
	shl.b32 	%r362, %r361, 3;
	setp.eq.b32	%p168, %r361, 1;
	selp.f64	%fd1404, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p168;
	mul.wide.u32 	%rd234, %r362, 8;
	add.s64 	%rd236, %rd234, %rd228;
	ld.const.f64 	%fd1405, [%rd236+8];
	mul.rn.f64 	%fd401, %fd1846, %fd1846;
	fma.rn.f64 	%fd1406, %fd1404, %fd401, %fd1405;
	ld.const.f64 	%fd1407, [%rd236+16];
	fma.rn.f64 	%fd1408, %fd1406, %fd401, %fd1407;
	ld.const.f64 	%fd1409, [%rd236+24];
	fma.rn.f64 	%fd1410, %fd1408, %fd401, %fd1409;
	ld.const.f64 	%fd1411, [%rd236+32];
	fma.rn.f64 	%fd1412, %fd1410, %fd401, %fd1411;
	ld.const.f64 	%fd1413, [%rd236+40];
	fma.rn.f64 	%fd1414, %fd1412, %fd401, %fd1413;
	ld.const.f64 	%fd1415, [%rd236+48];
	fma.rn.f64 	%fd402, %fd1414, %fd401, %fd1415;
	fma.rn.f64 	%fd1847, %fd402, %fd1846, %fd1846;
	setp.eq.s32	%p169, %r361, 0;
	@%p169 bra 	BB0_256;

	mov.f64 	%fd1416, 0d3FF0000000000000;
	fma.rn.f64 	%fd1847, %fd402, %fd401, %fd1416;

BB0_256:
	and.b32  	%r363, %r112, 2;
	setp.eq.s32	%p170, %r363, 0;
	@%p170 bra 	BB0_258;

	mov.f64 	%fd1417, 0d0000000000000000;
	mov.f64 	%fd1418, 0dBFF0000000000000;
	fma.rn.f64 	%fd1847, %fd1847, %fd1418, %fd1417;

BB0_258:
	ld.global.f64 	%fd408, [%rd2+24];
	ld.global.f64 	%fd1848, [%rd2+40];
	abs.f64 	%fd1419, %fd1848;
	setp.neu.f64	%p171, %fd1419, 0d7FF0000000000000;
	@%p171 bra 	BB0_260;

	mov.f64 	%fd1420, 0d0000000000000000;
	mul.rn.f64 	%fd1848, %fd1848, %fd1420;

BB0_260:
	add.f64 	%fd1421, %fd408, %fd408;
	mul.f64 	%fd1422, %fd408, %fd1421;
	mul.f64 	%fd1423, %fd1844, %fd1847;
	div.rn.f64 	%fd412, %fd1423, %fd1422;
	mul.f64 	%fd1424, %fd1848, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r463, %fd1424;
	st.local.u32 	[%rd224], %r463;
	cvt.rn.f64.s32	%fd1425, %r463;
	neg.f64 	%fd1426, %fd1425;
	fma.rn.f64 	%fd1428, %fd1426, %fd1374, %fd1848;
	fma.rn.f64 	%fd1430, %fd1426, %fd1376, %fd1428;
	fma.rn.f64 	%fd1849, %fd1426, %fd1378, %fd1430;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r364}, %fd1848;
	}
	and.b32  	%r365, %r364, 2145386496;
	setp.lt.u32	%p172, %r365, 1105199104;
	@%p172 bra 	BB0_262;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1848;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1849, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r463, [%rd224];

BB0_262:
	and.b32  	%r366, %r463, 1;
	shl.b32 	%r367, %r366, 3;
	setp.eq.b32	%p173, %r366, 1;
	selp.f64	%fd1432, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p173;
	mul.wide.u32 	%rd241, %r367, 8;
	add.s64 	%rd243, %rd241, %rd228;
	ld.const.f64 	%fd1433, [%rd243+8];
	mul.rn.f64 	%fd416, %fd1849, %fd1849;
	fma.rn.f64 	%fd1434, %fd1432, %fd416, %fd1433;
	ld.const.f64 	%fd1435, [%rd243+16];
	fma.rn.f64 	%fd1436, %fd1434, %fd416, %fd1435;
	ld.const.f64 	%fd1437, [%rd243+24];
	fma.rn.f64 	%fd1438, %fd1436, %fd416, %fd1437;
	ld.const.f64 	%fd1439, [%rd243+32];
	fma.rn.f64 	%fd1440, %fd1438, %fd416, %fd1439;
	ld.const.f64 	%fd1441, [%rd243+40];
	fma.rn.f64 	%fd1442, %fd1440, %fd416, %fd1441;
	ld.const.f64 	%fd1443, [%rd243+48];
	fma.rn.f64 	%fd417, %fd1442, %fd416, %fd1443;
	fma.rn.f64 	%fd1850, %fd417, %fd1849, %fd1849;
	setp.eq.s32	%p174, %r366, 0;
	@%p174 bra 	BB0_264;

	mov.f64 	%fd1444, 0d3FF0000000000000;
	fma.rn.f64 	%fd1850, %fd417, %fd416, %fd1444;

BB0_264:
	and.b32  	%r368, %r463, 2;
	setp.eq.s32	%p175, %r368, 0;
	@%p175 bra 	BB0_266;

	mov.f64 	%fd1445, 0d0000000000000000;
	mov.f64 	%fd1446, 0dBFF0000000000000;
	fma.rn.f64 	%fd1850, %fd1850, %fd1446, %fd1445;

BB0_266:
	ld.global.f64 	%fd1851, [%rd2+40];
	abs.f64 	%fd1447, %fd1851;
	setp.neu.f64	%p176, %fd1447, 0d7FF0000000000000;
	@%p176 bra 	BB0_268;

	mov.f64 	%fd1448, 0d0000000000000000;
	mul.rn.f64 	%fd1851, %fd1851, %fd1448;

BB0_268:
	mul.f64 	%fd1449, %fd1851, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r464, %fd1449;
	st.local.u32 	[%rd224], %r464;
	cvt.rn.f64.s32	%fd1450, %r464;
	neg.f64 	%fd1451, %fd1450;
	fma.rn.f64 	%fd1453, %fd1451, %fd1374, %fd1851;
	fma.rn.f64 	%fd1455, %fd1451, %fd1376, %fd1453;
	fma.rn.f64 	%fd1852, %fd1451, %fd1378, %fd1455;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r369}, %fd1851;
	}
	and.b32  	%r370, %r369, 2145386496;
	setp.lt.u32	%p177, %r370, 1105199104;
	@%p177 bra 	BB0_270;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1851;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1852, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r464, [%rd224];

BB0_270:
	and.b32  	%r371, %r464, 1;
	shl.b32 	%r372, %r371, 3;
	setp.eq.b32	%p178, %r371, 1;
	selp.f64	%fd1457, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p178;
	mul.wide.u32 	%rd248, %r372, 8;
	add.s64 	%rd250, %rd248, %rd228;
	ld.const.f64 	%fd1458, [%rd250+8];
	mul.rn.f64 	%fd429, %fd1852, %fd1852;
	fma.rn.f64 	%fd1459, %fd1457, %fd429, %fd1458;
	ld.const.f64 	%fd1460, [%rd250+16];
	fma.rn.f64 	%fd1461, %fd1459, %fd429, %fd1460;
	ld.const.f64 	%fd1462, [%rd250+24];
	fma.rn.f64 	%fd1463, %fd1461, %fd429, %fd1462;
	ld.const.f64 	%fd1464, [%rd250+32];
	fma.rn.f64 	%fd1465, %fd1463, %fd429, %fd1464;
	ld.const.f64 	%fd1466, [%rd250+40];
	fma.rn.f64 	%fd1467, %fd1465, %fd429, %fd1466;
	ld.const.f64 	%fd1468, [%rd250+48];
	fma.rn.f64 	%fd430, %fd1467, %fd429, %fd1468;
	fma.rn.f64 	%fd1853, %fd430, %fd1852, %fd1852;
	setp.eq.s32	%p179, %r371, 0;
	@%p179 bra 	BB0_272;

	mov.f64 	%fd1469, 0d3FF0000000000000;
	fma.rn.f64 	%fd1853, %fd430, %fd429, %fd1469;

BB0_272:
	and.b32  	%r373, %r464, 2;
	setp.eq.s32	%p180, %r373, 0;
	@%p180 bra 	BB0_274;

	mov.f64 	%fd1470, 0d0000000000000000;
	mov.f64 	%fd1471, 0dBFF0000000000000;
	fma.rn.f64 	%fd1853, %fd1853, %fd1471, %fd1470;

BB0_274:
	ld.global.f64 	%fd436, [%rd2+32];
	ld.global.f64 	%fd1472, [%rd2+40];
	add.f64 	%fd1854, %fd1472, %fd1472;
	abs.f64 	%fd1473, %fd1854;
	setp.neu.f64	%p181, %fd1473, 0d7FF0000000000000;
	@%p181 bra 	BB0_276;

	mov.f64 	%fd1474, 0d0000000000000000;
	mul.rn.f64 	%fd1854, %fd1854, %fd1474;

BB0_276:
	add.f64 	%fd1475, %fd436, %fd436;
	mul.f64 	%fd1476, %fd436, %fd1475;
	mul.f64 	%fd1477, %fd1850, %fd1853;
	div.rn.f64 	%fd1478, %fd1477, %fd1476;
	add.f64 	%fd440, %fd412, %fd1478;
	mul.f64 	%fd1479, %fd1854, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r465, %fd1479;
	st.local.u32 	[%rd224], %r465;
	cvt.rn.f64.s32	%fd1480, %r465;
	neg.f64 	%fd1481, %fd1480;
	fma.rn.f64 	%fd1483, %fd1481, %fd1374, %fd1854;
	fma.rn.f64 	%fd1485, %fd1481, %fd1376, %fd1483;
	fma.rn.f64 	%fd1855, %fd1481, %fd1378, %fd1485;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r374}, %fd1854;
	}
	and.b32  	%r375, %r374, 2145386496;
	setp.lt.u32	%p182, %r375, 1105199104;
	@%p182 bra 	BB0_278;

	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1854;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1855, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r465, [%rd224];

BB0_278:
	and.b32  	%r376, %r465, 1;
	shl.b32 	%r377, %r376, 3;
	setp.eq.b32	%p183, %r376, 1;
	selp.f64	%fd1487, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p183;
	mul.wide.u32 	%rd255, %r377, 8;
	add.s64 	%rd257, %rd255, %rd228;
	ld.const.f64 	%fd1488, [%rd257+8];
	mul.rn.f64 	%fd444, %fd1855, %fd1855;
	fma.rn.f64 	%fd1489, %fd1487, %fd444, %fd1488;
	ld.const.f64 	%fd1490, [%rd257+16];
	fma.rn.f64 	%fd1491, %fd1489, %fd444, %fd1490;
	ld.const.f64 	%fd1492, [%rd257+24];
	fma.rn.f64 	%fd1493, %fd1491, %fd444, %fd1492;
	ld.const.f64 	%fd1494, [%rd257+32];
	fma.rn.f64 	%fd1495, %fd1493, %fd444, %fd1494;
	ld.const.f64 	%fd1496, [%rd257+40];
	fma.rn.f64 	%fd1497, %fd1495, %fd444, %fd1496;
	ld.const.f64 	%fd1498, [%rd257+48];
	fma.rn.f64 	%fd445, %fd1497, %fd444, %fd1498;
	fma.rn.f64 	%fd1856, %fd445, %fd1855, %fd1855;
	setp.eq.s32	%p184, %r376, 0;
	@%p184 bra 	BB0_280;

	mov.f64 	%fd1499, 0d3FF0000000000000;
	fma.rn.f64 	%fd1856, %fd445, %fd444, %fd1499;

BB0_280:
	and.b32  	%r378, %r465, 2;
	setp.eq.s32	%p185, %r378, 0;
	@%p185 bra 	BB0_282;

	mov.f64 	%fd1500, 0d0000000000000000;
	mov.f64 	%fd1501, 0dBFF0000000000000;
	fma.rn.f64 	%fd1856, %fd1856, %fd1501, %fd1500;

BB0_282:
	ld.global.f64 	%fd451, [%rd2+24];
	ld.global.f64 	%fd1502, [%rd2+40];
	add.f64 	%fd1857, %fd1502, %fd1502;
	abs.f64 	%fd1503, %fd1857;
	setp.neu.f64	%p186, %fd1503, 0d7FF0000000000000;
	@%p186 bra 	BB0_284;

	mov.f64 	%fd1504, 0d0000000000000000;
	mul.rn.f64 	%fd1857, %fd1857, %fd1504;

BB0_284:
	mul.f64 	%fd1505, %fd451, 0dC010000000000000;
	mul.f64 	%fd1506, %fd451, %fd1505;
	div.rn.f64 	%fd455, %fd1856, %fd1506;
	mul.f64 	%fd1507, %fd1857, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r466, %fd1507;
	st.local.u32 	[%rd224], %r466;
	cvt.rn.f64.s32	%fd1508, %r466;
	neg.f64 	%fd1509, %fd1508;
	fma.rn.f64 	%fd1511, %fd1509, %fd1374, %fd1857;
	fma.rn.f64 	%fd1513, %fd1509, %fd1376, %fd1511;
	fma.rn.f64 	%fd1858, %fd1509, %fd1378, %fd1513;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r379}, %fd1857;
	}
	and.b32  	%r380, %r379, 2145386496;
	setp.lt.u32	%p187, %r380, 1105199104;
	@%p187 bra 	BB0_286;

	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1857;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1858, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r466, [%rd224];

BB0_286:
	and.b32  	%r381, %r466, 1;
	shl.b32 	%r382, %r381, 3;
	setp.eq.b32	%p188, %r381, 1;
	selp.f64	%fd1515, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p188;
	mul.wide.u32 	%rd262, %r382, 8;
	add.s64 	%rd264, %rd262, %rd228;
	ld.const.f64 	%fd1516, [%rd264+8];
	mul.rn.f64 	%fd459, %fd1858, %fd1858;
	fma.rn.f64 	%fd1517, %fd1515, %fd459, %fd1516;
	ld.const.f64 	%fd1518, [%rd264+16];
	fma.rn.f64 	%fd1519, %fd1517, %fd459, %fd1518;
	ld.const.f64 	%fd1520, [%rd264+24];
	fma.rn.f64 	%fd1521, %fd1519, %fd459, %fd1520;
	ld.const.f64 	%fd1522, [%rd264+32];
	fma.rn.f64 	%fd1523, %fd1521, %fd459, %fd1522;
	ld.const.f64 	%fd1524, [%rd264+40];
	fma.rn.f64 	%fd1525, %fd1523, %fd459, %fd1524;
	ld.const.f64 	%fd1526, [%rd264+48];
	fma.rn.f64 	%fd460, %fd1525, %fd459, %fd1526;
	fma.rn.f64 	%fd1859, %fd460, %fd1858, %fd1858;
	setp.eq.s32	%p189, %r381, 0;
	@%p189 bra 	BB0_288;

	mov.f64 	%fd1527, 0d3FF0000000000000;
	fma.rn.f64 	%fd1859, %fd460, %fd459, %fd1527;

BB0_288:
	and.b32  	%r383, %r466, 2;
	setp.eq.s32	%p190, %r383, 0;
	@%p190 bra 	BB0_290;

	mov.f64 	%fd1528, 0d0000000000000000;
	mov.f64 	%fd1529, 0dBFF0000000000000;
	fma.rn.f64 	%fd1859, %fd1859, %fd1529, %fd1528;

BB0_290:
	ld.global.f64 	%fd466, [%rd2+32];
	ld.global.f64 	%fd1860, [%rd2+40];
	abs.f64 	%fd1530, %fd1860;
	setp.neu.f64	%p191, %fd1530, 0d7FF0000000000000;
	@%p191 bra 	BB0_292;

	mov.f64 	%fd1531, 0d0000000000000000;
	mul.rn.f64 	%fd1860, %fd1860, %fd1531;

BB0_292:
	mul.f64 	%fd1532, %fd466, 0d4010000000000000;
	mul.f64 	%fd1533, %fd466, %fd1532;
	div.rn.f64 	%fd1534, %fd1859, %fd1533;
	add.f64 	%fd470, %fd455, %fd1534;
	mul.f64 	%fd1535, %fd1860, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r467, %fd1535;
	st.local.u32 	[%rd224], %r467;
	cvt.rn.f64.s32	%fd1536, %r467;
	neg.f64 	%fd1537, %fd1536;
	fma.rn.f64 	%fd1539, %fd1537, %fd1374, %fd1860;
	fma.rn.f64 	%fd1541, %fd1537, %fd1376, %fd1539;
	fma.rn.f64 	%fd1861, %fd1537, %fd1378, %fd1541;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r384}, %fd1860;
	}
	and.b32  	%r385, %r384, 2145386496;
	setp.lt.u32	%p192, %r385, 1105199104;
	@%p192 bra 	BB0_294;

	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1860;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1861, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r467, [%rd224];

BB0_294:
	and.b32  	%r386, %r467, 1;
	shl.b32 	%r387, %r386, 3;
	setp.eq.b32	%p193, %r386, 1;
	selp.f64	%fd1543, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p193;
	mul.wide.u32 	%rd269, %r387, 8;
	add.s64 	%rd271, %rd269, %rd228;
	ld.const.f64 	%fd1544, [%rd271+8];
	mul.rn.f64 	%fd474, %fd1861, %fd1861;
	fma.rn.f64 	%fd1545, %fd1543, %fd474, %fd1544;
	ld.const.f64 	%fd1546, [%rd271+16];
	fma.rn.f64 	%fd1547, %fd1545, %fd474, %fd1546;
	ld.const.f64 	%fd1548, [%rd271+24];
	fma.rn.f64 	%fd1549, %fd1547, %fd474, %fd1548;
	ld.const.f64 	%fd1550, [%rd271+32];
	fma.rn.f64 	%fd1551, %fd1549, %fd474, %fd1550;
	ld.const.f64 	%fd1552, [%rd271+40];
	fma.rn.f64 	%fd1553, %fd1551, %fd474, %fd1552;
	ld.const.f64 	%fd1554, [%rd271+48];
	fma.rn.f64 	%fd475, %fd1553, %fd474, %fd1554;
	fma.rn.f64 	%fd1862, %fd475, %fd1861, %fd1861;
	setp.eq.s32	%p194, %r386, 0;
	@%p194 bra 	BB0_296;

	mov.f64 	%fd1555, 0d3FF0000000000000;
	fma.rn.f64 	%fd1862, %fd475, %fd474, %fd1555;

BB0_296:
	and.b32  	%r388, %r467, 2;
	setp.eq.s32	%p195, %r388, 0;
	@%p195 bra 	BB0_298;

	mov.f64 	%fd1556, 0d0000000000000000;
	mov.f64 	%fd1557, 0dBFF0000000000000;
	fma.rn.f64 	%fd1862, %fd1862, %fd1557, %fd1556;

BB0_298:
	ld.global.f64 	%fd1863, [%rd2+40];
	abs.f64 	%fd1558, %fd1863;
	setp.neu.f64	%p196, %fd1558, 0d7FF0000000000000;
	@%p196 bra 	BB0_300;

	mov.f64 	%fd1559, 0d0000000000000000;
	mul.rn.f64 	%fd1863, %fd1863, %fd1559;

BB0_300:
	mul.f64 	%fd1560, %fd1863, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r468, %fd1560;
	st.local.u32 	[%rd224], %r468;
	cvt.rn.f64.s32	%fd1561, %r468;
	neg.f64 	%fd1562, %fd1561;
	fma.rn.f64 	%fd1564, %fd1562, %fd1374, %fd1863;
	fma.rn.f64 	%fd1566, %fd1562, %fd1376, %fd1564;
	fma.rn.f64 	%fd1864, %fd1562, %fd1378, %fd1566;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r389}, %fd1863;
	}
	and.b32  	%r390, %r389, 2145386496;
	setp.lt.u32	%p197, %r390, 1105199104;
	@%p197 bra 	BB0_302;

	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1863;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1864, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r468, [%rd224];

BB0_302:
	and.b32  	%r391, %r468, 1;
	shl.b32 	%r392, %r391, 3;
	setp.eq.b32	%p198, %r391, 1;
	selp.f64	%fd1568, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p198;
	mul.wide.u32 	%rd276, %r392, 8;
	add.s64 	%rd278, %rd276, %rd228;
	ld.const.f64 	%fd1569, [%rd278+8];
	mul.rn.f64 	%fd487, %fd1864, %fd1864;
	fma.rn.f64 	%fd1570, %fd1568, %fd487, %fd1569;
	ld.const.f64 	%fd1571, [%rd278+16];
	fma.rn.f64 	%fd1572, %fd1570, %fd487, %fd1571;
	ld.const.f64 	%fd1573, [%rd278+24];
	fma.rn.f64 	%fd1574, %fd1572, %fd487, %fd1573;
	ld.const.f64 	%fd1575, [%rd278+32];
	fma.rn.f64 	%fd1576, %fd1574, %fd487, %fd1575;
	ld.const.f64 	%fd1577, [%rd278+40];
	fma.rn.f64 	%fd1578, %fd1576, %fd487, %fd1577;
	ld.const.f64 	%fd1579, [%rd278+48];
	fma.rn.f64 	%fd488, %fd1578, %fd487, %fd1579;
	fma.rn.f64 	%fd1865, %fd488, %fd1864, %fd1864;
	setp.eq.s32	%p199, %r391, 0;
	@%p199 bra 	BB0_304;

	mov.f64 	%fd1580, 0d3FF0000000000000;
	fma.rn.f64 	%fd1865, %fd488, %fd487, %fd1580;

BB0_304:
	and.b32  	%r393, %r468, 2;
	setp.eq.s32	%p200, %r393, 0;
	@%p200 bra 	BB0_306;

	mov.f64 	%fd1581, 0d0000000000000000;
	mov.f64 	%fd1582, 0dBFF0000000000000;
	fma.rn.f64 	%fd1865, %fd1865, %fd1582, %fd1581;

BB0_306:
	ld.global.f64 	%fd494, [%rd2+24];
	ld.global.f64 	%fd1866, [%rd2+40];
	abs.f64 	%fd1583, %fd1866;
	setp.neu.f64	%p201, %fd1583, 0d7FF0000000000000;
	@%p201 bra 	BB0_308;

	mov.f64 	%fd1584, 0d0000000000000000;
	mul.rn.f64 	%fd1866, %fd1866, %fd1584;

BB0_308:
	add.f64 	%fd1585, %fd494, %fd494;
	mul.f64 	%fd1586, %fd494, %fd1585;
	mul.f64 	%fd1587, %fd1862, %fd1865;
	div.rn.f64 	%fd498, %fd1587, %fd1586;
	mul.f64 	%fd1588, %fd1866, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r469, %fd1588;
	st.local.u32 	[%rd224], %r469;
	cvt.rn.f64.s32	%fd1589, %r469;
	neg.f64 	%fd1590, %fd1589;
	fma.rn.f64 	%fd1592, %fd1590, %fd1374, %fd1866;
	fma.rn.f64 	%fd1594, %fd1590, %fd1376, %fd1592;
	fma.rn.f64 	%fd1867, %fd1590, %fd1378, %fd1594;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r394}, %fd1866;
	}
	and.b32  	%r395, %r394, 2145386496;
	setp.lt.u32	%p202, %r395, 1105199104;
	@%p202 bra 	BB0_310;

	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1866;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1867, [retval0+0];
	
	//{
	}// Callseq End 28
	ld.local.u32 	%r469, [%rd224];

BB0_310:
	add.s32 	%r134, %r469, 1;
	and.b32  	%r396, %r134, 1;
	shl.b32 	%r397, %r396, 3;
	setp.eq.b32	%p203, %r396, 1;
	selp.f64	%fd1596, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p203;
	mul.wide.u32 	%rd283, %r397, 8;
	add.s64 	%rd285, %rd283, %rd228;
	ld.const.f64 	%fd1597, [%rd285+8];
	mul.rn.f64 	%fd502, %fd1867, %fd1867;
	fma.rn.f64 	%fd1598, %fd1596, %fd502, %fd1597;
	ld.const.f64 	%fd1599, [%rd285+16];
	fma.rn.f64 	%fd1600, %fd1598, %fd502, %fd1599;
	ld.const.f64 	%fd1601, [%rd285+24];
	fma.rn.f64 	%fd1602, %fd1600, %fd502, %fd1601;
	ld.const.f64 	%fd1603, [%rd285+32];
	fma.rn.f64 	%fd1604, %fd1602, %fd502, %fd1603;
	ld.const.f64 	%fd1605, [%rd285+40];
	fma.rn.f64 	%fd1606, %fd1604, %fd502, %fd1605;
	ld.const.f64 	%fd1607, [%rd285+48];
	fma.rn.f64 	%fd503, %fd1606, %fd502, %fd1607;
	fma.rn.f64 	%fd1868, %fd503, %fd1867, %fd1867;
	setp.eq.s32	%p204, %r396, 0;
	@%p204 bra 	BB0_312;

	mov.f64 	%fd1608, 0d3FF0000000000000;
	fma.rn.f64 	%fd1868, %fd503, %fd502, %fd1608;

BB0_312:
	and.b32  	%r398, %r134, 2;
	setp.eq.s32	%p205, %r398, 0;
	@%p205 bra 	BB0_314;

	mov.f64 	%fd1609, 0d0000000000000000;
	mov.f64 	%fd1610, 0dBFF0000000000000;
	fma.rn.f64 	%fd1868, %fd1868, %fd1610, %fd1609;

BB0_314:
	ld.global.f64 	%fd1869, [%rd2+40];
	abs.f64 	%fd1611, %fd1869;
	setp.neu.f64	%p206, %fd1611, 0d7FF0000000000000;
	@%p206 bra 	BB0_316;

	mov.f64 	%fd1612, 0d0000000000000000;
	mul.rn.f64 	%fd1869, %fd1869, %fd1612;

BB0_316:
	mul.f64 	%fd1613, %fd1869, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r470, %fd1613;
	st.local.u32 	[%rd224], %r470;
	cvt.rn.f64.s32	%fd1614, %r470;
	neg.f64 	%fd1615, %fd1614;
	fma.rn.f64 	%fd1617, %fd1615, %fd1374, %fd1869;
	fma.rn.f64 	%fd1619, %fd1615, %fd1376, %fd1617;
	fma.rn.f64 	%fd1870, %fd1615, %fd1378, %fd1619;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd1869;
	}
	and.b32  	%r400, %r399, 2145386496;
	setp.lt.u32	%p207, %r400, 1105199104;
	@%p207 bra 	BB0_318;

	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1869;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1870, [retval0+0];
	
	//{
	}// Callseq End 29
	ld.local.u32 	%r470, [%rd224];

BB0_318:
	add.s32 	%r138, %r470, 1;
	and.b32  	%r401, %r138, 1;
	shl.b32 	%r402, %r401, 3;
	setp.eq.b32	%p208, %r401, 1;
	selp.f64	%fd1621, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p208;
	mul.wide.u32 	%rd290, %r402, 8;
	add.s64 	%rd292, %rd290, %rd228;
	ld.const.f64 	%fd1622, [%rd292+8];
	mul.rn.f64 	%fd515, %fd1870, %fd1870;
	fma.rn.f64 	%fd1623, %fd1621, %fd515, %fd1622;
	ld.const.f64 	%fd1624, [%rd292+16];
	fma.rn.f64 	%fd1625, %fd1623, %fd515, %fd1624;
	ld.const.f64 	%fd1626, [%rd292+24];
	fma.rn.f64 	%fd1627, %fd1625, %fd515, %fd1626;
	ld.const.f64 	%fd1628, [%rd292+32];
	fma.rn.f64 	%fd1629, %fd1627, %fd515, %fd1628;
	ld.const.f64 	%fd1630, [%rd292+40];
	fma.rn.f64 	%fd1631, %fd1629, %fd515, %fd1630;
	ld.const.f64 	%fd1632, [%rd292+48];
	fma.rn.f64 	%fd516, %fd1631, %fd515, %fd1632;
	fma.rn.f64 	%fd1871, %fd516, %fd1870, %fd1870;
	setp.eq.s32	%p209, %r401, 0;
	@%p209 bra 	BB0_320;

	mov.f64 	%fd1633, 0d3FF0000000000000;
	fma.rn.f64 	%fd1871, %fd516, %fd515, %fd1633;

BB0_320:
	and.b32  	%r403, %r138, 2;
	setp.eq.s32	%p210, %r403, 0;
	@%p210 bra 	BB0_322;

	mov.f64 	%fd1634, 0d0000000000000000;
	mov.f64 	%fd1635, 0dBFF0000000000000;
	fma.rn.f64 	%fd1871, %fd1871, %fd1635, %fd1634;

BB0_322:
	mov.u32 	%r427, %ntid.x;
	mov.u32 	%r405, %nctaid.x;
	mul.lo.s32 	%r139, %r405, %r427;
	ld.global.f64 	%fd1638, [%rd2+32];
	add.f64 	%fd1639, %fd1638, %fd1638;
	mul.f64 	%fd1640, %fd1638, %fd1639;
	mul.f64 	%fd1641, %fd1868, %fd1871;
	div.rn.f64 	%fd1642, %fd1641, %fd1640;
	add.f64 	%fd522, %fd498, %fd1642;
	mov.f64 	%fd1874, 0d0000000000000000;
	mov.f64 	%fd1873, %fd1874;
	@%p2 bra 	BB0_328;

	ld.global.f64 	%fd523, [%rd2];
	ld.global.f64 	%fd524, [%rd2+8];
	add.f64 	%fd525, %fd470, %fd470;
	ld.global.f64 	%fd526, [%rd2+16];
	ld.global.f64 	%fd527, [%rd2+48];
	mov.f64 	%fd1874, 0d0000000000000000;
	mov.u32 	%r471, 0;
	mov.f64 	%fd1873, %fd1874;

BB0_324:
	rem.s32 	%r409, %r471, %r1;
	cvt.rn.f64.s32	%fd1645, %r409;
	sub.f64 	%fd1646, %fd1645, %fd524;
	mul.f64 	%fd1647, %fd440, %fd1646;
	mul.f64 	%fd1648, %fd1646, %fd1647;
	mul.f64 	%fd1649, %fd525, %fd1646;
	div.s32 	%r410, %r471, %r1;
	cvt.rn.f64.s32	%fd1650, %r410;
	sub.f64 	%fd1651, %fd1650, %fd526;
	mul.f64 	%fd1652, %fd1649, %fd1651;
	sub.f64 	%fd1653, %fd1648, %fd1652;
	mul.f64 	%fd1654, %fd522, %fd1651;
	fma.rn.f64 	%fd530, %fd1651, %fd1654, %fd1653;
	neg.f64 	%fd1655, %fd530;
	mov.f64 	%fd1656, 0d4338000000000000;
	mov.f64 	%fd1657, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1658, %fd1655, %fd1657, %fd1656;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r141, %temp}, %fd1658;
	}
	mov.f64 	%fd1659, 0dC338000000000000;
	add.rn.f64 	%fd1660, %fd1658, %fd1659;
	mov.f64 	%fd1661, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1662, %fd1660, %fd1661, %fd1655;
	mov.f64 	%fd1663, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1664, %fd1660, %fd1663, %fd1662;
	mov.f64 	%fd1665, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1666, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1667, %fd1666, %fd1664, %fd1665;
	mov.f64 	%fd1668, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1669, %fd1667, %fd1664, %fd1668;
	mov.f64 	%fd1670, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1671, %fd1669, %fd1664, %fd1670;
	mov.f64 	%fd1672, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1673, %fd1671, %fd1664, %fd1672;
	mov.f64 	%fd1674, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1675, %fd1673, %fd1664, %fd1674;
	mov.f64 	%fd1676, 0d3F81111111122322;
	fma.rn.f64 	%fd1677, %fd1675, %fd1664, %fd1676;
	mov.f64 	%fd1678, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1679, %fd1677, %fd1664, %fd1678;
	mov.f64 	%fd1680, 0d3FC5555555555511;
	fma.rn.f64 	%fd1681, %fd1679, %fd1664, %fd1680;
	mov.f64 	%fd1682, 0d3FE000000000000B;
	fma.rn.f64 	%fd1683, %fd1681, %fd1664, %fd1682;
	mov.f64 	%fd1684, 0d3FF0000000000000;
	fma.rn.f64 	%fd1685, %fd1683, %fd1664, %fd1684;
	fma.rn.f64 	%fd1686, %fd1685, %fd1664, %fd1684;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r142, %temp}, %fd1686;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r143}, %fd1686;
	}
	shl.b32 	%r411, %r141, 20;
	add.s32 	%r412, %r143, %r411;
	mov.b64 	%fd1872, {%r142, %r412};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r413}, %fd1655;
	}
	mov.b32 	 %f10, %r413;
	abs.f32 	%f5, %f10;
	setp.lt.f32	%p212, %f5, 0f4086232B;
	@%p212 bra 	BB0_327;

	setp.gt.f64	%p213, %fd530, 0d8000000000000000;
	mov.f64 	%fd1687, 0d7FF0000000000000;
	sub.f64 	%fd1688, %fd1687, %fd530;
	selp.f64	%fd1872, 0d0000000000000000, %fd1688, %p213;
	setp.geu.f32	%p214, %f5, 0f40874800;
	@%p214 bra 	BB0_327;

	shr.u32 	%r414, %r141, 31;
	add.s32 	%r415, %r141, %r414;
	shr.s32 	%r416, %r415, 1;
	shl.b32 	%r417, %r416, 20;
	add.s32 	%r418, %r417, %r143;
	mov.b64 	%fd1689, {%r142, %r418};
	sub.s32 	%r419, %r141, %r416;
	shl.b32 	%r420, %r419, 20;
	add.s32 	%r421, %r420, 1072693248;
	mov.u32 	%r422, 0;
	mov.b64 	%fd1690, {%r422, %r421};
	mul.f64 	%fd1872, %fd1689, %fd1690;

BB0_327:
	fma.rn.f64 	%fd1691, %fd523, %fd1872, %fd527;
	add.f64 	%fd1873, %fd1873, %fd1691;
	add.s32 	%r423, %r471, %r5;
	mul.wide.s32 	%rd298, %r423, 4;
	add.s64 	%rd299, %rd1, %rd298;
	ld.global.u32 	%r424, [%rd299];
	cvt.rn.f64.s32	%fd1692, %r424;
	sub.f64 	%fd1693, %fd1691, %fd1692;
	fma.rn.f64 	%fd1874, %fd1693, %fd1693, %fd1874;
	add.s32 	%r471, %r471, 1;
	setp.lt.s32	%p215, %r471, %r2;
	@%p215 bra 	BB0_324;

BB0_328:
	ld.param.u32 	%r428, [gaussFitter_param_0];
	st.global.f64 	[%rd2], %fd1873;
	div.rn.f64 	%fd1694, %fd1874, %fd1711;
	mov.f64 	%fd1695, 0d3FF0000000000000;
	sub.f64 	%fd1696, %fd1695, %fd1694;
	st.global.f64 	[%rd2+48], %fd1696;
	add.s32 	%r429, %r429, %r139;
	setp.lt.s32	%p216, %r429, %r428;
	@%p216 bra 	BB0_2;

BB0_329:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot1;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB1_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB1_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd92, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	neg.s32 	%r25, %r24;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd90, %rd45, 120;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r7;

BB1_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r39;
	mov.u64 	%rd7, %rd91;
	ld.const.u64 	%rd48, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd48;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd46, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd92], %rd46;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r26, %r9, %r7;
	mul.wide.s32 	%rd51, %r26, 8;
	add.s64 	%rd92, %rd1, %rd51;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd93, %rd13;
	add.s64 	%rd90, %rd90, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd91, %rd13;
	mov.u32 	%r39, %r9;
	@%p3 bra 	BB1_3;

BB1_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB1_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r10;
	shl.b64 	%rd52, %rd96, %r10;
	shr.u64 	%rd53, %rd95, %r28;
	or.b64  	%rd96, %rd52, %rd53;
	shl.b64 	%rd54, %rd95, %r10;
	ld.local.u64 	%rd55, [%rd1+8];
	shr.u64 	%rd56, %rd55, %r28;
	or.b64  	%rd95, %rd56, %rd54;

BB1_6:
	cvta.to.local.u64 	%rd57, %rd37;
	shr.u64 	%rd58, %rd96, 62;
	cvt.u32.u64	%r29, %rd58;
	shr.u64 	%rd59, %rd95, 62;
	shl.b64 	%rd60, %rd96, 2;
	or.b64  	%rd98, %rd60, %rd59;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd61, %rd96, 61;
	cvt.u32.u64	%r30, %rd61;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.ne.s32	%p5, %r40, 0;
	selp.b32	%r34, %r33, %r32, %p5;
	st.local.u32 	[%rd57], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB1_8;

	mov.u64 	%rd65, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd65;
	mov.b64         {a2,a3}, %rd65;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB1_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB1_10;

	shl.b64 	%rd68, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd69, %rd97, %r36;
	or.b64  	%rd98, %rd69, %rd68;

BB1_10:
	mov.u64 	%rd73, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd73;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd70, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB1_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd70;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd70;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd74, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB1_12:
	cvt.u64.u32	%rd80, %r40;
	shl.b64 	%rd81, %rd80, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd82, %r38;
	shl.b64 	%rd83, %rd82, 52;
	add.s64 	%rd84, %rd99, 1;
	shr.u64 	%rd85, %rd84, 10;
	add.s64 	%rd86, %rd85, 1;
	shr.u64 	%rd87, %rd86, 1;
	add.s64 	%rd88, %rd87, %rd83;
	or.b64  	%rd89, %rd88, %rd81;
	mov.b64 	 %fd4, %rd89;

BB1_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


