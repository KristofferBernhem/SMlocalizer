//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-20732876
// Cuda compilation tools, release 8.0, V8.0.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	gaussFitter
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry gaussFitter(
	.param .u64 gaussFitter_param_0,
	.param .u32 gaussFitter_param_1,
	.param .u64 gaussFitter_param_2,
	.param .u32 gaussFitter_param_3,
	.param .u16 gaussFitter_param_4,
	.param .u64 gaussFitter_param_5,
	.param .u32 gaussFitter_param_6,
	.param .u64 gaussFitter_param_7,
	.param .u32 gaussFitter_param_8,
	.param .f64 gaussFitter_param_9,
	.param .u32 gaussFitter_param_10
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<210>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<11>;
	.reg .b32 	%r<513>;
	.reg .f64 	%fd<1855>;
	.reg .b64 	%rd<282>;


	mov.u64 	%rd281, __local_depot0;
	cvta.local.u64 	%SP, %rd281;
	ld.param.u64 	%rd11, [gaussFitter_param_0];
	ld.param.u32 	%r149, [gaussFitter_param_1];
	ld.param.u64 	%rd12, [gaussFitter_param_2];
	ld.param.u16 	%rs5, [gaussFitter_param_4];
	ld.param.u64 	%rd13, [gaussFitter_param_5];
	ld.param.u64 	%rd14, [gaussFitter_param_7];
	cvta.to.global.u64 	%rd1, %rd11;
	mov.u32 	%r150, %nctaid.x;
	mov.u32 	%r151, %ctaid.y;
	mov.u32 	%r152, %ctaid.x;
	mad.lo.s32 	%r1, %r150, %r151, %r152;
	cvt.u32.u16	%r2, %rs5;
	mul.wide.u16 	%r3, %rs5, %rs5;
	div.s32 	%r153, %r149, %r3;
	setp.ge.s32	%p1, %r1, %r153;
	@%p1 bra 	BB0_316;

	mul.lo.s32 	%r4, %r1, 7;
	mul.lo.s32 	%r5, %r1, %r3;
	setp.eq.s32	%p2, %r3, 0;
	mov.f64 	%fd1701, 0d0000000000000000;
	mov.f64 	%fd1698, %fd1701;
	mov.f64 	%fd1695, %fd1701;
	mov.u32 	%r471, 0;
	mov.f64 	%fd1700, %fd1701;
	mov.f64 	%fd1697, %fd1701;
	mov.f64 	%fd1694, %fd1701;
	@%p2 bra 	BB0_3;

BB0_2:
	add.s32 	%r155, %r471, %r5;
	mul.wide.s32 	%rd15, %r155, 4;
	add.s64 	%rd16, %rd1, %rd15;
	ldu.global.u32 	%r156, [%rd16];
	cvt.rn.f64.s32	%fd540, %r156;
	add.f64 	%fd1695, %fd1695, %fd540;
	rem.s32 	%r157, %r471, %r2;
	mul.lo.s32 	%r158, %r157, %r156;
	cvt.rn.f64.s32	%fd541, %r158;
	add.f64 	%fd1701, %fd1701, %fd541;
	div.s32 	%r159, %r471, %r2;
	mul.lo.s32 	%r160, %r156, %r159;
	cvt.rn.f64.s32	%fd542, %r160;
	add.f64 	%fd1698, %fd1698, %fd542;
	add.s32 	%r471, %r471, 1;
	setp.lt.s32	%p3, %r471, %r3;
	mov.f64 	%fd1694, %fd1695;
	mov.f64 	%fd1697, %fd1698;
	mov.f64 	%fd1700, %fd1701;
	@%p3 bra 	BB0_2;

BB0_3:
	cvta.to.global.u64 	%rd17, %rd12;
	mul.wide.s32 	%rd18, %r4, 8;
	add.s64 	%rd2, %rd17, %rd18;
	div.rn.f64 	%fd545, %fd1700, %fd1694;
	st.global.f64 	[%rd2+8], %fd545;
	div.rn.f64 	%fd546, %fd1697, %fd1694;
	st.global.f64 	[%rd2+16], %fd546;
	cvt.rn.f64.s32	%fd547, %r3;
	div.rn.f64 	%fd10, %fd1694, %fd547;
	mov.f64 	%fd1704, 0d0000000000000000;
	mov.u32 	%r472, 0;
	mov.f64 	%fd1703, %fd1704;
	@%p2 bra 	BB0_5;

BB0_4:
	add.s32 	%r162, %r472, %r5;
	mul.wide.s32 	%rd19, %r162, 4;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u32 	%r163, [%rd20];
	cvt.rn.f64.s32	%fd548, %r163;
	sub.f64 	%fd549, %fd548, %fd10;
	fma.rn.f64 	%fd1704, %fd549, %fd549, %fd1704;
	add.s32 	%r472, %r472, 1;
	setp.lt.s32	%p5, %r472, %r3;
	mov.f64 	%fd1703, %fd1704;
	@%p5 bra 	BB0_4;

BB0_5:
	cvta.to.global.u64 	%rd21, %rd14;
	cvta.to.global.u64 	%rd22, %rd13;
	ld.global.f64 	%fd550, [%rd22];
	ld.global.f64 	%fd551, [%rd2];
	mul.f64 	%fd14, %fd551, %fd550;
	ld.global.f64 	%fd552, [%rd22+8];
	mul.f64 	%fd15, %fd551, %fd552;
	ld.global.f64 	%fd553, [%rd22+96];
	mul.f64 	%fd16, %fd551, %fd553;
	ld.global.f64 	%fd554, [%rd22+104];
	mul.f64 	%fd17, %fd551, %fd554;
	add.s64 	%rd3, %rd21, %rd18;
	ld.global.f64 	%fd555, [%rd3];
	mul.f64 	%fd556, %fd551, %fd555;
	ld.global.f64 	%fd557, [%rd3+48];
	st.global.f64 	[%rd3], %fd556;
	ld.global.f64 	%fd558, [%rd2];
	mul.f64 	%fd559, %fd558, %fd557;
	st.global.f64 	[%rd3+48], %fd559;
	ld.global.f64 	%fd560, [%rd22+56];
	ld.global.f64 	%fd1705, [%rd22+48];
	setp.gtu.f64	%p6, %fd1705, %fd560;
	@%p6 bra 	BB0_18;

	ld.global.f64 	%fd1706, [%rd22+72];
	mov.f64 	%fd1717, 0d3FF0000000000000;

BB0_7:
	mov.f64 	%fd1711, %fd1717;
	mov.f64 	%fd1714, %fd1711;
	add.f64 	%fd562, %fd1705, %fd1705;
	mul.f64 	%fd563, %fd1705, %fd562;
	rcp.rn.f64 	%fd23, %fd563;
	ld.global.f64 	%fd1707, [%rd22+64];
	setp.gtu.f64	%p7, %fd1707, %fd1706;
	mov.f64 	%fd1716, %fd1714;
	@%p7 bra 	BB0_17;

BB0_8:
	mov.f64 	%fd1708, %fd1714;
	mov.f64 	%fd27, %fd1708;
	mov.f64 	%fd1719, 0d0000000000000000;
	@%p2 bra 	BB0_14;

	add.f64 	%fd566, %fd1707, %fd1707;
	mul.f64 	%fd567, %fd1707, %fd566;
	rcp.rn.f64 	%fd28, %fd567;
	ld.global.f64 	%fd29, [%rd2];
	ld.global.f64 	%fd30, [%rd2+8];
	ld.global.f64 	%fd31, [%rd2+16];
	mov.f64 	%fd1719, 0d0000000000000000;
	mov.u32 	%r473, 0;

BB0_10:
	rem.s32 	%r172, %r473, %r2;
	div.s32 	%r173, %r473, %r2;
	cvt.rn.f64.s32	%fd568, %r172;
	sub.f64 	%fd569, %fd568, %fd30;
	mul.f64 	%fd570, %fd23, %fd569;
	cvt.rn.f64.s32	%fd571, %r173;
	sub.f64 	%fd572, %fd571, %fd31;
	mul.f64 	%fd573, %fd28, %fd572;
	mul.f64 	%fd574, %fd572, %fd573;
	fma.rn.f64 	%fd33, %fd569, %fd570, %fd574;
	neg.f64 	%fd575, %fd33;
	mov.f64 	%fd576, 0d4338000000000000;
	mov.f64 	%fd577, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd578, %fd575, %fd577, %fd576;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r11, %temp}, %fd578;
	}
	mov.f64 	%fd579, 0dC338000000000000;
	add.rn.f64 	%fd580, %fd578, %fd579;
	mov.f64 	%fd581, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd582, %fd580, %fd581, %fd575;
	mov.f64 	%fd583, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd584, %fd580, %fd583, %fd582;
	mov.f64 	%fd585, 0d3E928AF3FCA213EA;
	mov.f64 	%fd586, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd587, %fd586, %fd584, %fd585;
	mov.f64 	%fd588, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd589, %fd587, %fd584, %fd588;
	mov.f64 	%fd590, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd591, %fd589, %fd584, %fd590;
	mov.f64 	%fd592, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd593, %fd591, %fd584, %fd592;
	mov.f64 	%fd594, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd595, %fd593, %fd584, %fd594;
	mov.f64 	%fd596, 0d3F81111111122322;
	fma.rn.f64 	%fd597, %fd595, %fd584, %fd596;
	mov.f64 	%fd598, 0d3FA55555555502A1;
	fma.rn.f64 	%fd599, %fd597, %fd584, %fd598;
	mov.f64 	%fd600, 0d3FC5555555555511;
	fma.rn.f64 	%fd601, %fd599, %fd584, %fd600;
	mov.f64 	%fd602, 0d3FE000000000000B;
	fma.rn.f64 	%fd603, %fd601, %fd584, %fd602;
	mov.f64 	%fd604, 0d3FF0000000000000;
	fma.rn.f64 	%fd605, %fd603, %fd584, %fd604;
	fma.rn.f64 	%fd606, %fd605, %fd584, %fd604;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r12, %temp}, %fd606;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r13}, %fd606;
	}
	shl.b32 	%r174, %r11, 20;
	add.s32 	%r175, %r13, %r174;
	mov.b64 	%fd1718, {%r12, %r175};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd575;
	}
	mov.b32 	 %f6, %r176;
	abs.f32 	%f1, %f6;
	setp.lt.f32	%p9, %f1, 0f4086232B;
	@%p9 bra 	BB0_13;

	setp.gt.f64	%p10, %fd33, 0d8000000000000000;
	mov.f64 	%fd607, 0d7FF0000000000000;
	sub.f64 	%fd608, %fd607, %fd33;
	selp.f64	%fd1718, 0d0000000000000000, %fd608, %p10;
	setp.geu.f32	%p11, %f1, 0f40874800;
	@%p11 bra 	BB0_13;

	shr.u32 	%r177, %r11, 31;
	add.s32 	%r178, %r11, %r177;
	shr.s32 	%r179, %r178, 1;
	shl.b32 	%r180, %r179, 20;
	add.s32 	%r181, %r180, %r13;
	mov.b64 	%fd609, {%r12, %r181};
	sub.s32 	%r182, %r11, %r179;
	shl.b32 	%r183, %r182, 20;
	add.s32 	%r184, %r183, 1072693248;
	mov.u32 	%r185, 0;
	mov.b64 	%fd610, {%r185, %r184};
	mul.f64 	%fd1718, %fd609, %fd610;

BB0_13:
	add.s32 	%r186, %r473, %r5;
	mul.wide.s32 	%rd30, %r186, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.u32 	%r187, [%rd31];
	cvt.rn.f64.s32	%fd611, %r187;
	mul.f64 	%fd612, %fd29, %fd1718;
	sub.f64 	%fd613, %fd612, %fd611;
	fma.rn.f64 	%fd1719, %fd613, %fd613, %fd1719;
	add.s32 	%r473, %r473, 1;
	setp.lt.s32	%p12, %r473, %r3;
	@%p12 bra 	BB0_10;

BB0_14:
	div.rn.f64 	%fd40, %fd1719, %fd1703;
	setp.geu.f64	%p13, %fd40, %fd27;
	mov.f64 	%fd1715, %fd27;
	@%p13 bra 	BB0_16;

	st.global.f64 	[%rd2+24], %fd1705;
	st.global.f64 	[%rd2+32], %fd1707;
	ld.global.f64 	%fd1706, [%rd22+72];
	mov.f64 	%fd1715, %fd40;

BB0_16:
	mov.f64 	%fd1714, %fd1715;
	ld.global.f64 	%fd614, [%rd3+32];
	add.f64 	%fd1707, %fd1707, %fd614;
	setp.le.f64	%p14, %fd1707, %fd1706;
	mov.f64 	%fd1716, %fd1714;
	@%p14 bra 	BB0_8;

BB0_17:
	mov.f64 	%fd1717, %fd1716;
	ld.global.f64 	%fd615, [%rd3+24];
	add.f64 	%fd1705, %fd1705, %fd615;
	ld.global.f64 	%fd616, [%rd22+56];
	setp.le.f64	%p15, %fd1705, %fd616;
	@%p15 bra 	BB0_7;

BB0_18:
	ld.global.f64 	%fd620, [%rd2+24];
	add.f64 	%fd621, %fd620, %fd620;
	mul.f64 	%fd622, %fd620, %fd621;
	rcp.rn.f64 	%fd1722, %fd622;
	ld.global.f64 	%fd623, [%rd2+32];
	add.f64 	%fd624, %fd623, %fd623;
	mul.f64 	%fd625, %fd623, %fd624;
	rcp.rn.f64 	%fd1720, %fd625;
	mov.u16 	%rs8, 1;
	mov.u32 	%r478, 1;
	mov.f64 	%fd1754, 0d3FF0000000000000;
	mov.f64 	%fd1752, %fd1754;
	mov.u32 	%r476, 0;
	mov.f64 	%fd1721, 0d0000000000000000;
	bra.uni 	BB0_19;

BB0_198:
	neg.f64 	%fd1242, %fd344;
	st.global.f64 	[%rd3+48], %fd1242;
	mov.f64 	%fd1728, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1728;
	bra.uni 	BB0_225;

BB0_208:
	neg.f64 	%fd1297, %fd359;
	st.global.f64 	[%rd3+48], %fd1297;
	mov.f64 	%fd1731, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1731;
	bra.uni 	BB0_225;

BB0_194:
	neg.f64 	%fd1239, %fd343;
	st.global.f64 	[%rd5], %fd1239;
	mov.f64 	%fd1726, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1726;
	bra.uni 	BB0_225;

BB0_19:
	mov.f64 	%fd54, %fd1754;
	mov.f64 	%fd53, %fd1752;
	setp.eq.s32	%p16, %r476, 0;
	@%p16 bra 	BB0_210;
	bra.uni 	BB0_20;

BB0_210:
	ld.global.f64 	%fd360, [%rd3];
	ld.global.f64 	%fd1299, [%rd2];
	add.f64 	%fd361, %fd1299, %fd360;
	setp.gt.f64	%p139, %fd361, %fd14;
	setp.lt.f64	%p140, %fd361, %fd15;
	and.pred  	%p141, %p139, %p140;
	@%p141 bra 	BB0_214;
	bra.uni 	BB0_211;

BB0_214:
	st.global.f64 	[%rd2], %fd361;
	mov.f64 	%fd1821, 0d0000000000000000;
	@%p2 bra 	BB0_220;

	ld.global.f64 	%fd362, [%rd2+8];
	add.f64 	%fd363, %fd1721, %fd1721;
	ld.global.f64 	%fd364, [%rd2+16];
	ld.global.f64 	%fd365, [%rd2+48];
	mov.f64 	%fd1821, 0d0000000000000000;
	mov.u32 	%r501, 0;

BB0_216:
	rem.s32 	%r368, %r501, %r2;
	cvt.rn.f64.s32	%fd1304, %r368;
	sub.f64 	%fd1305, %fd1304, %fd362;
	mul.f64 	%fd1306, %fd1722, %fd1305;
	mul.f64 	%fd1307, %fd1305, %fd1306;
	mul.f64 	%fd1308, %fd363, %fd1305;
	div.s32 	%r369, %r501, %r2;
	cvt.rn.f64.s32	%fd1309, %r369;
	sub.f64 	%fd1310, %fd1309, %fd364;
	mul.f64 	%fd1311, %fd1308, %fd1310;
	sub.f64 	%fd1312, %fd1307, %fd1311;
	mul.f64 	%fd1313, %fd1720, %fd1310;
	fma.rn.f64 	%fd367, %fd1310, %fd1313, %fd1312;
	neg.f64 	%fd1314, %fd367;
	mov.f64 	%fd1315, 0d4338000000000000;
	mov.f64 	%fd1316, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1317, %fd1314, %fd1316, %fd1315;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r100, %temp}, %fd1317;
	}
	mov.f64 	%fd1318, 0dC338000000000000;
	add.rn.f64 	%fd1319, %fd1317, %fd1318;
	mov.f64 	%fd1320, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1321, %fd1319, %fd1320, %fd1314;
	mov.f64 	%fd1322, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1323, %fd1319, %fd1322, %fd1321;
	mov.f64 	%fd1324, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1325, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1326, %fd1325, %fd1323, %fd1324;
	mov.f64 	%fd1327, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1328, %fd1326, %fd1323, %fd1327;
	mov.f64 	%fd1329, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1330, %fd1328, %fd1323, %fd1329;
	mov.f64 	%fd1331, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1332, %fd1330, %fd1323, %fd1331;
	mov.f64 	%fd1333, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1334, %fd1332, %fd1323, %fd1333;
	mov.f64 	%fd1335, 0d3F81111111122322;
	fma.rn.f64 	%fd1336, %fd1334, %fd1323, %fd1335;
	mov.f64 	%fd1337, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1338, %fd1336, %fd1323, %fd1337;
	mov.f64 	%fd1339, 0d3FC5555555555511;
	fma.rn.f64 	%fd1340, %fd1338, %fd1323, %fd1339;
	mov.f64 	%fd1341, 0d3FE000000000000B;
	fma.rn.f64 	%fd1342, %fd1340, %fd1323, %fd1341;
	mov.f64 	%fd1343, 0d3FF0000000000000;
	fma.rn.f64 	%fd1344, %fd1342, %fd1323, %fd1343;
	fma.rn.f64 	%fd1345, %fd1344, %fd1323, %fd1343;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r101, %temp}, %fd1345;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r102}, %fd1345;
	}
	shl.b32 	%r370, %r100, 20;
	add.s32 	%r371, %r102, %r370;
	mov.b64 	%fd1820, {%r101, %r371};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r372}, %fd1314;
	}
	mov.b32 	 %f9, %r372;
	abs.f32 	%f4, %f9;
	setp.lt.f32	%p144, %f4, 0f4086232B;
	@%p144 bra 	BB0_219;

	setp.gt.f64	%p145, %fd367, 0d8000000000000000;
	mov.f64 	%fd1346, 0d7FF0000000000000;
	sub.f64 	%fd1347, %fd1346, %fd367;
	selp.f64	%fd1820, 0d0000000000000000, %fd1347, %p145;
	setp.geu.f32	%p146, %f4, 0f40874800;
	@%p146 bra 	BB0_219;

	shr.u32 	%r373, %r100, 31;
	add.s32 	%r374, %r100, %r373;
	shr.s32 	%r375, %r374, 1;
	shl.b32 	%r376, %r375, 20;
	add.s32 	%r377, %r376, %r102;
	mov.b64 	%fd1348, {%r101, %r377};
	sub.s32 	%r378, %r100, %r375;
	shl.b32 	%r379, %r378, 20;
	add.s32 	%r380, %r379, 1072693248;
	mov.u32 	%r381, 0;
	mov.b64 	%fd1349, {%r381, %r380};
	mul.f64 	%fd1820, %fd1348, %fd1349;

BB0_219:
	fma.rn.f64 	%fd1350, %fd361, %fd1820, %fd365;
	add.s32 	%r382, %r501, %r5;
	mul.wide.s32 	%rd197, %r382, 4;
	add.s64 	%rd198, %rd1, %rd197;
	ld.global.u32 	%r383, [%rd198];
	cvt.rn.f64.s32	%fd1351, %r383;
	sub.f64 	%fd1352, %fd1350, %fd1351;
	fma.rn.f64 	%fd1821, %fd1352, %fd1352, %fd1821;
	add.s32 	%r501, %r501, 1;
	setp.lt.s32	%p147, %r501, %r3;
	@%p147 bra 	BB0_216;

BB0_220:
	div.rn.f64 	%fd1755, %fd1821, %fd1703;
	setp.lt.f64	%p148, %fd1755, %fd54;
	mov.f64 	%fd1753, %fd54;
	@%p148 bra 	BB0_225;

	ld.global.f64 	%fd1353, [%rd3];
	sub.f64 	%fd1354, %fd361, %fd1353;
	st.global.f64 	[%rd2], %fd1354;
	ld.global.f64 	%fd375, [%rd3];
	setp.lt.f64	%p149, %fd375, 0d0000000000000000;
	@%p149 bra 	BB0_223;
	bra.uni 	BB0_222;

BB0_223:
	mul.f64 	%fd1356, %fd375, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3], %fd1356;
	bra.uni 	BB0_224;

BB0_20:
	setp.eq.s32	%p17, %r476, 6;
	@%p17 bra 	BB0_196;
	bra.uni 	BB0_21;

BB0_196:
	ld.global.f64 	%fd344, [%rd3+48];
	ld.global.f64 	%fd1241, [%rd2+48];
	add.f64 	%fd345, %fd1241, %fd344;
	setp.gt.f64	%p128, %fd345, %fd16;
	setp.lt.f64	%p129, %fd345, %fd17;
	and.pred  	%p130, %p128, %p129;
	@%p130 bra 	BB0_200;
	bra.uni 	BB0_197;

BB0_200:
	st.global.f64 	[%rd2+48], %fd345;
	mov.f64 	%fd1819, 0d0000000000000000;
	@%p2 bra 	BB0_206;

	ld.global.f64 	%fd346, [%rd2];
	ld.global.f64 	%fd347, [%rd2+8];
	add.f64 	%fd348, %fd1721, %fd1721;
	ld.global.f64 	%fd349, [%rd2+16];
	mov.f64 	%fd1819, 0d0000000000000000;
	mov.u32 	%r500, 0;

BB0_202:
	rem.s32 	%r331, %r500, %r2;
	div.s32 	%r332, %r500, %r2;
	cvt.rn.f64.s32	%fd1246, %r331;
	sub.f64 	%fd1247, %fd1246, %fd347;
	mul.f64 	%fd1248, %fd1722, %fd1247;
	mul.f64 	%fd1249, %fd1247, %fd1248;
	mul.f64 	%fd1250, %fd348, %fd1247;
	cvt.rn.f64.s32	%fd1251, %r332;
	sub.f64 	%fd1252, %fd1251, %fd349;
	mul.f64 	%fd1253, %fd1250, %fd1252;
	sub.f64 	%fd1254, %fd1249, %fd1253;
	mul.f64 	%fd1255, %fd1720, %fd1252;
	fma.rn.f64 	%fd351, %fd1252, %fd1255, %fd1254;
	neg.f64 	%fd1256, %fd351;
	mov.f64 	%fd1257, 0d4338000000000000;
	mov.f64 	%fd1258, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1259, %fd1256, %fd1258, %fd1257;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r92, %temp}, %fd1259;
	}
	mov.f64 	%fd1260, 0dC338000000000000;
	add.rn.f64 	%fd1261, %fd1259, %fd1260;
	mov.f64 	%fd1262, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1263, %fd1261, %fd1262, %fd1256;
	mov.f64 	%fd1264, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1265, %fd1261, %fd1264, %fd1263;
	mov.f64 	%fd1266, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1267, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1268, %fd1267, %fd1265, %fd1266;
	mov.f64 	%fd1269, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1270, %fd1268, %fd1265, %fd1269;
	mov.f64 	%fd1271, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1272, %fd1270, %fd1265, %fd1271;
	mov.f64 	%fd1273, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1274, %fd1272, %fd1265, %fd1273;
	mov.f64 	%fd1275, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1276, %fd1274, %fd1265, %fd1275;
	mov.f64 	%fd1277, 0d3F81111111122322;
	fma.rn.f64 	%fd1278, %fd1276, %fd1265, %fd1277;
	mov.f64 	%fd1279, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1280, %fd1278, %fd1265, %fd1279;
	mov.f64 	%fd1281, 0d3FC5555555555511;
	fma.rn.f64 	%fd1282, %fd1280, %fd1265, %fd1281;
	mov.f64 	%fd1283, 0d3FE000000000000B;
	fma.rn.f64 	%fd1284, %fd1282, %fd1265, %fd1283;
	mov.f64 	%fd1285, 0d3FF0000000000000;
	fma.rn.f64 	%fd1286, %fd1284, %fd1265, %fd1285;
	fma.rn.f64 	%fd1287, %fd1286, %fd1265, %fd1285;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r93, %temp}, %fd1287;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r94}, %fd1287;
	}
	shl.b32 	%r333, %r92, 20;
	add.s32 	%r334, %r94, %r333;
	mov.b64 	%fd1818, {%r93, %r334};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r335}, %fd1256;
	}
	mov.b32 	 %f8, %r335;
	abs.f32 	%f3, %f8;
	setp.lt.f32	%p133, %f3, 0f4086232B;
	@%p133 bra 	BB0_205;

	setp.gt.f64	%p134, %fd351, 0d8000000000000000;
	mov.f64 	%fd1288, 0d7FF0000000000000;
	sub.f64 	%fd1289, %fd1288, %fd351;
	selp.f64	%fd1818, 0d0000000000000000, %fd1289, %p134;
	setp.geu.f32	%p135, %f3, 0f40874800;
	@%p135 bra 	BB0_205;

	shr.u32 	%r336, %r92, 31;
	add.s32 	%r337, %r92, %r336;
	shr.s32 	%r338, %r337, 1;
	shl.b32 	%r339, %r338, 20;
	add.s32 	%r340, %r339, %r94;
	mov.b64 	%fd1290, {%r93, %r340};
	sub.s32 	%r341, %r92, %r338;
	shl.b32 	%r342, %r341, 20;
	add.s32 	%r343, %r342, 1072693248;
	mov.u32 	%r344, 0;
	mov.b64 	%fd1291, {%r344, %r343};
	mul.f64 	%fd1818, %fd1290, %fd1291;

BB0_205:
	fma.rn.f64 	%fd1292, %fd346, %fd1818, %fd345;
	mad.lo.s32 	%r350, %r1, %r3, %r500;
	mul.wide.s32 	%rd189, %r350, 4;
	add.s64 	%rd190, %rd1, %rd189;
	ld.global.u32 	%r351, [%rd190];
	cvt.rn.f64.s32	%fd1293, %r351;
	sub.f64 	%fd1294, %fd1292, %fd1293;
	fma.rn.f64 	%fd1819, %fd1294, %fd1294, %fd1819;
	add.s32 	%r500, %r500, 1;
	setp.lt.s32	%p136, %r500, %r3;
	@%p136 bra 	BB0_202;

BB0_206:
	div.rn.f64 	%fd1755, %fd1819, %fd1703;
	setp.lt.f64	%p137, %fd1755, %fd54;
	mov.f64 	%fd1730, %fd53;
	mov.f64 	%fd1753, %fd1730;
	@%p137 bra 	BB0_225;

	ld.global.f64 	%fd1295, [%rd3+48];
	sub.f64 	%fd1296, %fd345, %fd1295;
	st.global.f64 	[%rd2+48], %fd1296;
	ld.global.f64 	%fd359, [%rd3+48];
	setp.lt.f64	%p138, %fd359, 0d0000000000000000;
	@%p138 bra 	BB0_209;
	bra.uni 	BB0_208;

BB0_209:
	mul.f64 	%fd1298, %fd359, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3+48], %fd1298;
	mov.f64 	%fd1732, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1732;
	bra.uni 	BB0_225;

BB0_211:
	setp.lt.f64	%p142, %fd360, 0d0000000000000000;
	@%p142 bra 	BB0_213;
	bra.uni 	BB0_212;

BB0_213:
	mul.f64 	%fd1301, %fd360, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3], %fd1301;
	bra.uni 	BB0_224;

BB0_21:
	mad.lo.s32 	%r195, %r1, 7, %r476;
	mul.wide.s32 	%rd35, %r195, 8;
	add.s64 	%rd4, %rd17, %rd35;
	add.s64 	%rd5, %rd21, %rd35;
	ld.global.f64 	%fd55, [%rd5];
	ld.global.f64 	%fd626, [%rd4];
	add.f64 	%fd56, %fd626, %fd55;
	shl.b32 	%r196, %r476, 1;
	mul.wide.s32 	%rd38, %r196, 8;
	add.s64 	%rd6, %rd22, %rd38;
	ld.global.f64 	%fd627, [%rd6];
	setp.leu.f64	%p18, %fd56, %fd627;
	@%p18 bra 	BB0_23;

	ld.global.f64 	%fd628, [%rd6+8];
	setp.lt.f64	%p19, %fd56, %fd628;
	@%p19 bra 	BB0_26;
	bra.uni 	BB0_23;

BB0_26:
	st.global.f64 	[%rd4], %fd56;
	ld.global.f64 	%fd1756, [%rd2+40];
	abs.f64 	%fd631, %fd1756;
	setp.neu.f64	%p21, %fd631, 0d7FF0000000000000;
	@%p21 bra 	BB0_28;

	mov.f64 	%fd632, 0d0000000000000000;
	mul.rn.f64 	%fd1756, %fd1756, %fd632;

BB0_28:
	mul.f64 	%fd633, %fd1756, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r479, %fd633;
	add.u64 	%rd39, %SP, 0;
	cvta.to.local.u64 	%rd40, %rd39;
	st.local.u32 	[%rd40], %r479;
	cvt.rn.f64.s32	%fd634, %r479;
	neg.f64 	%fd635, %fd634;
	mov.f64 	%fd636, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd637, %fd635, %fd636, %fd1756;
	mov.f64 	%fd638, 0d3C91A62633145C00;
	fma.rn.f64 	%fd639, %fd635, %fd638, %fd637;
	mov.f64 	%fd640, 0d397B839A252049C0;
	fma.rn.f64 	%fd1757, %fd635, %fd640, %fd639;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r197}, %fd1756;
	}
	and.b32  	%r198, %r197, 2145386496;
	setp.lt.u32	%p22, %r198, 1105199104;
	@%p22 bra 	BB0_30;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1756;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1757, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r479, [%rd40];

BB0_30:
	add.s32 	%r20, %r479, 1;
	and.b32  	%r199, %r20, 1;
	shl.b32 	%r200, %r199, 3;
	setp.eq.b32	%p23, %r199, 1;
	selp.f64	%fd641, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p23;
	mul.wide.u32 	%rd43, %r200, 8;
	mov.u64 	%rd44, __cudart_sin_cos_coeffs;
	add.s64 	%rd45, %rd43, %rd44;
	ld.const.f64 	%fd642, [%rd45+8];
	mul.rn.f64 	%fd63, %fd1757, %fd1757;
	fma.rn.f64 	%fd643, %fd641, %fd63, %fd642;
	ld.const.f64 	%fd644, [%rd45+16];
	fma.rn.f64 	%fd645, %fd643, %fd63, %fd644;
	ld.const.f64 	%fd646, [%rd45+24];
	fma.rn.f64 	%fd647, %fd645, %fd63, %fd646;
	ld.const.f64 	%fd648, [%rd45+32];
	fma.rn.f64 	%fd649, %fd647, %fd63, %fd648;
	ld.const.f64 	%fd650, [%rd45+40];
	fma.rn.f64 	%fd651, %fd649, %fd63, %fd650;
	ld.const.f64 	%fd652, [%rd45+48];
	fma.rn.f64 	%fd64, %fd651, %fd63, %fd652;
	fma.rn.f64 	%fd1758, %fd64, %fd1757, %fd1757;
	setp.eq.s32	%p24, %r199, 0;
	@%p24 bra 	BB0_32;

	mov.f64 	%fd653, 0d3FF0000000000000;
	fma.rn.f64 	%fd1758, %fd64, %fd63, %fd653;

BB0_32:
	and.b32  	%r201, %r20, 2;
	setp.eq.s32	%p25, %r201, 0;
	@%p25 bra 	BB0_34;

	mov.f64 	%fd654, 0d0000000000000000;
	mov.f64 	%fd655, 0dBFF0000000000000;
	fma.rn.f64 	%fd1758, %fd1758, %fd655, %fd654;

BB0_34:
	ld.global.f64 	%fd1759, [%rd2+40];
	abs.f64 	%fd656, %fd1759;
	setp.neu.f64	%p26, %fd656, 0d7FF0000000000000;
	@%p26 bra 	BB0_36;

	mov.f64 	%fd657, 0d0000000000000000;
	mul.rn.f64 	%fd1759, %fd1759, %fd657;

BB0_36:
	mul.f64 	%fd658, %fd1759, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r480, %fd658;
	st.local.u32 	[%rd40], %r480;
	cvt.rn.f64.s32	%fd659, %r480;
	neg.f64 	%fd660, %fd659;
	fma.rn.f64 	%fd662, %fd660, %fd636, %fd1759;
	fma.rn.f64 	%fd664, %fd660, %fd638, %fd662;
	fma.rn.f64 	%fd1760, %fd660, %fd640, %fd664;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r202}, %fd1759;
	}
	and.b32  	%r203, %r202, 2145386496;
	setp.lt.u32	%p27, %r203, 1105199104;
	@%p27 bra 	BB0_38;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1759;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1760, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r480, [%rd40];

BB0_38:
	add.s32 	%r24, %r480, 1;
	and.b32  	%r204, %r24, 1;
	shl.b32 	%r205, %r204, 3;
	setp.eq.b32	%p28, %r204, 1;
	selp.f64	%fd666, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p28;
	mul.wide.u32 	%rd50, %r205, 8;
	add.s64 	%rd52, %rd50, %rd44;
	ld.const.f64 	%fd667, [%rd52+8];
	mul.rn.f64 	%fd76, %fd1760, %fd1760;
	fma.rn.f64 	%fd668, %fd666, %fd76, %fd667;
	ld.const.f64 	%fd669, [%rd52+16];
	fma.rn.f64 	%fd670, %fd668, %fd76, %fd669;
	ld.const.f64 	%fd671, [%rd52+24];
	fma.rn.f64 	%fd672, %fd670, %fd76, %fd671;
	ld.const.f64 	%fd673, [%rd52+32];
	fma.rn.f64 	%fd674, %fd672, %fd76, %fd673;
	ld.const.f64 	%fd675, [%rd52+40];
	fma.rn.f64 	%fd676, %fd674, %fd76, %fd675;
	ld.const.f64 	%fd677, [%rd52+48];
	fma.rn.f64 	%fd77, %fd676, %fd76, %fd677;
	fma.rn.f64 	%fd1761, %fd77, %fd1760, %fd1760;
	setp.eq.s32	%p29, %r204, 0;
	@%p29 bra 	BB0_40;

	mov.f64 	%fd678, 0d3FF0000000000000;
	fma.rn.f64 	%fd1761, %fd77, %fd76, %fd678;

BB0_40:
	and.b32  	%r206, %r24, 2;
	setp.eq.s32	%p30, %r206, 0;
	@%p30 bra 	BB0_42;

	mov.f64 	%fd679, 0d0000000000000000;
	mov.f64 	%fd680, 0dBFF0000000000000;
	fma.rn.f64 	%fd1761, %fd1761, %fd680, %fd679;

BB0_42:
	ld.global.f64 	%fd681, [%rd2+24];
	add.f64 	%fd682, %fd681, %fd681;
	mul.f64 	%fd683, %fd681, %fd682;
	mul.f64 	%fd684, %fd1758, %fd1761;
	div.rn.f64 	%fd83, %fd684, %fd683;
	ld.global.f64 	%fd1762, [%rd2+40];
	abs.f64 	%fd685, %fd1762;
	setp.neu.f64	%p31, %fd685, 0d7FF0000000000000;
	@%p31 bra 	BB0_44;

	mov.f64 	%fd686, 0d0000000000000000;
	mul.rn.f64 	%fd1762, %fd1762, %fd686;

BB0_44:
	mul.f64 	%fd687, %fd1762, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r481, %fd687;
	st.local.u32 	[%rd40], %r481;
	cvt.rn.f64.s32	%fd688, %r481;
	neg.f64 	%fd689, %fd688;
	fma.rn.f64 	%fd691, %fd689, %fd636, %fd1762;
	fma.rn.f64 	%fd693, %fd689, %fd638, %fd691;
	fma.rn.f64 	%fd1763, %fd689, %fd640, %fd693;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r207}, %fd1762;
	}
	and.b32  	%r208, %r207, 2145386496;
	setp.lt.u32	%p32, %r208, 1105199104;
	@%p32 bra 	BB0_46;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1762;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1763, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r481, [%rd40];

BB0_46:
	and.b32  	%r209, %r481, 1;
	shl.b32 	%r210, %r209, 3;
	setp.eq.b32	%p33, %r209, 1;
	selp.f64	%fd695, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p33;
	mul.wide.u32 	%rd57, %r210, 8;
	add.s64 	%rd59, %rd57, %rd44;
	ld.const.f64 	%fd696, [%rd59+8];
	mul.rn.f64 	%fd90, %fd1763, %fd1763;
	fma.rn.f64 	%fd697, %fd695, %fd90, %fd696;
	ld.const.f64 	%fd698, [%rd59+16];
	fma.rn.f64 	%fd699, %fd697, %fd90, %fd698;
	ld.const.f64 	%fd700, [%rd59+24];
	fma.rn.f64 	%fd701, %fd699, %fd90, %fd700;
	ld.const.f64 	%fd702, [%rd59+32];
	fma.rn.f64 	%fd703, %fd701, %fd90, %fd702;
	ld.const.f64 	%fd704, [%rd59+40];
	fma.rn.f64 	%fd705, %fd703, %fd90, %fd704;
	ld.const.f64 	%fd706, [%rd59+48];
	fma.rn.f64 	%fd91, %fd705, %fd90, %fd706;
	fma.rn.f64 	%fd1764, %fd91, %fd1763, %fd1763;
	setp.eq.s32	%p34, %r209, 0;
	@%p34 bra 	BB0_48;

	mov.f64 	%fd707, 0d3FF0000000000000;
	fma.rn.f64 	%fd1764, %fd91, %fd90, %fd707;

BB0_48:
	and.b32  	%r211, %r481, 2;
	setp.eq.s32	%p35, %r211, 0;
	@%p35 bra 	BB0_50;

	mov.f64 	%fd708, 0d0000000000000000;
	mov.f64 	%fd709, 0dBFF0000000000000;
	fma.rn.f64 	%fd1764, %fd1764, %fd709, %fd708;

BB0_50:
	ld.global.f64 	%fd1765, [%rd2+40];
	abs.f64 	%fd710, %fd1765;
	setp.neu.f64	%p36, %fd710, 0d7FF0000000000000;
	@%p36 bra 	BB0_52;

	mov.f64 	%fd711, 0d0000000000000000;
	mul.rn.f64 	%fd1765, %fd1765, %fd711;

BB0_52:
	mul.f64 	%fd712, %fd1765, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r482, %fd712;
	st.local.u32 	[%rd40], %r482;
	cvt.rn.f64.s32	%fd713, %r482;
	neg.f64 	%fd714, %fd713;
	fma.rn.f64 	%fd716, %fd714, %fd636, %fd1765;
	fma.rn.f64 	%fd718, %fd714, %fd638, %fd716;
	fma.rn.f64 	%fd1766, %fd714, %fd640, %fd718;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r212}, %fd1765;
	}
	and.b32  	%r213, %r212, 2145386496;
	setp.lt.u32	%p37, %r213, 1105199104;
	@%p37 bra 	BB0_54;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1765;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1766, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r482, [%rd40];

BB0_54:
	and.b32  	%r214, %r482, 1;
	shl.b32 	%r215, %r214, 3;
	setp.eq.b32	%p38, %r214, 1;
	selp.f64	%fd720, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p38;
	mul.wide.u32 	%rd64, %r215, 8;
	add.s64 	%rd66, %rd64, %rd44;
	ld.const.f64 	%fd721, [%rd66+8];
	mul.rn.f64 	%fd103, %fd1766, %fd1766;
	fma.rn.f64 	%fd722, %fd720, %fd103, %fd721;
	ld.const.f64 	%fd723, [%rd66+16];
	fma.rn.f64 	%fd724, %fd722, %fd103, %fd723;
	ld.const.f64 	%fd725, [%rd66+24];
	fma.rn.f64 	%fd726, %fd724, %fd103, %fd725;
	ld.const.f64 	%fd727, [%rd66+32];
	fma.rn.f64 	%fd728, %fd726, %fd103, %fd727;
	ld.const.f64 	%fd729, [%rd66+40];
	fma.rn.f64 	%fd730, %fd728, %fd103, %fd729;
	ld.const.f64 	%fd731, [%rd66+48];
	fma.rn.f64 	%fd104, %fd730, %fd103, %fd731;
	fma.rn.f64 	%fd1767, %fd104, %fd1766, %fd1766;
	setp.eq.s32	%p39, %r214, 0;
	@%p39 bra 	BB0_56;

	mov.f64 	%fd732, 0d3FF0000000000000;
	fma.rn.f64 	%fd1767, %fd104, %fd103, %fd732;

BB0_56:
	and.b32  	%r216, %r482, 2;
	setp.eq.s32	%p40, %r216, 0;
	@%p40 bra 	BB0_58;

	mov.f64 	%fd733, 0d0000000000000000;
	mov.f64 	%fd734, 0dBFF0000000000000;
	fma.rn.f64 	%fd1767, %fd1767, %fd734, %fd733;

BB0_58:
	ld.global.f64 	%fd735, [%rd2+32];
	add.f64 	%fd736, %fd735, %fd735;
	mul.f64 	%fd737, %fd735, %fd736;
	mul.f64 	%fd738, %fd1764, %fd1767;
	div.rn.f64 	%fd739, %fd738, %fd737;
	add.f64 	%fd1722, %fd83, %fd739;
	ld.global.f64 	%fd740, [%rd2+40];
	add.f64 	%fd1768, %fd740, %fd740;
	abs.f64 	%fd741, %fd1768;
	setp.neu.f64	%p41, %fd741, 0d7FF0000000000000;
	@%p41 bra 	BB0_60;

	mov.f64 	%fd742, 0d0000000000000000;
	mul.rn.f64 	%fd1768, %fd1768, %fd742;

BB0_60:
	mul.f64 	%fd743, %fd1768, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r483, %fd743;
	st.local.u32 	[%rd40], %r483;
	cvt.rn.f64.s32	%fd744, %r483;
	neg.f64 	%fd745, %fd744;
	fma.rn.f64 	%fd747, %fd745, %fd636, %fd1768;
	fma.rn.f64 	%fd749, %fd745, %fd638, %fd747;
	fma.rn.f64 	%fd1769, %fd745, %fd640, %fd749;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r217}, %fd1768;
	}
	and.b32  	%r218, %r217, 2145386496;
	setp.lt.u32	%p42, %r218, 1105199104;
	@%p42 bra 	BB0_62;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1768;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1769, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r483, [%rd40];

BB0_62:
	and.b32  	%r219, %r483, 1;
	shl.b32 	%r220, %r219, 3;
	setp.eq.b32	%p43, %r219, 1;
	selp.f64	%fd751, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p43;
	mul.wide.u32 	%rd71, %r220, 8;
	add.s64 	%rd73, %rd71, %rd44;
	ld.const.f64 	%fd752, [%rd73+8];
	mul.rn.f64 	%fd117, %fd1769, %fd1769;
	fma.rn.f64 	%fd753, %fd751, %fd117, %fd752;
	ld.const.f64 	%fd754, [%rd73+16];
	fma.rn.f64 	%fd755, %fd753, %fd117, %fd754;
	ld.const.f64 	%fd756, [%rd73+24];
	fma.rn.f64 	%fd757, %fd755, %fd117, %fd756;
	ld.const.f64 	%fd758, [%rd73+32];
	fma.rn.f64 	%fd759, %fd757, %fd117, %fd758;
	ld.const.f64 	%fd760, [%rd73+40];
	fma.rn.f64 	%fd761, %fd759, %fd117, %fd760;
	ld.const.f64 	%fd762, [%rd73+48];
	fma.rn.f64 	%fd118, %fd761, %fd117, %fd762;
	fma.rn.f64 	%fd1770, %fd118, %fd1769, %fd1769;
	setp.eq.s32	%p44, %r219, 0;
	@%p44 bra 	BB0_64;

	mov.f64 	%fd763, 0d3FF0000000000000;
	fma.rn.f64 	%fd1770, %fd118, %fd117, %fd763;

BB0_64:
	and.b32  	%r221, %r483, 2;
	setp.eq.s32	%p45, %r221, 0;
	@%p45 bra 	BB0_66;

	mov.f64 	%fd764, 0d0000000000000000;
	mov.f64 	%fd765, 0dBFF0000000000000;
	fma.rn.f64 	%fd1770, %fd1770, %fd765, %fd764;

BB0_66:
	ld.global.f64 	%fd766, [%rd2+24];
	mul.f64 	%fd767, %fd766, 0dC010000000000000;
	mul.f64 	%fd768, %fd766, %fd767;
	div.rn.f64 	%fd124, %fd1770, %fd768;
	ld.global.f64 	%fd769, [%rd2+40];
	add.f64 	%fd1771, %fd769, %fd769;
	abs.f64 	%fd770, %fd1771;
	setp.neu.f64	%p46, %fd770, 0d7FF0000000000000;
	@%p46 bra 	BB0_68;

	mov.f64 	%fd771, 0d0000000000000000;
	mul.rn.f64 	%fd1771, %fd1771, %fd771;

BB0_68:
	mul.f64 	%fd772, %fd1771, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r484, %fd772;
	st.local.u32 	[%rd40], %r484;
	cvt.rn.f64.s32	%fd773, %r484;
	neg.f64 	%fd774, %fd773;
	fma.rn.f64 	%fd776, %fd774, %fd636, %fd1771;
	fma.rn.f64 	%fd778, %fd774, %fd638, %fd776;
	fma.rn.f64 	%fd1772, %fd774, %fd640, %fd778;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r222}, %fd1771;
	}
	and.b32  	%r223, %r222, 2145386496;
	setp.lt.u32	%p47, %r223, 1105199104;
	@%p47 bra 	BB0_70;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1771;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1772, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r484, [%rd40];

BB0_70:
	and.b32  	%r224, %r484, 1;
	shl.b32 	%r225, %r224, 3;
	setp.eq.b32	%p48, %r224, 1;
	selp.f64	%fd780, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p48;
	mul.wide.u32 	%rd78, %r225, 8;
	add.s64 	%rd80, %rd78, %rd44;
	ld.const.f64 	%fd781, [%rd80+8];
	mul.rn.f64 	%fd131, %fd1772, %fd1772;
	fma.rn.f64 	%fd782, %fd780, %fd131, %fd781;
	ld.const.f64 	%fd783, [%rd80+16];
	fma.rn.f64 	%fd784, %fd782, %fd131, %fd783;
	ld.const.f64 	%fd785, [%rd80+24];
	fma.rn.f64 	%fd786, %fd784, %fd131, %fd785;
	ld.const.f64 	%fd787, [%rd80+32];
	fma.rn.f64 	%fd788, %fd786, %fd131, %fd787;
	ld.const.f64 	%fd789, [%rd80+40];
	fma.rn.f64 	%fd790, %fd788, %fd131, %fd789;
	ld.const.f64 	%fd791, [%rd80+48];
	fma.rn.f64 	%fd132, %fd790, %fd131, %fd791;
	fma.rn.f64 	%fd1773, %fd132, %fd1772, %fd1772;
	setp.eq.s32	%p49, %r224, 0;
	@%p49 bra 	BB0_72;

	mov.f64 	%fd792, 0d3FF0000000000000;
	fma.rn.f64 	%fd1773, %fd132, %fd131, %fd792;

BB0_72:
	and.b32  	%r226, %r484, 2;
	setp.eq.s32	%p50, %r226, 0;
	@%p50 bra 	BB0_74;

	mov.f64 	%fd793, 0d0000000000000000;
	mov.f64 	%fd794, 0dBFF0000000000000;
	fma.rn.f64 	%fd1773, %fd1773, %fd794, %fd793;

BB0_74:
	ld.global.f64 	%fd795, [%rd2+32];
	mul.f64 	%fd796, %fd795, 0d4010000000000000;
	mul.f64 	%fd797, %fd795, %fd796;
	div.rn.f64 	%fd798, %fd1773, %fd797;
	add.f64 	%fd1721, %fd124, %fd798;
	ld.global.f64 	%fd1774, [%rd2+40];
	abs.f64 	%fd799, %fd1774;
	setp.neu.f64	%p51, %fd799, 0d7FF0000000000000;
	@%p51 bra 	BB0_76;

	mov.f64 	%fd800, 0d0000000000000000;
	mul.rn.f64 	%fd1774, %fd1774, %fd800;

BB0_76:
	mul.f64 	%fd801, %fd1774, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r485, %fd801;
	st.local.u32 	[%rd40], %r485;
	cvt.rn.f64.s32	%fd802, %r485;
	neg.f64 	%fd803, %fd802;
	fma.rn.f64 	%fd805, %fd803, %fd636, %fd1774;
	fma.rn.f64 	%fd807, %fd803, %fd638, %fd805;
	fma.rn.f64 	%fd1775, %fd803, %fd640, %fd807;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r227}, %fd1774;
	}
	and.b32  	%r228, %r227, 2145386496;
	setp.lt.u32	%p52, %r228, 1105199104;
	@%p52 bra 	BB0_78;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1774;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1775, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r485, [%rd40];

BB0_78:
	and.b32  	%r229, %r485, 1;
	shl.b32 	%r230, %r229, 3;
	setp.eq.b32	%p53, %r229, 1;
	selp.f64	%fd809, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p53;
	mul.wide.u32 	%rd85, %r230, 8;
	add.s64 	%rd87, %rd85, %rd44;
	ld.const.f64 	%fd810, [%rd87+8];
	mul.rn.f64 	%fd145, %fd1775, %fd1775;
	fma.rn.f64 	%fd811, %fd809, %fd145, %fd810;
	ld.const.f64 	%fd812, [%rd87+16];
	fma.rn.f64 	%fd813, %fd811, %fd145, %fd812;
	ld.const.f64 	%fd814, [%rd87+24];
	fma.rn.f64 	%fd815, %fd813, %fd145, %fd814;
	ld.const.f64 	%fd816, [%rd87+32];
	fma.rn.f64 	%fd817, %fd815, %fd145, %fd816;
	ld.const.f64 	%fd818, [%rd87+40];
	fma.rn.f64 	%fd819, %fd817, %fd145, %fd818;
	ld.const.f64 	%fd820, [%rd87+48];
	fma.rn.f64 	%fd146, %fd819, %fd145, %fd820;
	fma.rn.f64 	%fd1776, %fd146, %fd1775, %fd1775;
	setp.eq.s32	%p54, %r229, 0;
	@%p54 bra 	BB0_80;

	mov.f64 	%fd821, 0d3FF0000000000000;
	fma.rn.f64 	%fd1776, %fd146, %fd145, %fd821;

BB0_80:
	and.b32  	%r231, %r485, 2;
	setp.eq.s32	%p55, %r231, 0;
	@%p55 bra 	BB0_82;

	mov.f64 	%fd822, 0d0000000000000000;
	mov.f64 	%fd823, 0dBFF0000000000000;
	fma.rn.f64 	%fd1776, %fd1776, %fd823, %fd822;

BB0_82:
	ld.global.f64 	%fd1777, [%rd2+40];
	abs.f64 	%fd824, %fd1777;
	setp.neu.f64	%p56, %fd824, 0d7FF0000000000000;
	@%p56 bra 	BB0_84;

	mov.f64 	%fd825, 0d0000000000000000;
	mul.rn.f64 	%fd1777, %fd1777, %fd825;

BB0_84:
	mul.f64 	%fd826, %fd1777, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r486, %fd826;
	st.local.u32 	[%rd40], %r486;
	cvt.rn.f64.s32	%fd827, %r486;
	neg.f64 	%fd828, %fd827;
	fma.rn.f64 	%fd830, %fd828, %fd636, %fd1777;
	fma.rn.f64 	%fd832, %fd828, %fd638, %fd830;
	fma.rn.f64 	%fd1778, %fd828, %fd640, %fd832;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r232}, %fd1777;
	}
	and.b32  	%r233, %r232, 2145386496;
	setp.lt.u32	%p57, %r233, 1105199104;
	@%p57 bra 	BB0_86;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1777;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1778, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r486, [%rd40];

BB0_86:
	and.b32  	%r234, %r486, 1;
	shl.b32 	%r235, %r234, 3;
	setp.eq.b32	%p58, %r234, 1;
	selp.f64	%fd834, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p58;
	mul.wide.u32 	%rd92, %r235, 8;
	add.s64 	%rd94, %rd92, %rd44;
	ld.const.f64 	%fd835, [%rd94+8];
	mul.rn.f64 	%fd158, %fd1778, %fd1778;
	fma.rn.f64 	%fd836, %fd834, %fd158, %fd835;
	ld.const.f64 	%fd837, [%rd94+16];
	fma.rn.f64 	%fd838, %fd836, %fd158, %fd837;
	ld.const.f64 	%fd839, [%rd94+24];
	fma.rn.f64 	%fd840, %fd838, %fd158, %fd839;
	ld.const.f64 	%fd841, [%rd94+32];
	fma.rn.f64 	%fd842, %fd840, %fd158, %fd841;
	ld.const.f64 	%fd843, [%rd94+40];
	fma.rn.f64 	%fd844, %fd842, %fd158, %fd843;
	ld.const.f64 	%fd845, [%rd94+48];
	fma.rn.f64 	%fd159, %fd844, %fd158, %fd845;
	fma.rn.f64 	%fd1779, %fd159, %fd1778, %fd1778;
	setp.eq.s32	%p59, %r234, 0;
	@%p59 bra 	BB0_88;

	mov.f64 	%fd846, 0d3FF0000000000000;
	fma.rn.f64 	%fd1779, %fd159, %fd158, %fd846;

BB0_88:
	and.b32  	%r236, %r486, 2;
	setp.eq.s32	%p60, %r236, 0;
	@%p60 bra 	BB0_90;

	mov.f64 	%fd847, 0d0000000000000000;
	mov.f64 	%fd848, 0dBFF0000000000000;
	fma.rn.f64 	%fd1779, %fd1779, %fd848, %fd847;

BB0_90:
	ld.global.f64 	%fd849, [%rd2+24];
	add.f64 	%fd850, %fd849, %fd849;
	mul.f64 	%fd851, %fd849, %fd850;
	mul.f64 	%fd852, %fd1776, %fd1779;
	div.rn.f64 	%fd165, %fd852, %fd851;
	ld.global.f64 	%fd1780, [%rd2+40];
	abs.f64 	%fd853, %fd1780;
	setp.neu.f64	%p61, %fd853, 0d7FF0000000000000;
	@%p61 bra 	BB0_92;

	mov.f64 	%fd854, 0d0000000000000000;
	mul.rn.f64 	%fd1780, %fd1780, %fd854;

BB0_92:
	mul.f64 	%fd855, %fd1780, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r487, %fd855;
	st.local.u32 	[%rd40], %r487;
	cvt.rn.f64.s32	%fd856, %r487;
	neg.f64 	%fd857, %fd856;
	fma.rn.f64 	%fd859, %fd857, %fd636, %fd1780;
	fma.rn.f64 	%fd861, %fd857, %fd638, %fd859;
	fma.rn.f64 	%fd1781, %fd857, %fd640, %fd861;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r237}, %fd1780;
	}
	and.b32  	%r238, %r237, 2145386496;
	setp.lt.u32	%p62, %r238, 1105199104;
	@%p62 bra 	BB0_94;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1780;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1781, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r487, [%rd40];

BB0_94:
	add.s32 	%r46, %r487, 1;
	and.b32  	%r239, %r46, 1;
	shl.b32 	%r240, %r239, 3;
	setp.eq.b32	%p63, %r239, 1;
	selp.f64	%fd863, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p63;
	mul.wide.u32 	%rd99, %r240, 8;
	add.s64 	%rd101, %rd99, %rd44;
	ld.const.f64 	%fd864, [%rd101+8];
	mul.rn.f64 	%fd172, %fd1781, %fd1781;
	fma.rn.f64 	%fd865, %fd863, %fd172, %fd864;
	ld.const.f64 	%fd866, [%rd101+16];
	fma.rn.f64 	%fd867, %fd865, %fd172, %fd866;
	ld.const.f64 	%fd868, [%rd101+24];
	fma.rn.f64 	%fd869, %fd867, %fd172, %fd868;
	ld.const.f64 	%fd870, [%rd101+32];
	fma.rn.f64 	%fd871, %fd869, %fd172, %fd870;
	ld.const.f64 	%fd872, [%rd101+40];
	fma.rn.f64 	%fd873, %fd871, %fd172, %fd872;
	ld.const.f64 	%fd874, [%rd101+48];
	fma.rn.f64 	%fd173, %fd873, %fd172, %fd874;
	fma.rn.f64 	%fd1782, %fd173, %fd1781, %fd1781;
	setp.eq.s32	%p64, %r239, 0;
	@%p64 bra 	BB0_96;

	mov.f64 	%fd875, 0d3FF0000000000000;
	fma.rn.f64 	%fd1782, %fd173, %fd172, %fd875;

BB0_96:
	and.b32  	%r241, %r46, 2;
	setp.eq.s32	%p65, %r241, 0;
	@%p65 bra 	BB0_98;

	mov.f64 	%fd876, 0d0000000000000000;
	mov.f64 	%fd877, 0dBFF0000000000000;
	fma.rn.f64 	%fd1782, %fd1782, %fd877, %fd876;

BB0_98:
	ld.global.f64 	%fd1783, [%rd2+40];
	abs.f64 	%fd878, %fd1783;
	setp.neu.f64	%p66, %fd878, 0d7FF0000000000000;
	@%p66 bra 	BB0_100;

	mov.f64 	%fd879, 0d0000000000000000;
	mul.rn.f64 	%fd1783, %fd1783, %fd879;

BB0_100:
	mul.f64 	%fd880, %fd1783, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r488, %fd880;
	st.local.u32 	[%rd40], %r488;
	cvt.rn.f64.s32	%fd881, %r488;
	neg.f64 	%fd882, %fd881;
	fma.rn.f64 	%fd884, %fd882, %fd636, %fd1783;
	fma.rn.f64 	%fd886, %fd882, %fd638, %fd884;
	fma.rn.f64 	%fd1784, %fd882, %fd640, %fd886;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r242}, %fd1783;
	}
	and.b32  	%r243, %r242, 2145386496;
	setp.lt.u32	%p67, %r243, 1105199104;
	@%p67 bra 	BB0_102;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1783;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1784, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r488, [%rd40];

BB0_102:
	add.s32 	%r50, %r488, 1;
	and.b32  	%r244, %r50, 1;
	shl.b32 	%r245, %r244, 3;
	setp.eq.b32	%p68, %r244, 1;
	selp.f64	%fd888, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p68;
	mul.wide.u32 	%rd106, %r245, 8;
	add.s64 	%rd108, %rd106, %rd44;
	ld.const.f64 	%fd889, [%rd108+8];
	mul.rn.f64 	%fd185, %fd1784, %fd1784;
	fma.rn.f64 	%fd890, %fd888, %fd185, %fd889;
	ld.const.f64 	%fd891, [%rd108+16];
	fma.rn.f64 	%fd892, %fd890, %fd185, %fd891;
	ld.const.f64 	%fd893, [%rd108+24];
	fma.rn.f64 	%fd894, %fd892, %fd185, %fd893;
	ld.const.f64 	%fd895, [%rd108+32];
	fma.rn.f64 	%fd896, %fd894, %fd185, %fd895;
	ld.const.f64 	%fd897, [%rd108+40];
	fma.rn.f64 	%fd898, %fd896, %fd185, %fd897;
	ld.const.f64 	%fd899, [%rd108+48];
	fma.rn.f64 	%fd186, %fd898, %fd185, %fd899;
	fma.rn.f64 	%fd1785, %fd186, %fd1784, %fd1784;
	setp.eq.s32	%p69, %r244, 0;
	@%p69 bra 	BB0_104;

	mov.f64 	%fd900, 0d3FF0000000000000;
	fma.rn.f64 	%fd1785, %fd186, %fd185, %fd900;

BB0_104:
	and.b32  	%r246, %r50, 2;
	setp.eq.s32	%p70, %r246, 0;
	@%p70 bra 	BB0_106;

	mov.f64 	%fd901, 0d0000000000000000;
	mov.f64 	%fd902, 0dBFF0000000000000;
	fma.rn.f64 	%fd1785, %fd1785, %fd902, %fd901;

BB0_106:
	ld.global.f64 	%fd904, [%rd2+32];
	add.f64 	%fd905, %fd904, %fd904;
	mul.f64 	%fd906, %fd904, %fd905;
	mul.f64 	%fd907, %fd1782, %fd1785;
	div.rn.f64 	%fd908, %fd907, %fd906;
	add.f64 	%fd1720, %fd165, %fd908;
	mov.f64 	%fd1787, 0d0000000000000000;
	@%p2 bra 	BB0_112;

	ld.global.f64 	%fd193, [%rd2];
	ld.global.f64 	%fd194, [%rd2+8];
	ld.global.f64 	%fd196, [%rd2+16];
	ld.global.f64 	%fd197, [%rd2+48];
	mov.f64 	%fd1787, 0d0000000000000000;
	mov.u32 	%r489, 0;

BB0_108:
	add.f64 	%fd1692, %fd1721, %fd1721;
	rem.s32 	%r256, %r489, %r2;
	div.s32 	%r257, %r489, %r2;
	cvt.rn.f64.s32	%fd910, %r256;
	sub.f64 	%fd911, %fd910, %fd194;
	mul.f64 	%fd912, %fd1722, %fd911;
	mul.f64 	%fd913, %fd911, %fd912;
	mul.f64 	%fd914, %fd1692, %fd911;
	cvt.rn.f64.s32	%fd915, %r257;
	sub.f64 	%fd916, %fd915, %fd196;
	mul.f64 	%fd917, %fd914, %fd916;
	sub.f64 	%fd918, %fd913, %fd917;
	mul.f64 	%fd919, %fd1720, %fd916;
	fma.rn.f64 	%fd199, %fd916, %fd919, %fd918;
	neg.f64 	%fd920, %fd199;
	mov.f64 	%fd921, 0d4338000000000000;
	mov.f64 	%fd922, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd923, %fd920, %fd922, %fd921;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd923;
	}
	mov.f64 	%fd924, 0dC338000000000000;
	add.rn.f64 	%fd925, %fd923, %fd924;
	mov.f64 	%fd926, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd927, %fd925, %fd926, %fd920;
	mov.f64 	%fd928, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd929, %fd925, %fd928, %fd927;
	mov.f64 	%fd930, 0d3E928AF3FCA213EA;
	mov.f64 	%fd931, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd932, %fd931, %fd929, %fd930;
	mov.f64 	%fd933, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd934, %fd932, %fd929, %fd933;
	mov.f64 	%fd935, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd936, %fd934, %fd929, %fd935;
	mov.f64 	%fd937, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd938, %fd936, %fd929, %fd937;
	mov.f64 	%fd939, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd940, %fd938, %fd929, %fd939;
	mov.f64 	%fd941, 0d3F81111111122322;
	fma.rn.f64 	%fd942, %fd940, %fd929, %fd941;
	mov.f64 	%fd943, 0d3FA55555555502A1;
	fma.rn.f64 	%fd944, %fd942, %fd929, %fd943;
	mov.f64 	%fd945, 0d3FC5555555555511;
	fma.rn.f64 	%fd946, %fd944, %fd929, %fd945;
	mov.f64 	%fd947, 0d3FE000000000000B;
	fma.rn.f64 	%fd948, %fd946, %fd929, %fd947;
	mov.f64 	%fd949, 0d3FF0000000000000;
	fma.rn.f64 	%fd950, %fd948, %fd929, %fd949;
	fma.rn.f64 	%fd951, %fd950, %fd929, %fd949;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd951;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd951;
	}
	shl.b32 	%r258, %r53, 20;
	add.s32 	%r259, %r55, %r258;
	mov.b64 	%fd1786, {%r54, %r259};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r260}, %fd920;
	}
	mov.b32 	 %f7, %r260;
	abs.f32 	%f2, %f7;
	setp.lt.f32	%p72, %f2, 0f4086232B;
	@%p72 bra 	BB0_111;

	setp.gt.f64	%p73, %fd199, 0d8000000000000000;
	mov.f64 	%fd952, 0d7FF0000000000000;
	sub.f64 	%fd953, %fd952, %fd199;
	selp.f64	%fd1786, 0d0000000000000000, %fd953, %p73;
	setp.geu.f32	%p74, %f2, 0f40874800;
	@%p74 bra 	BB0_111;

	shr.u32 	%r261, %r53, 31;
	add.s32 	%r262, %r53, %r261;
	shr.s32 	%r263, %r262, 1;
	shl.b32 	%r264, %r263, 20;
	add.s32 	%r265, %r264, %r55;
	mov.b64 	%fd954, {%r54, %r265};
	sub.s32 	%r266, %r53, %r263;
	shl.b32 	%r267, %r266, 20;
	add.s32 	%r268, %r267, 1072693248;
	mov.u32 	%r269, 0;
	mov.b64 	%fd955, {%r269, %r268};
	mul.f64 	%fd1786, %fd954, %fd955;

BB0_111:
	fma.rn.f64 	%fd956, %fd193, %fd1786, %fd197;
	add.s32 	%r270, %r489, %r5;
	mul.wide.s32 	%rd113, %r270, 4;
	add.s64 	%rd114, %rd1, %rd113;
	ld.global.u32 	%r271, [%rd114];
	cvt.rn.f64.s32	%fd957, %r271;
	sub.f64 	%fd958, %fd956, %fd957;
	fma.rn.f64 	%fd1787, %fd958, %fd958, %fd1787;
	add.s32 	%r489, %r489, 1;
	setp.lt.s32	%p75, %r489, %r3;
	@%p75 bra 	BB0_108;

BB0_112:
	div.rn.f64 	%fd1755, %fd1787, %fd1703;
	setp.lt.f64	%p76, %fd1755, %fd54;
	mov.f64 	%fd1753, %fd53;
	@%p76 bra 	BB0_225;

	ld.global.f64 	%fd959, [%rd5];
	ld.global.f64 	%fd960, [%rd4];
	sub.f64 	%fd961, %fd960, %fd959;
	st.global.f64 	[%rd4], %fd961;
	ld.global.f64 	%fd1788, [%rd2+40];
	abs.f64 	%fd962, %fd1788;
	setp.neu.f64	%p77, %fd962, 0d7FF0000000000000;
	@%p77 bra 	BB0_115;

	mov.f64 	%fd963, 0d0000000000000000;
	mul.rn.f64 	%fd1788, %fd1788, %fd963;

BB0_115:
	mul.f64 	%fd964, %fd1788, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r490, %fd964;
	st.local.u32 	[%rd40], %r490;
	cvt.rn.f64.s32	%fd965, %r490;
	neg.f64 	%fd966, %fd965;
	fma.rn.f64 	%fd968, %fd966, %fd636, %fd1788;
	fma.rn.f64 	%fd970, %fd966, %fd638, %fd968;
	fma.rn.f64 	%fd1789, %fd966, %fd640, %fd970;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r273}, %fd1788;
	}
	and.b32  	%r274, %r273, 2145386496;
	setp.lt.u32	%p78, %r274, 1105199104;
	@%p78 bra 	BB0_117;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1788;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1789, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r490, [%rd40];

BB0_117:
	add.s32 	%r60, %r490, 1;
	and.b32  	%r275, %r60, 1;
	shl.b32 	%r276, %r275, 3;
	setp.eq.b32	%p79, %r275, 1;
	selp.f64	%fd972, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p79;
	mul.wide.u32 	%rd119, %r276, 8;
	add.s64 	%rd121, %rd119, %rd44;
	ld.const.f64 	%fd973, [%rd121+8];
	mul.rn.f64 	%fd213, %fd1789, %fd1789;
	fma.rn.f64 	%fd974, %fd972, %fd213, %fd973;
	ld.const.f64 	%fd975, [%rd121+16];
	fma.rn.f64 	%fd976, %fd974, %fd213, %fd975;
	ld.const.f64 	%fd977, [%rd121+24];
	fma.rn.f64 	%fd978, %fd976, %fd213, %fd977;
	ld.const.f64 	%fd979, [%rd121+32];
	fma.rn.f64 	%fd980, %fd978, %fd213, %fd979;
	ld.const.f64 	%fd981, [%rd121+40];
	fma.rn.f64 	%fd982, %fd980, %fd213, %fd981;
	ld.const.f64 	%fd983, [%rd121+48];
	fma.rn.f64 	%fd214, %fd982, %fd213, %fd983;
	fma.rn.f64 	%fd1790, %fd214, %fd1789, %fd1789;
	setp.eq.s32	%p80, %r275, 0;
	@%p80 bra 	BB0_119;

	mov.f64 	%fd984, 0d3FF0000000000000;
	fma.rn.f64 	%fd1790, %fd214, %fd213, %fd984;

BB0_119:
	and.b32  	%r277, %r60, 2;
	setp.eq.s32	%p81, %r277, 0;
	@%p81 bra 	BB0_121;

	mov.f64 	%fd985, 0d0000000000000000;
	mov.f64 	%fd986, 0dBFF0000000000000;
	fma.rn.f64 	%fd1790, %fd1790, %fd986, %fd985;

BB0_121:
	ld.global.f64 	%fd1791, [%rd2+40];
	abs.f64 	%fd987, %fd1791;
	setp.neu.f64	%p82, %fd987, 0d7FF0000000000000;
	@%p82 bra 	BB0_123;

	mov.f64 	%fd988, 0d0000000000000000;
	mul.rn.f64 	%fd1791, %fd1791, %fd988;

BB0_123:
	mul.f64 	%fd989, %fd1791, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r491, %fd989;
	st.local.u32 	[%rd40], %r491;
	cvt.rn.f64.s32	%fd990, %r491;
	neg.f64 	%fd991, %fd990;
	fma.rn.f64 	%fd993, %fd991, %fd636, %fd1791;
	fma.rn.f64 	%fd995, %fd991, %fd638, %fd993;
	fma.rn.f64 	%fd1792, %fd991, %fd640, %fd995;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r278}, %fd1791;
	}
	and.b32  	%r279, %r278, 2145386496;
	setp.lt.u32	%p83, %r279, 1105199104;
	@%p83 bra 	BB0_125;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1791;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1792, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r491, [%rd40];

BB0_125:
	add.s32 	%r64, %r491, 1;
	and.b32  	%r280, %r64, 1;
	shl.b32 	%r281, %r280, 3;
	setp.eq.b32	%p84, %r280, 1;
	selp.f64	%fd997, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p84;
	mul.wide.u32 	%rd126, %r281, 8;
	add.s64 	%rd128, %rd126, %rd44;
	ld.const.f64 	%fd998, [%rd128+8];
	mul.rn.f64 	%fd226, %fd1792, %fd1792;
	fma.rn.f64 	%fd999, %fd997, %fd226, %fd998;
	ld.const.f64 	%fd1000, [%rd128+16];
	fma.rn.f64 	%fd1001, %fd999, %fd226, %fd1000;
	ld.const.f64 	%fd1002, [%rd128+24];
	fma.rn.f64 	%fd1003, %fd1001, %fd226, %fd1002;
	ld.const.f64 	%fd1004, [%rd128+32];
	fma.rn.f64 	%fd1005, %fd1003, %fd226, %fd1004;
	ld.const.f64 	%fd1006, [%rd128+40];
	fma.rn.f64 	%fd1007, %fd1005, %fd226, %fd1006;
	ld.const.f64 	%fd1008, [%rd128+48];
	fma.rn.f64 	%fd227, %fd1007, %fd226, %fd1008;
	fma.rn.f64 	%fd1793, %fd227, %fd1792, %fd1792;
	setp.eq.s32	%p85, %r280, 0;
	@%p85 bra 	BB0_127;

	mov.f64 	%fd1009, 0d3FF0000000000000;
	fma.rn.f64 	%fd1793, %fd227, %fd226, %fd1009;

BB0_127:
	and.b32  	%r282, %r64, 2;
	setp.eq.s32	%p86, %r282, 0;
	@%p86 bra 	BB0_129;

	mov.f64 	%fd1010, 0d0000000000000000;
	mov.f64 	%fd1011, 0dBFF0000000000000;
	fma.rn.f64 	%fd1793, %fd1793, %fd1011, %fd1010;

BB0_129:
	ld.global.f64 	%fd1012, [%rd2+24];
	add.f64 	%fd1013, %fd1012, %fd1012;
	mul.f64 	%fd1014, %fd1012, %fd1013;
	mul.f64 	%fd1015, %fd1790, %fd1793;
	div.rn.f64 	%fd233, %fd1015, %fd1014;
	ld.global.f64 	%fd1794, [%rd2+40];
	abs.f64 	%fd1016, %fd1794;
	setp.neu.f64	%p87, %fd1016, 0d7FF0000000000000;
	@%p87 bra 	BB0_131;

	mov.f64 	%fd1017, 0d0000000000000000;
	mul.rn.f64 	%fd1794, %fd1794, %fd1017;

BB0_131:
	mul.f64 	%fd1018, %fd1794, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r492, %fd1018;
	st.local.u32 	[%rd40], %r492;
	cvt.rn.f64.s32	%fd1019, %r492;
	neg.f64 	%fd1020, %fd1019;
	fma.rn.f64 	%fd1022, %fd1020, %fd636, %fd1794;
	fma.rn.f64 	%fd1024, %fd1020, %fd638, %fd1022;
	fma.rn.f64 	%fd1795, %fd1020, %fd640, %fd1024;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r283}, %fd1794;
	}
	and.b32  	%r284, %r283, 2145386496;
	setp.lt.u32	%p88, %r284, 1105199104;
	@%p88 bra 	BB0_133;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1794;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1795, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r492, [%rd40];

BB0_133:
	and.b32  	%r285, %r492, 1;
	shl.b32 	%r286, %r285, 3;
	setp.eq.b32	%p89, %r285, 1;
	selp.f64	%fd1026, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p89;
	mul.wide.u32 	%rd133, %r286, 8;
	add.s64 	%rd135, %rd133, %rd44;
	ld.const.f64 	%fd1027, [%rd135+8];
	mul.rn.f64 	%fd240, %fd1795, %fd1795;
	fma.rn.f64 	%fd1028, %fd1026, %fd240, %fd1027;
	ld.const.f64 	%fd1029, [%rd135+16];
	fma.rn.f64 	%fd1030, %fd1028, %fd240, %fd1029;
	ld.const.f64 	%fd1031, [%rd135+24];
	fma.rn.f64 	%fd1032, %fd1030, %fd240, %fd1031;
	ld.const.f64 	%fd1033, [%rd135+32];
	fma.rn.f64 	%fd1034, %fd1032, %fd240, %fd1033;
	ld.const.f64 	%fd1035, [%rd135+40];
	fma.rn.f64 	%fd1036, %fd1034, %fd240, %fd1035;
	ld.const.f64 	%fd1037, [%rd135+48];
	fma.rn.f64 	%fd241, %fd1036, %fd240, %fd1037;
	fma.rn.f64 	%fd1796, %fd241, %fd1795, %fd1795;
	setp.eq.s32	%p90, %r285, 0;
	@%p90 bra 	BB0_135;

	mov.f64 	%fd1038, 0d3FF0000000000000;
	fma.rn.f64 	%fd1796, %fd241, %fd240, %fd1038;

BB0_135:
	and.b32  	%r287, %r492, 2;
	setp.eq.s32	%p91, %r287, 0;
	@%p91 bra 	BB0_137;

	mov.f64 	%fd1039, 0d0000000000000000;
	mov.f64 	%fd1040, 0dBFF0000000000000;
	fma.rn.f64 	%fd1796, %fd1796, %fd1040, %fd1039;

BB0_137:
	ld.global.f64 	%fd1797, [%rd2+40];
	abs.f64 	%fd1041, %fd1797;
	setp.neu.f64	%p92, %fd1041, 0d7FF0000000000000;
	@%p92 bra 	BB0_139;

	mov.f64 	%fd1042, 0d0000000000000000;
	mul.rn.f64 	%fd1797, %fd1797, %fd1042;

BB0_139:
	mul.f64 	%fd1043, %fd1797, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r493, %fd1043;
	st.local.u32 	[%rd40], %r493;
	cvt.rn.f64.s32	%fd1044, %r493;
	neg.f64 	%fd1045, %fd1044;
	fma.rn.f64 	%fd1047, %fd1045, %fd636, %fd1797;
	fma.rn.f64 	%fd1049, %fd1045, %fd638, %fd1047;
	fma.rn.f64 	%fd1798, %fd1045, %fd640, %fd1049;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r288}, %fd1797;
	}
	and.b32  	%r289, %r288, 2145386496;
	setp.lt.u32	%p93, %r289, 1105199104;
	@%p93 bra 	BB0_141;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1797;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1798, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r493, [%rd40];

BB0_141:
	and.b32  	%r290, %r493, 1;
	shl.b32 	%r291, %r290, 3;
	setp.eq.b32	%p94, %r290, 1;
	selp.f64	%fd1051, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p94;
	mul.wide.u32 	%rd140, %r291, 8;
	add.s64 	%rd142, %rd140, %rd44;
	ld.const.f64 	%fd1052, [%rd142+8];
	mul.rn.f64 	%fd253, %fd1798, %fd1798;
	fma.rn.f64 	%fd1053, %fd1051, %fd253, %fd1052;
	ld.const.f64 	%fd1054, [%rd142+16];
	fma.rn.f64 	%fd1055, %fd1053, %fd253, %fd1054;
	ld.const.f64 	%fd1056, [%rd142+24];
	fma.rn.f64 	%fd1057, %fd1055, %fd253, %fd1056;
	ld.const.f64 	%fd1058, [%rd142+32];
	fma.rn.f64 	%fd1059, %fd1057, %fd253, %fd1058;
	ld.const.f64 	%fd1060, [%rd142+40];
	fma.rn.f64 	%fd1061, %fd1059, %fd253, %fd1060;
	ld.const.f64 	%fd1062, [%rd142+48];
	fma.rn.f64 	%fd254, %fd1061, %fd253, %fd1062;
	fma.rn.f64 	%fd1799, %fd254, %fd1798, %fd1798;
	setp.eq.s32	%p95, %r290, 0;
	@%p95 bra 	BB0_143;

	mov.f64 	%fd1063, 0d3FF0000000000000;
	fma.rn.f64 	%fd1799, %fd254, %fd253, %fd1063;

BB0_143:
	and.b32  	%r292, %r493, 2;
	setp.eq.s32	%p96, %r292, 0;
	@%p96 bra 	BB0_145;

	mov.f64 	%fd1064, 0d0000000000000000;
	mov.f64 	%fd1065, 0dBFF0000000000000;
	fma.rn.f64 	%fd1799, %fd1799, %fd1065, %fd1064;

BB0_145:
	ld.global.f64 	%fd1066, [%rd2+32];
	add.f64 	%fd1067, %fd1066, %fd1066;
	mul.f64 	%fd1068, %fd1066, %fd1067;
	mul.f64 	%fd1069, %fd1796, %fd1799;
	div.rn.f64 	%fd1070, %fd1069, %fd1068;
	add.f64 	%fd1722, %fd233, %fd1070;
	ld.global.f64 	%fd1071, [%rd2+40];
	add.f64 	%fd1800, %fd1071, %fd1071;
	abs.f64 	%fd1072, %fd1800;
	setp.neu.f64	%p97, %fd1072, 0d7FF0000000000000;
	@%p97 bra 	BB0_147;

	mov.f64 	%fd1073, 0d0000000000000000;
	mul.rn.f64 	%fd1800, %fd1800, %fd1073;

BB0_147:
	mul.f64 	%fd1074, %fd1800, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r494, %fd1074;
	st.local.u32 	[%rd40], %r494;
	cvt.rn.f64.s32	%fd1075, %r494;
	neg.f64 	%fd1076, %fd1075;
	fma.rn.f64 	%fd1078, %fd1076, %fd636, %fd1800;
	fma.rn.f64 	%fd1080, %fd1076, %fd638, %fd1078;
	fma.rn.f64 	%fd1801, %fd1076, %fd640, %fd1080;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r293}, %fd1800;
	}
	and.b32  	%r294, %r293, 2145386496;
	setp.lt.u32	%p98, %r294, 1105199104;
	@%p98 bra 	BB0_149;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1800;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1801, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r494, [%rd40];

BB0_149:
	and.b32  	%r295, %r494, 1;
	shl.b32 	%r296, %r295, 3;
	setp.eq.b32	%p99, %r295, 1;
	selp.f64	%fd1082, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p99;
	mul.wide.u32 	%rd147, %r296, 8;
	add.s64 	%rd149, %rd147, %rd44;
	ld.const.f64 	%fd1083, [%rd149+8];
	mul.rn.f64 	%fd267, %fd1801, %fd1801;
	fma.rn.f64 	%fd1084, %fd1082, %fd267, %fd1083;
	ld.const.f64 	%fd1085, [%rd149+16];
	fma.rn.f64 	%fd1086, %fd1084, %fd267, %fd1085;
	ld.const.f64 	%fd1087, [%rd149+24];
	fma.rn.f64 	%fd1088, %fd1086, %fd267, %fd1087;
	ld.const.f64 	%fd1089, [%rd149+32];
	fma.rn.f64 	%fd1090, %fd1088, %fd267, %fd1089;
	ld.const.f64 	%fd1091, [%rd149+40];
	fma.rn.f64 	%fd1092, %fd1090, %fd267, %fd1091;
	ld.const.f64 	%fd1093, [%rd149+48];
	fma.rn.f64 	%fd268, %fd1092, %fd267, %fd1093;
	fma.rn.f64 	%fd1802, %fd268, %fd1801, %fd1801;
	setp.eq.s32	%p100, %r295, 0;
	@%p100 bra 	BB0_151;

	mov.f64 	%fd1094, 0d3FF0000000000000;
	fma.rn.f64 	%fd1802, %fd268, %fd267, %fd1094;

BB0_151:
	and.b32  	%r297, %r494, 2;
	setp.eq.s32	%p101, %r297, 0;
	@%p101 bra 	BB0_153;

	mov.f64 	%fd1095, 0d0000000000000000;
	mov.f64 	%fd1096, 0dBFF0000000000000;
	fma.rn.f64 	%fd1802, %fd1802, %fd1096, %fd1095;

BB0_153:
	ld.global.f64 	%fd1097, [%rd2+24];
	mul.f64 	%fd1098, %fd1097, 0dC010000000000000;
	mul.f64 	%fd1099, %fd1097, %fd1098;
	div.rn.f64 	%fd274, %fd1802, %fd1099;
	ld.global.f64 	%fd1100, [%rd2+40];
	add.f64 	%fd1803, %fd1100, %fd1100;
	abs.f64 	%fd1101, %fd1803;
	setp.neu.f64	%p102, %fd1101, 0d7FF0000000000000;
	@%p102 bra 	BB0_155;

	mov.f64 	%fd1102, 0d0000000000000000;
	mul.rn.f64 	%fd1803, %fd1803, %fd1102;

BB0_155:
	mul.f64 	%fd1103, %fd1803, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r495, %fd1103;
	st.local.u32 	[%rd40], %r495;
	cvt.rn.f64.s32	%fd1104, %r495;
	neg.f64 	%fd1105, %fd1104;
	fma.rn.f64 	%fd1107, %fd1105, %fd636, %fd1803;
	fma.rn.f64 	%fd1109, %fd1105, %fd638, %fd1107;
	fma.rn.f64 	%fd1804, %fd1105, %fd640, %fd1109;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r298}, %fd1803;
	}
	and.b32  	%r299, %r298, 2145386496;
	setp.lt.u32	%p103, %r299, 1105199104;
	@%p103 bra 	BB0_157;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1803;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1804, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r495, [%rd40];

BB0_157:
	and.b32  	%r300, %r495, 1;
	shl.b32 	%r301, %r300, 3;
	setp.eq.b32	%p104, %r300, 1;
	selp.f64	%fd1111, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p104;
	mul.wide.u32 	%rd154, %r301, 8;
	add.s64 	%rd156, %rd154, %rd44;
	ld.const.f64 	%fd1112, [%rd156+8];
	mul.rn.f64 	%fd281, %fd1804, %fd1804;
	fma.rn.f64 	%fd1113, %fd1111, %fd281, %fd1112;
	ld.const.f64 	%fd1114, [%rd156+16];
	fma.rn.f64 	%fd1115, %fd1113, %fd281, %fd1114;
	ld.const.f64 	%fd1116, [%rd156+24];
	fma.rn.f64 	%fd1117, %fd1115, %fd281, %fd1116;
	ld.const.f64 	%fd1118, [%rd156+32];
	fma.rn.f64 	%fd1119, %fd1117, %fd281, %fd1118;
	ld.const.f64 	%fd1120, [%rd156+40];
	fma.rn.f64 	%fd1121, %fd1119, %fd281, %fd1120;
	ld.const.f64 	%fd1122, [%rd156+48];
	fma.rn.f64 	%fd282, %fd1121, %fd281, %fd1122;
	fma.rn.f64 	%fd1805, %fd282, %fd1804, %fd1804;
	setp.eq.s32	%p105, %r300, 0;
	@%p105 bra 	BB0_159;

	mov.f64 	%fd1123, 0d3FF0000000000000;
	fma.rn.f64 	%fd1805, %fd282, %fd281, %fd1123;

BB0_159:
	and.b32  	%r302, %r495, 2;
	setp.eq.s32	%p106, %r302, 0;
	@%p106 bra 	BB0_161;

	mov.f64 	%fd1124, 0d0000000000000000;
	mov.f64 	%fd1125, 0dBFF0000000000000;
	fma.rn.f64 	%fd1805, %fd1805, %fd1125, %fd1124;

BB0_161:
	ld.global.f64 	%fd1126, [%rd2+32];
	mul.f64 	%fd1127, %fd1126, 0d4010000000000000;
	mul.f64 	%fd1128, %fd1126, %fd1127;
	div.rn.f64 	%fd1129, %fd1805, %fd1128;
	add.f64 	%fd1721, %fd274, %fd1129;
	ld.global.f64 	%fd1806, [%rd2+40];
	abs.f64 	%fd1130, %fd1806;
	setp.neu.f64	%p107, %fd1130, 0d7FF0000000000000;
	@%p107 bra 	BB0_163;

	mov.f64 	%fd1131, 0d0000000000000000;
	mul.rn.f64 	%fd1806, %fd1806, %fd1131;

BB0_163:
	mul.f64 	%fd1132, %fd1806, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r496, %fd1132;
	st.local.u32 	[%rd40], %r496;
	cvt.rn.f64.s32	%fd1133, %r496;
	neg.f64 	%fd1134, %fd1133;
	fma.rn.f64 	%fd1136, %fd1134, %fd636, %fd1806;
	fma.rn.f64 	%fd1138, %fd1134, %fd638, %fd1136;
	fma.rn.f64 	%fd1807, %fd1134, %fd640, %fd1138;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r303}, %fd1806;
	}
	and.b32  	%r304, %r303, 2145386496;
	setp.lt.u32	%p108, %r304, 1105199104;
	@%p108 bra 	BB0_165;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1806;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1807, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r496, [%rd40];

BB0_165:
	and.b32  	%r305, %r496, 1;
	shl.b32 	%r306, %r305, 3;
	setp.eq.b32	%p109, %r305, 1;
	selp.f64	%fd1140, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p109;
	mul.wide.u32 	%rd161, %r306, 8;
	add.s64 	%rd163, %rd161, %rd44;
	ld.const.f64 	%fd1141, [%rd163+8];
	mul.rn.f64 	%fd295, %fd1807, %fd1807;
	fma.rn.f64 	%fd1142, %fd1140, %fd295, %fd1141;
	ld.const.f64 	%fd1143, [%rd163+16];
	fma.rn.f64 	%fd1144, %fd1142, %fd295, %fd1143;
	ld.const.f64 	%fd1145, [%rd163+24];
	fma.rn.f64 	%fd1146, %fd1144, %fd295, %fd1145;
	ld.const.f64 	%fd1147, [%rd163+32];
	fma.rn.f64 	%fd1148, %fd1146, %fd295, %fd1147;
	ld.const.f64 	%fd1149, [%rd163+40];
	fma.rn.f64 	%fd1150, %fd1148, %fd295, %fd1149;
	ld.const.f64 	%fd1151, [%rd163+48];
	fma.rn.f64 	%fd296, %fd1150, %fd295, %fd1151;
	fma.rn.f64 	%fd1808, %fd296, %fd1807, %fd1807;
	setp.eq.s32	%p110, %r305, 0;
	@%p110 bra 	BB0_167;

	mov.f64 	%fd1152, 0d3FF0000000000000;
	fma.rn.f64 	%fd1808, %fd296, %fd295, %fd1152;

BB0_167:
	and.b32  	%r307, %r496, 2;
	setp.eq.s32	%p111, %r307, 0;
	@%p111 bra 	BB0_169;

	mov.f64 	%fd1153, 0d0000000000000000;
	mov.f64 	%fd1154, 0dBFF0000000000000;
	fma.rn.f64 	%fd1808, %fd1808, %fd1154, %fd1153;

BB0_169:
	ld.global.f64 	%fd1809, [%rd2+40];
	abs.f64 	%fd1155, %fd1809;
	setp.neu.f64	%p112, %fd1155, 0d7FF0000000000000;
	@%p112 bra 	BB0_171;

	mov.f64 	%fd1156, 0d0000000000000000;
	mul.rn.f64 	%fd1809, %fd1809, %fd1156;

BB0_171:
	mul.f64 	%fd1157, %fd1809, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r497, %fd1157;
	st.local.u32 	[%rd40], %r497;
	cvt.rn.f64.s32	%fd1158, %r497;
	neg.f64 	%fd1159, %fd1158;
	fma.rn.f64 	%fd1161, %fd1159, %fd636, %fd1809;
	fma.rn.f64 	%fd1163, %fd1159, %fd638, %fd1161;
	fma.rn.f64 	%fd1810, %fd1159, %fd640, %fd1163;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r308}, %fd1809;
	}
	and.b32  	%r309, %r308, 2145386496;
	setp.lt.u32	%p113, %r309, 1105199104;
	@%p113 bra 	BB0_173;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1809;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1810, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r497, [%rd40];

BB0_173:
	and.b32  	%r310, %r497, 1;
	shl.b32 	%r311, %r310, 3;
	setp.eq.b32	%p114, %r310, 1;
	selp.f64	%fd1165, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p114;
	mul.wide.u32 	%rd168, %r311, 8;
	add.s64 	%rd170, %rd168, %rd44;
	ld.const.f64 	%fd1166, [%rd170+8];
	mul.rn.f64 	%fd308, %fd1810, %fd1810;
	fma.rn.f64 	%fd1167, %fd1165, %fd308, %fd1166;
	ld.const.f64 	%fd1168, [%rd170+16];
	fma.rn.f64 	%fd1169, %fd1167, %fd308, %fd1168;
	ld.const.f64 	%fd1170, [%rd170+24];
	fma.rn.f64 	%fd1171, %fd1169, %fd308, %fd1170;
	ld.const.f64 	%fd1172, [%rd170+32];
	fma.rn.f64 	%fd1173, %fd1171, %fd308, %fd1172;
	ld.const.f64 	%fd1174, [%rd170+40];
	fma.rn.f64 	%fd1175, %fd1173, %fd308, %fd1174;
	ld.const.f64 	%fd1176, [%rd170+48];
	fma.rn.f64 	%fd309, %fd1175, %fd308, %fd1176;
	fma.rn.f64 	%fd1811, %fd309, %fd1810, %fd1810;
	setp.eq.s32	%p115, %r310, 0;
	@%p115 bra 	BB0_175;

	mov.f64 	%fd1177, 0d3FF0000000000000;
	fma.rn.f64 	%fd1811, %fd309, %fd308, %fd1177;

BB0_175:
	and.b32  	%r312, %r497, 2;
	setp.eq.s32	%p116, %r312, 0;
	@%p116 bra 	BB0_177;

	mov.f64 	%fd1178, 0d0000000000000000;
	mov.f64 	%fd1179, 0dBFF0000000000000;
	fma.rn.f64 	%fd1811, %fd1811, %fd1179, %fd1178;

BB0_177:
	ld.global.f64 	%fd1180, [%rd2+24];
	add.f64 	%fd1181, %fd1180, %fd1180;
	mul.f64 	%fd1182, %fd1180, %fd1181;
	mul.f64 	%fd1183, %fd1808, %fd1811;
	div.rn.f64 	%fd315, %fd1183, %fd1182;
	ld.global.f64 	%fd1812, [%rd2+40];
	abs.f64 	%fd1184, %fd1812;
	setp.neu.f64	%p117, %fd1184, 0d7FF0000000000000;
	@%p117 bra 	BB0_179;

	mov.f64 	%fd1185, 0d0000000000000000;
	mul.rn.f64 	%fd1812, %fd1812, %fd1185;

BB0_179:
	mul.f64 	%fd1186, %fd1812, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r498, %fd1186;
	st.local.u32 	[%rd40], %r498;
	cvt.rn.f64.s32	%fd1187, %r498;
	neg.f64 	%fd1188, %fd1187;
	fma.rn.f64 	%fd1190, %fd1188, %fd636, %fd1812;
	fma.rn.f64 	%fd1192, %fd1188, %fd638, %fd1190;
	fma.rn.f64 	%fd1813, %fd1188, %fd640, %fd1192;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r313}, %fd1812;
	}
	and.b32  	%r314, %r313, 2145386496;
	setp.lt.u32	%p118, %r314, 1105199104;
	@%p118 bra 	BB0_181;

	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1812;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1813, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r498, [%rd40];

BB0_181:
	add.s32 	%r86, %r498, 1;
	and.b32  	%r315, %r86, 1;
	shl.b32 	%r316, %r315, 3;
	setp.eq.b32	%p119, %r315, 1;
	selp.f64	%fd1194, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p119;
	mul.wide.u32 	%rd175, %r316, 8;
	add.s64 	%rd177, %rd175, %rd44;
	ld.const.f64 	%fd1195, [%rd177+8];
	mul.rn.f64 	%fd322, %fd1813, %fd1813;
	fma.rn.f64 	%fd1196, %fd1194, %fd322, %fd1195;
	ld.const.f64 	%fd1197, [%rd177+16];
	fma.rn.f64 	%fd1198, %fd1196, %fd322, %fd1197;
	ld.const.f64 	%fd1199, [%rd177+24];
	fma.rn.f64 	%fd1200, %fd1198, %fd322, %fd1199;
	ld.const.f64 	%fd1201, [%rd177+32];
	fma.rn.f64 	%fd1202, %fd1200, %fd322, %fd1201;
	ld.const.f64 	%fd1203, [%rd177+40];
	fma.rn.f64 	%fd1204, %fd1202, %fd322, %fd1203;
	ld.const.f64 	%fd1205, [%rd177+48];
	fma.rn.f64 	%fd323, %fd1204, %fd322, %fd1205;
	fma.rn.f64 	%fd1814, %fd323, %fd1813, %fd1813;
	setp.eq.s32	%p120, %r315, 0;
	@%p120 bra 	BB0_183;

	mov.f64 	%fd1206, 0d3FF0000000000000;
	fma.rn.f64 	%fd1814, %fd323, %fd322, %fd1206;

BB0_183:
	and.b32  	%r317, %r86, 2;
	setp.eq.s32	%p121, %r317, 0;
	@%p121 bra 	BB0_185;

	mov.f64 	%fd1207, 0d0000000000000000;
	mov.f64 	%fd1208, 0dBFF0000000000000;
	fma.rn.f64 	%fd1814, %fd1814, %fd1208, %fd1207;

BB0_185:
	ld.global.f64 	%fd1815, [%rd2+40];
	abs.f64 	%fd1209, %fd1815;
	setp.neu.f64	%p122, %fd1209, 0d7FF0000000000000;
	@%p122 bra 	BB0_187;

	mov.f64 	%fd1210, 0d0000000000000000;
	mul.rn.f64 	%fd1815, %fd1815, %fd1210;

BB0_187:
	mul.f64 	%fd1211, %fd1815, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r499, %fd1211;
	st.local.u32 	[%rd40], %r499;
	cvt.rn.f64.s32	%fd1212, %r499;
	neg.f64 	%fd1213, %fd1212;
	fma.rn.f64 	%fd1215, %fd1213, %fd636, %fd1815;
	fma.rn.f64 	%fd1217, %fd1213, %fd638, %fd1215;
	fma.rn.f64 	%fd1816, %fd1213, %fd640, %fd1217;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r318}, %fd1815;
	}
	and.b32  	%r319, %r318, 2145386496;
	setp.lt.u32	%p123, %r319, 1105199104;
	@%p123 bra 	BB0_189;

	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1815;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd39;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1816, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r499, [%rd40];

BB0_189:
	add.s32 	%r90, %r499, 1;
	and.b32  	%r320, %r90, 1;
	shl.b32 	%r321, %r320, 3;
	setp.eq.b32	%p124, %r320, 1;
	selp.f64	%fd1219, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p124;
	mul.wide.u32 	%rd182, %r321, 8;
	add.s64 	%rd184, %rd182, %rd44;
	ld.const.f64 	%fd1220, [%rd184+8];
	mul.rn.f64 	%fd335, %fd1816, %fd1816;
	fma.rn.f64 	%fd1221, %fd1219, %fd335, %fd1220;
	ld.const.f64 	%fd1222, [%rd184+16];
	fma.rn.f64 	%fd1223, %fd1221, %fd335, %fd1222;
	ld.const.f64 	%fd1224, [%rd184+24];
	fma.rn.f64 	%fd1225, %fd1223, %fd335, %fd1224;
	ld.const.f64 	%fd1226, [%rd184+32];
	fma.rn.f64 	%fd1227, %fd1225, %fd335, %fd1226;
	ld.const.f64 	%fd1228, [%rd184+40];
	fma.rn.f64 	%fd1229, %fd1227, %fd335, %fd1228;
	ld.const.f64 	%fd1230, [%rd184+48];
	fma.rn.f64 	%fd336, %fd1229, %fd335, %fd1230;
	fma.rn.f64 	%fd1817, %fd336, %fd1816, %fd1816;
	setp.eq.s32	%p125, %r320, 0;
	@%p125 bra 	BB0_191;

	mov.f64 	%fd1231, 0d3FF0000000000000;
	fma.rn.f64 	%fd1817, %fd336, %fd335, %fd1231;

BB0_191:
	and.b32  	%r322, %r90, 2;
	setp.eq.s32	%p126, %r322, 0;
	@%p126 bra 	BB0_193;

	mov.f64 	%fd1232, 0d0000000000000000;
	mov.f64 	%fd1233, 0dBFF0000000000000;
	fma.rn.f64 	%fd1817, %fd1817, %fd1233, %fd1232;

BB0_193:
	ld.global.f64 	%fd1234, [%rd2+32];
	add.f64 	%fd1235, %fd1234, %fd1234;
	mul.f64 	%fd1236, %fd1234, %fd1235;
	mul.f64 	%fd1237, %fd1814, %fd1817;
	div.rn.f64 	%fd1238, %fd1237, %fd1236;
	add.f64 	%fd1720, %fd315, %fd1238;
	ld.global.f64 	%fd343, [%rd5];
	setp.lt.f64	%p127, %fd343, 0d0000000000000000;
	@%p127 bra 	BB0_195;
	bra.uni 	BB0_194;

BB0_195:
	mul.f64 	%fd1240, %fd343, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd5], %fd1240;
	mov.f64 	%fd1727, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1727;
	bra.uni 	BB0_225;

BB0_23:
	setp.lt.f64	%p20, %fd55, 0d0000000000000000;
	@%p20 bra 	BB0_25;
	bra.uni 	BB0_24;

BB0_25:
	mul.f64 	%fd630, %fd55, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd5], %fd630;
	mov.f64 	%fd1724, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1724;
	bra.uni 	BB0_225;

BB0_197:
	setp.lt.f64	%p131, %fd344, 0d0000000000000000;
	@%p131 bra 	BB0_199;
	bra.uni 	BB0_198;

BB0_199:
	mul.f64 	%fd1243, %fd344, 0dBFE5559B3D07C84B;
	st.global.f64 	[%rd3+48], %fd1243;
	mov.f64 	%fd1729, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1729;
	bra.uni 	BB0_225;

BB0_212:
	neg.f64 	%fd1300, %fd360;
	st.global.f64 	[%rd3], %fd1300;
	bra.uni 	BB0_224;

BB0_24:
	neg.f64 	%fd629, %fd55;
	st.global.f64 	[%rd5], %fd629;
	mov.f64 	%fd1725, %fd53;
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd1725;
	bra.uni 	BB0_225;

BB0_222:
	neg.f64 	%fd1355, %fd375;
	st.global.f64 	[%rd3], %fd1355;

BB0_224:
	mov.f64 	%fd1755, %fd54;
	mov.f64 	%fd1753, %fd54;

BB0_225:
	mov.f64 	%fd1754, %fd1755;
	mov.f64 	%fd1752, %fd1753;
	add.s32 	%r477, %r476, 1;
	setp.lt.s32	%p150, %r476, 6;
	@%p150 bra 	BB0_228;

	mov.u32 	%r477, 0;
	setp.lt.s32	%p151, %r478, 51;
	@%p151 bra 	BB0_228;

	ld.param.f64 	%fd1691, [gaussFitter_param_9];
	sub.f64 	%fd1357, %fd1752, %fd1754;
	setp.lt.f64	%p152, %fd1357, %fd1691;
	selp.b16	%rs8, 0, %rs8, %p152;

BB0_228:
	mov.u32 	%r476, %r477;
	ld.param.u32 	%r470, [gaussFitter_param_10];
	setp.gt.s32	%p153, %r478, %r470;
	selp.b16	%rs8, 0, %rs8, %p153;
	and.b16  	%rs7, %rs8, 255;
	add.s32 	%r478, %r478, 1;
	setp.ne.s16	%p154, %rs7, 0;
	@%p154 bra 	BB0_19;

	ld.global.f64 	%fd1822, [%rd2+40];
	abs.f64 	%fd1358, %fd1822;
	setp.neu.f64	%p155, %fd1358, 0d7FF0000000000000;
	@%p155 bra 	BB0_231;

	mov.f64 	%fd1359, 0d0000000000000000;
	mul.rn.f64 	%fd1822, %fd1822, %fd1359;

BB0_231:
	mul.f64 	%fd1360, %fd1822, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r502, %fd1360;
	add.u64 	%rd202, %SP, 0;
	cvta.to.local.u64 	%rd203, %rd202;
	st.local.u32 	[%rd203], %r502;
	cvt.rn.f64.s32	%fd1361, %r502;
	neg.f64 	%fd1362, %fd1361;
	mov.f64 	%fd1363, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd1364, %fd1362, %fd1363, %fd1822;
	mov.f64 	%fd1365, 0d3C91A62633145C00;
	fma.rn.f64 	%fd1366, %fd1362, %fd1365, %fd1364;
	mov.f64 	%fd1367, 0d397B839A252049C0;
	fma.rn.f64 	%fd1823, %fd1362, %fd1367, %fd1366;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r391}, %fd1822;
	}
	and.b32  	%r392, %r391, 2145386496;
	setp.lt.u32	%p156, %r392, 1105199104;
	@%p156 bra 	BB0_233;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1822;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1823, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r502, [%rd203];

BB0_233:
	add.s32 	%r110, %r502, 1;
	and.b32  	%r393, %r110, 1;
	shl.b32 	%r394, %r393, 3;
	setp.eq.b32	%p157, %r393, 1;
	selp.f64	%fd1368, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p157;
	mul.wide.u32 	%rd206, %r394, 8;
	mov.u64 	%rd207, __cudart_sin_cos_coeffs;
	add.s64 	%rd208, %rd206, %rd207;
	ld.const.f64 	%fd1369, [%rd208+8];
	mul.rn.f64 	%fd387, %fd1823, %fd1823;
	fma.rn.f64 	%fd1370, %fd1368, %fd387, %fd1369;
	ld.const.f64 	%fd1371, [%rd208+16];
	fma.rn.f64 	%fd1372, %fd1370, %fd387, %fd1371;
	ld.const.f64 	%fd1373, [%rd208+24];
	fma.rn.f64 	%fd1374, %fd1372, %fd387, %fd1373;
	ld.const.f64 	%fd1375, [%rd208+32];
	fma.rn.f64 	%fd1376, %fd1374, %fd387, %fd1375;
	ld.const.f64 	%fd1377, [%rd208+40];
	fma.rn.f64 	%fd1378, %fd1376, %fd387, %fd1377;
	ld.const.f64 	%fd1379, [%rd208+48];
	fma.rn.f64 	%fd388, %fd1378, %fd387, %fd1379;
	fma.rn.f64 	%fd1824, %fd388, %fd1823, %fd1823;
	setp.eq.s32	%p158, %r393, 0;
	@%p158 bra 	BB0_235;

	mov.f64 	%fd1380, 0d3FF0000000000000;
	fma.rn.f64 	%fd1824, %fd388, %fd387, %fd1380;

BB0_235:
	and.b32  	%r395, %r110, 2;
	setp.eq.s32	%p159, %r395, 0;
	@%p159 bra 	BB0_237;

	mov.f64 	%fd1381, 0d0000000000000000;
	mov.f64 	%fd1382, 0dBFF0000000000000;
	fma.rn.f64 	%fd1824, %fd1824, %fd1382, %fd1381;

BB0_237:
	ld.global.f64 	%fd1825, [%rd2+40];
	abs.f64 	%fd1383, %fd1825;
	setp.neu.f64	%p160, %fd1383, 0d7FF0000000000000;
	@%p160 bra 	BB0_239;

	mov.f64 	%fd1384, 0d0000000000000000;
	mul.rn.f64 	%fd1825, %fd1825, %fd1384;

BB0_239:
	mul.f64 	%fd1385, %fd1825, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r503, %fd1385;
	st.local.u32 	[%rd203], %r503;
	cvt.rn.f64.s32	%fd1386, %r503;
	neg.f64 	%fd1387, %fd1386;
	fma.rn.f64 	%fd1389, %fd1387, %fd1363, %fd1825;
	fma.rn.f64 	%fd1391, %fd1387, %fd1365, %fd1389;
	fma.rn.f64 	%fd1826, %fd1387, %fd1367, %fd1391;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r396}, %fd1825;
	}
	and.b32  	%r397, %r396, 2145386496;
	setp.lt.u32	%p161, %r397, 1105199104;
	@%p161 bra 	BB0_241;

	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1825;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1826, [retval0+0];
	
	//{
	}// Callseq End 21
	ld.local.u32 	%r503, [%rd203];

BB0_241:
	add.s32 	%r114, %r503, 1;
	and.b32  	%r398, %r114, 1;
	shl.b32 	%r399, %r398, 3;
	setp.eq.b32	%p162, %r398, 1;
	selp.f64	%fd1393, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p162;
	mul.wide.u32 	%rd213, %r399, 8;
	add.s64 	%rd215, %rd213, %rd207;
	ld.const.f64 	%fd1394, [%rd215+8];
	mul.rn.f64 	%fd400, %fd1826, %fd1826;
	fma.rn.f64 	%fd1395, %fd1393, %fd400, %fd1394;
	ld.const.f64 	%fd1396, [%rd215+16];
	fma.rn.f64 	%fd1397, %fd1395, %fd400, %fd1396;
	ld.const.f64 	%fd1398, [%rd215+24];
	fma.rn.f64 	%fd1399, %fd1397, %fd400, %fd1398;
	ld.const.f64 	%fd1400, [%rd215+32];
	fma.rn.f64 	%fd1401, %fd1399, %fd400, %fd1400;
	ld.const.f64 	%fd1402, [%rd215+40];
	fma.rn.f64 	%fd1403, %fd1401, %fd400, %fd1402;
	ld.const.f64 	%fd1404, [%rd215+48];
	fma.rn.f64 	%fd401, %fd1403, %fd400, %fd1404;
	fma.rn.f64 	%fd1827, %fd401, %fd1826, %fd1826;
	setp.eq.s32	%p163, %r398, 0;
	@%p163 bra 	BB0_243;

	mov.f64 	%fd1405, 0d3FF0000000000000;
	fma.rn.f64 	%fd1827, %fd401, %fd400, %fd1405;

BB0_243:
	and.b32  	%r400, %r114, 2;
	setp.eq.s32	%p164, %r400, 0;
	@%p164 bra 	BB0_245;

	mov.f64 	%fd1406, 0d0000000000000000;
	mov.f64 	%fd1407, 0dBFF0000000000000;
	fma.rn.f64 	%fd1827, %fd1827, %fd1407, %fd1406;

BB0_245:
	ld.global.f64 	%fd1408, [%rd2+24];
	add.f64 	%fd1409, %fd1408, %fd1408;
	mul.f64 	%fd1410, %fd1408, %fd1409;
	mul.f64 	%fd1411, %fd1824, %fd1827;
	div.rn.f64 	%fd407, %fd1411, %fd1410;
	ld.global.f64 	%fd1828, [%rd2+40];
	abs.f64 	%fd1412, %fd1828;
	setp.neu.f64	%p165, %fd1412, 0d7FF0000000000000;
	@%p165 bra 	BB0_247;

	mov.f64 	%fd1413, 0d0000000000000000;
	mul.rn.f64 	%fd1828, %fd1828, %fd1413;

BB0_247:
	mul.f64 	%fd1414, %fd1828, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r504, %fd1414;
	st.local.u32 	[%rd203], %r504;
	cvt.rn.f64.s32	%fd1415, %r504;
	neg.f64 	%fd1416, %fd1415;
	fma.rn.f64 	%fd1418, %fd1416, %fd1363, %fd1828;
	fma.rn.f64 	%fd1420, %fd1416, %fd1365, %fd1418;
	fma.rn.f64 	%fd1829, %fd1416, %fd1367, %fd1420;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r401}, %fd1828;
	}
	and.b32  	%r402, %r401, 2145386496;
	setp.lt.u32	%p166, %r402, 1105199104;
	@%p166 bra 	BB0_249;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1828;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1829, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r504, [%rd203];

BB0_249:
	and.b32  	%r403, %r504, 1;
	shl.b32 	%r404, %r403, 3;
	setp.eq.b32	%p167, %r403, 1;
	selp.f64	%fd1422, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p167;
	mul.wide.u32 	%rd220, %r404, 8;
	add.s64 	%rd222, %rd220, %rd207;
	ld.const.f64 	%fd1423, [%rd222+8];
	mul.rn.f64 	%fd414, %fd1829, %fd1829;
	fma.rn.f64 	%fd1424, %fd1422, %fd414, %fd1423;
	ld.const.f64 	%fd1425, [%rd222+16];
	fma.rn.f64 	%fd1426, %fd1424, %fd414, %fd1425;
	ld.const.f64 	%fd1427, [%rd222+24];
	fma.rn.f64 	%fd1428, %fd1426, %fd414, %fd1427;
	ld.const.f64 	%fd1429, [%rd222+32];
	fma.rn.f64 	%fd1430, %fd1428, %fd414, %fd1429;
	ld.const.f64 	%fd1431, [%rd222+40];
	fma.rn.f64 	%fd1432, %fd1430, %fd414, %fd1431;
	ld.const.f64 	%fd1433, [%rd222+48];
	fma.rn.f64 	%fd415, %fd1432, %fd414, %fd1433;
	fma.rn.f64 	%fd1830, %fd415, %fd1829, %fd1829;
	setp.eq.s32	%p168, %r403, 0;
	@%p168 bra 	BB0_251;

	mov.f64 	%fd1434, 0d3FF0000000000000;
	fma.rn.f64 	%fd1830, %fd415, %fd414, %fd1434;

BB0_251:
	and.b32  	%r405, %r504, 2;
	setp.eq.s32	%p169, %r405, 0;
	@%p169 bra 	BB0_253;

	mov.f64 	%fd1435, 0d0000000000000000;
	mov.f64 	%fd1436, 0dBFF0000000000000;
	fma.rn.f64 	%fd1830, %fd1830, %fd1436, %fd1435;

BB0_253:
	ld.global.f64 	%fd1831, [%rd2+40];
	abs.f64 	%fd1437, %fd1831;
	setp.neu.f64	%p170, %fd1437, 0d7FF0000000000000;
	@%p170 bra 	BB0_255;

	mov.f64 	%fd1438, 0d0000000000000000;
	mul.rn.f64 	%fd1831, %fd1831, %fd1438;

BB0_255:
	mul.f64 	%fd1439, %fd1831, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r505, %fd1439;
	st.local.u32 	[%rd203], %r505;
	cvt.rn.f64.s32	%fd1440, %r505;
	neg.f64 	%fd1441, %fd1440;
	fma.rn.f64 	%fd1443, %fd1441, %fd1363, %fd1831;
	fma.rn.f64 	%fd1445, %fd1441, %fd1365, %fd1443;
	fma.rn.f64 	%fd1832, %fd1441, %fd1367, %fd1445;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r406}, %fd1831;
	}
	and.b32  	%r407, %r406, 2145386496;
	setp.lt.u32	%p171, %r407, 1105199104;
	@%p171 bra 	BB0_257;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1831;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1832, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r505, [%rd203];

BB0_257:
	and.b32  	%r408, %r505, 1;
	shl.b32 	%r409, %r408, 3;
	setp.eq.b32	%p172, %r408, 1;
	selp.f64	%fd1447, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p172;
	mul.wide.u32 	%rd227, %r409, 8;
	add.s64 	%rd229, %rd227, %rd207;
	ld.const.f64 	%fd1448, [%rd229+8];
	mul.rn.f64 	%fd427, %fd1832, %fd1832;
	fma.rn.f64 	%fd1449, %fd1447, %fd427, %fd1448;
	ld.const.f64 	%fd1450, [%rd229+16];
	fma.rn.f64 	%fd1451, %fd1449, %fd427, %fd1450;
	ld.const.f64 	%fd1452, [%rd229+24];
	fma.rn.f64 	%fd1453, %fd1451, %fd427, %fd1452;
	ld.const.f64 	%fd1454, [%rd229+32];
	fma.rn.f64 	%fd1455, %fd1453, %fd427, %fd1454;
	ld.const.f64 	%fd1456, [%rd229+40];
	fma.rn.f64 	%fd1457, %fd1455, %fd427, %fd1456;
	ld.const.f64 	%fd1458, [%rd229+48];
	fma.rn.f64 	%fd428, %fd1457, %fd427, %fd1458;
	fma.rn.f64 	%fd1833, %fd428, %fd1832, %fd1832;
	setp.eq.s32	%p173, %r408, 0;
	@%p173 bra 	BB0_259;

	mov.f64 	%fd1459, 0d3FF0000000000000;
	fma.rn.f64 	%fd1833, %fd428, %fd427, %fd1459;

BB0_259:
	and.b32  	%r410, %r505, 2;
	setp.eq.s32	%p174, %r410, 0;
	@%p174 bra 	BB0_261;

	mov.f64 	%fd1460, 0d0000000000000000;
	mov.f64 	%fd1461, 0dBFF0000000000000;
	fma.rn.f64 	%fd1833, %fd1833, %fd1461, %fd1460;

BB0_261:
	ld.global.f64 	%fd1462, [%rd2+32];
	add.f64 	%fd1463, %fd1462, %fd1462;
	mul.f64 	%fd1464, %fd1462, %fd1463;
	mul.f64 	%fd1465, %fd1830, %fd1833;
	div.rn.f64 	%fd1466, %fd1465, %fd1464;
	add.f64 	%fd434, %fd407, %fd1466;
	ld.global.f64 	%fd1467, [%rd2+40];
	add.f64 	%fd1834, %fd1467, %fd1467;
	abs.f64 	%fd1468, %fd1834;
	setp.neu.f64	%p175, %fd1468, 0d7FF0000000000000;
	@%p175 bra 	BB0_263;

	mov.f64 	%fd1469, 0d0000000000000000;
	mul.rn.f64 	%fd1834, %fd1834, %fd1469;

BB0_263:
	mul.f64 	%fd1470, %fd1834, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r506, %fd1470;
	st.local.u32 	[%rd203], %r506;
	cvt.rn.f64.s32	%fd1471, %r506;
	neg.f64 	%fd1472, %fd1471;
	fma.rn.f64 	%fd1474, %fd1472, %fd1363, %fd1834;
	fma.rn.f64 	%fd1476, %fd1472, %fd1365, %fd1474;
	fma.rn.f64 	%fd1835, %fd1472, %fd1367, %fd1476;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r411}, %fd1834;
	}
	and.b32  	%r412, %r411, 2145386496;
	setp.lt.u32	%p176, %r412, 1105199104;
	@%p176 bra 	BB0_265;

	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1834;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1835, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r506, [%rd203];

BB0_265:
	and.b32  	%r413, %r506, 1;
	shl.b32 	%r414, %r413, 3;
	setp.eq.b32	%p177, %r413, 1;
	selp.f64	%fd1478, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p177;
	mul.wide.u32 	%rd234, %r414, 8;
	add.s64 	%rd236, %rd234, %rd207;
	ld.const.f64 	%fd1479, [%rd236+8];
	mul.rn.f64 	%fd441, %fd1835, %fd1835;
	fma.rn.f64 	%fd1480, %fd1478, %fd441, %fd1479;
	ld.const.f64 	%fd1481, [%rd236+16];
	fma.rn.f64 	%fd1482, %fd1480, %fd441, %fd1481;
	ld.const.f64 	%fd1483, [%rd236+24];
	fma.rn.f64 	%fd1484, %fd1482, %fd441, %fd1483;
	ld.const.f64 	%fd1485, [%rd236+32];
	fma.rn.f64 	%fd1486, %fd1484, %fd441, %fd1485;
	ld.const.f64 	%fd1487, [%rd236+40];
	fma.rn.f64 	%fd1488, %fd1486, %fd441, %fd1487;
	ld.const.f64 	%fd1489, [%rd236+48];
	fma.rn.f64 	%fd442, %fd1488, %fd441, %fd1489;
	fma.rn.f64 	%fd1836, %fd442, %fd1835, %fd1835;
	setp.eq.s32	%p178, %r413, 0;
	@%p178 bra 	BB0_267;

	mov.f64 	%fd1490, 0d3FF0000000000000;
	fma.rn.f64 	%fd1836, %fd442, %fd441, %fd1490;

BB0_267:
	and.b32  	%r415, %r506, 2;
	setp.eq.s32	%p179, %r415, 0;
	@%p179 bra 	BB0_269;

	mov.f64 	%fd1491, 0d0000000000000000;
	mov.f64 	%fd1492, 0dBFF0000000000000;
	fma.rn.f64 	%fd1836, %fd1836, %fd1492, %fd1491;

BB0_269:
	ld.global.f64 	%fd1493, [%rd2+24];
	mul.f64 	%fd1494, %fd1493, 0dC010000000000000;
	mul.f64 	%fd1495, %fd1493, %fd1494;
	div.rn.f64 	%fd448, %fd1836, %fd1495;
	ld.global.f64 	%fd1496, [%rd2+40];
	add.f64 	%fd1837, %fd1496, %fd1496;
	abs.f64 	%fd1497, %fd1837;
	setp.neu.f64	%p180, %fd1497, 0d7FF0000000000000;
	@%p180 bra 	BB0_271;

	mov.f64 	%fd1498, 0d0000000000000000;
	mul.rn.f64 	%fd1837, %fd1837, %fd1498;

BB0_271:
	mul.f64 	%fd1499, %fd1837, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r507, %fd1499;
	st.local.u32 	[%rd203], %r507;
	cvt.rn.f64.s32	%fd1500, %r507;
	neg.f64 	%fd1501, %fd1500;
	fma.rn.f64 	%fd1503, %fd1501, %fd1363, %fd1837;
	fma.rn.f64 	%fd1505, %fd1501, %fd1365, %fd1503;
	fma.rn.f64 	%fd1838, %fd1501, %fd1367, %fd1505;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r416}, %fd1837;
	}
	and.b32  	%r417, %r416, 2145386496;
	setp.lt.u32	%p181, %r417, 1105199104;
	@%p181 bra 	BB0_273;

	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1837;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1838, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r507, [%rd203];

BB0_273:
	and.b32  	%r418, %r507, 1;
	shl.b32 	%r419, %r418, 3;
	setp.eq.b32	%p182, %r418, 1;
	selp.f64	%fd1507, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p182;
	mul.wide.u32 	%rd241, %r419, 8;
	add.s64 	%rd243, %rd241, %rd207;
	ld.const.f64 	%fd1508, [%rd243+8];
	mul.rn.f64 	%fd455, %fd1838, %fd1838;
	fma.rn.f64 	%fd1509, %fd1507, %fd455, %fd1508;
	ld.const.f64 	%fd1510, [%rd243+16];
	fma.rn.f64 	%fd1511, %fd1509, %fd455, %fd1510;
	ld.const.f64 	%fd1512, [%rd243+24];
	fma.rn.f64 	%fd1513, %fd1511, %fd455, %fd1512;
	ld.const.f64 	%fd1514, [%rd243+32];
	fma.rn.f64 	%fd1515, %fd1513, %fd455, %fd1514;
	ld.const.f64 	%fd1516, [%rd243+40];
	fma.rn.f64 	%fd1517, %fd1515, %fd455, %fd1516;
	ld.const.f64 	%fd1518, [%rd243+48];
	fma.rn.f64 	%fd456, %fd1517, %fd455, %fd1518;
	fma.rn.f64 	%fd1839, %fd456, %fd1838, %fd1838;
	setp.eq.s32	%p183, %r418, 0;
	@%p183 bra 	BB0_275;

	mov.f64 	%fd1519, 0d3FF0000000000000;
	fma.rn.f64 	%fd1839, %fd456, %fd455, %fd1519;

BB0_275:
	and.b32  	%r420, %r507, 2;
	setp.eq.s32	%p184, %r420, 0;
	@%p184 bra 	BB0_277;

	mov.f64 	%fd1520, 0d0000000000000000;
	mov.f64 	%fd1521, 0dBFF0000000000000;
	fma.rn.f64 	%fd1839, %fd1839, %fd1521, %fd1520;

BB0_277:
	ld.global.f64 	%fd1522, [%rd2+32];
	mul.f64 	%fd1523, %fd1522, 0d4010000000000000;
	mul.f64 	%fd1524, %fd1522, %fd1523;
	div.rn.f64 	%fd1525, %fd1839, %fd1524;
	add.f64 	%fd462, %fd448, %fd1525;
	ld.global.f64 	%fd1840, [%rd2+40];
	abs.f64 	%fd1526, %fd1840;
	setp.neu.f64	%p185, %fd1526, 0d7FF0000000000000;
	@%p185 bra 	BB0_279;

	mov.f64 	%fd1527, 0d0000000000000000;
	mul.rn.f64 	%fd1840, %fd1840, %fd1527;

BB0_279:
	mul.f64 	%fd1528, %fd1840, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r508, %fd1528;
	st.local.u32 	[%rd203], %r508;
	cvt.rn.f64.s32	%fd1529, %r508;
	neg.f64 	%fd1530, %fd1529;
	fma.rn.f64 	%fd1532, %fd1530, %fd1363, %fd1840;
	fma.rn.f64 	%fd1534, %fd1530, %fd1365, %fd1532;
	fma.rn.f64 	%fd1841, %fd1530, %fd1367, %fd1534;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r421}, %fd1840;
	}
	and.b32  	%r422, %r421, 2145386496;
	setp.lt.u32	%p186, %r422, 1105199104;
	@%p186 bra 	BB0_281;

	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1840;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1841, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r508, [%rd203];

BB0_281:
	and.b32  	%r423, %r508, 1;
	shl.b32 	%r424, %r423, 3;
	setp.eq.b32	%p187, %r423, 1;
	selp.f64	%fd1536, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p187;
	mul.wide.u32 	%rd248, %r424, 8;
	add.s64 	%rd250, %rd248, %rd207;
	ld.const.f64 	%fd1537, [%rd250+8];
	mul.rn.f64 	%fd469, %fd1841, %fd1841;
	fma.rn.f64 	%fd1538, %fd1536, %fd469, %fd1537;
	ld.const.f64 	%fd1539, [%rd250+16];
	fma.rn.f64 	%fd1540, %fd1538, %fd469, %fd1539;
	ld.const.f64 	%fd1541, [%rd250+24];
	fma.rn.f64 	%fd1542, %fd1540, %fd469, %fd1541;
	ld.const.f64 	%fd1543, [%rd250+32];
	fma.rn.f64 	%fd1544, %fd1542, %fd469, %fd1543;
	ld.const.f64 	%fd1545, [%rd250+40];
	fma.rn.f64 	%fd1546, %fd1544, %fd469, %fd1545;
	ld.const.f64 	%fd1547, [%rd250+48];
	fma.rn.f64 	%fd470, %fd1546, %fd469, %fd1547;
	fma.rn.f64 	%fd1842, %fd470, %fd1841, %fd1841;
	setp.eq.s32	%p188, %r423, 0;
	@%p188 bra 	BB0_283;

	mov.f64 	%fd1548, 0d3FF0000000000000;
	fma.rn.f64 	%fd1842, %fd470, %fd469, %fd1548;

BB0_283:
	and.b32  	%r425, %r508, 2;
	setp.eq.s32	%p189, %r425, 0;
	@%p189 bra 	BB0_285;

	mov.f64 	%fd1549, 0d0000000000000000;
	mov.f64 	%fd1550, 0dBFF0000000000000;
	fma.rn.f64 	%fd1842, %fd1842, %fd1550, %fd1549;

BB0_285:
	ld.global.f64 	%fd1843, [%rd2+40];
	abs.f64 	%fd1551, %fd1843;
	setp.neu.f64	%p190, %fd1551, 0d7FF0000000000000;
	@%p190 bra 	BB0_287;

	mov.f64 	%fd1552, 0d0000000000000000;
	mul.rn.f64 	%fd1843, %fd1843, %fd1552;

BB0_287:
	mul.f64 	%fd1553, %fd1843, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r509, %fd1553;
	st.local.u32 	[%rd203], %r509;
	cvt.rn.f64.s32	%fd1554, %r509;
	neg.f64 	%fd1555, %fd1554;
	fma.rn.f64 	%fd1557, %fd1555, %fd1363, %fd1843;
	fma.rn.f64 	%fd1559, %fd1555, %fd1365, %fd1557;
	fma.rn.f64 	%fd1844, %fd1555, %fd1367, %fd1559;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r426}, %fd1843;
	}
	and.b32  	%r427, %r426, 2145386496;
	setp.lt.u32	%p191, %r427, 1105199104;
	@%p191 bra 	BB0_289;

	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1843;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1844, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r509, [%rd203];

BB0_289:
	and.b32  	%r428, %r509, 1;
	shl.b32 	%r429, %r428, 3;
	setp.eq.b32	%p192, %r428, 1;
	selp.f64	%fd1561, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p192;
	mul.wide.u32 	%rd255, %r429, 8;
	add.s64 	%rd257, %rd255, %rd207;
	ld.const.f64 	%fd1562, [%rd257+8];
	mul.rn.f64 	%fd482, %fd1844, %fd1844;
	fma.rn.f64 	%fd1563, %fd1561, %fd482, %fd1562;
	ld.const.f64 	%fd1564, [%rd257+16];
	fma.rn.f64 	%fd1565, %fd1563, %fd482, %fd1564;
	ld.const.f64 	%fd1566, [%rd257+24];
	fma.rn.f64 	%fd1567, %fd1565, %fd482, %fd1566;
	ld.const.f64 	%fd1568, [%rd257+32];
	fma.rn.f64 	%fd1569, %fd1567, %fd482, %fd1568;
	ld.const.f64 	%fd1570, [%rd257+40];
	fma.rn.f64 	%fd1571, %fd1569, %fd482, %fd1570;
	ld.const.f64 	%fd1572, [%rd257+48];
	fma.rn.f64 	%fd483, %fd1571, %fd482, %fd1572;
	fma.rn.f64 	%fd1845, %fd483, %fd1844, %fd1844;
	setp.eq.s32	%p193, %r428, 0;
	@%p193 bra 	BB0_291;

	mov.f64 	%fd1573, 0d3FF0000000000000;
	fma.rn.f64 	%fd1845, %fd483, %fd482, %fd1573;

BB0_291:
	and.b32  	%r430, %r509, 2;
	setp.eq.s32	%p194, %r430, 0;
	@%p194 bra 	BB0_293;

	mov.f64 	%fd1574, 0d0000000000000000;
	mov.f64 	%fd1575, 0dBFF0000000000000;
	fma.rn.f64 	%fd1845, %fd1845, %fd1575, %fd1574;

BB0_293:
	ld.global.f64 	%fd1576, [%rd2+24];
	add.f64 	%fd1577, %fd1576, %fd1576;
	mul.f64 	%fd1578, %fd1576, %fd1577;
	mul.f64 	%fd1579, %fd1842, %fd1845;
	div.rn.f64 	%fd489, %fd1579, %fd1578;
	ld.global.f64 	%fd1846, [%rd2+40];
	abs.f64 	%fd1580, %fd1846;
	setp.neu.f64	%p195, %fd1580, 0d7FF0000000000000;
	@%p195 bra 	BB0_295;

	mov.f64 	%fd1581, 0d0000000000000000;
	mul.rn.f64 	%fd1846, %fd1846, %fd1581;

BB0_295:
	mul.f64 	%fd1582, %fd1846, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r510, %fd1582;
	st.local.u32 	[%rd203], %r510;
	cvt.rn.f64.s32	%fd1583, %r510;
	neg.f64 	%fd1584, %fd1583;
	fma.rn.f64 	%fd1586, %fd1584, %fd1363, %fd1846;
	fma.rn.f64 	%fd1588, %fd1584, %fd1365, %fd1586;
	fma.rn.f64 	%fd1847, %fd1584, %fd1367, %fd1588;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r431}, %fd1846;
	}
	and.b32  	%r432, %r431, 2145386496;
	setp.lt.u32	%p196, %r432, 1105199104;
	@%p196 bra 	BB0_297;

	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1846;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1847, [retval0+0];
	
	//{
	}// Callseq End 28
	ld.local.u32 	%r510, [%rd203];

BB0_297:
	add.s32 	%r136, %r510, 1;
	and.b32  	%r433, %r136, 1;
	shl.b32 	%r434, %r433, 3;
	setp.eq.b32	%p197, %r433, 1;
	selp.f64	%fd1590, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p197;
	mul.wide.u32 	%rd262, %r434, 8;
	add.s64 	%rd264, %rd262, %rd207;
	ld.const.f64 	%fd1591, [%rd264+8];
	mul.rn.f64 	%fd496, %fd1847, %fd1847;
	fma.rn.f64 	%fd1592, %fd1590, %fd496, %fd1591;
	ld.const.f64 	%fd1593, [%rd264+16];
	fma.rn.f64 	%fd1594, %fd1592, %fd496, %fd1593;
	ld.const.f64 	%fd1595, [%rd264+24];
	fma.rn.f64 	%fd1596, %fd1594, %fd496, %fd1595;
	ld.const.f64 	%fd1597, [%rd264+32];
	fma.rn.f64 	%fd1598, %fd1596, %fd496, %fd1597;
	ld.const.f64 	%fd1599, [%rd264+40];
	fma.rn.f64 	%fd1600, %fd1598, %fd496, %fd1599;
	ld.const.f64 	%fd1601, [%rd264+48];
	fma.rn.f64 	%fd497, %fd1600, %fd496, %fd1601;
	fma.rn.f64 	%fd1848, %fd497, %fd1847, %fd1847;
	setp.eq.s32	%p198, %r433, 0;
	@%p198 bra 	BB0_299;

	mov.f64 	%fd1602, 0d3FF0000000000000;
	fma.rn.f64 	%fd1848, %fd497, %fd496, %fd1602;

BB0_299:
	and.b32  	%r435, %r136, 2;
	setp.eq.s32	%p199, %r435, 0;
	@%p199 bra 	BB0_301;

	mov.f64 	%fd1603, 0d0000000000000000;
	mov.f64 	%fd1604, 0dBFF0000000000000;
	fma.rn.f64 	%fd1848, %fd1848, %fd1604, %fd1603;

BB0_301:
	ld.global.f64 	%fd1849, [%rd2+40];
	abs.f64 	%fd1605, %fd1849;
	setp.neu.f64	%p200, %fd1605, 0d7FF0000000000000;
	@%p200 bra 	BB0_303;

	mov.f64 	%fd1606, 0d0000000000000000;
	mul.rn.f64 	%fd1849, %fd1849, %fd1606;

BB0_303:
	mul.f64 	%fd1607, %fd1849, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r511, %fd1607;
	st.local.u32 	[%rd203], %r511;
	cvt.rn.f64.s32	%fd1608, %r511;
	neg.f64 	%fd1609, %fd1608;
	fma.rn.f64 	%fd1611, %fd1609, %fd1363, %fd1849;
	fma.rn.f64 	%fd1613, %fd1609, %fd1365, %fd1611;
	fma.rn.f64 	%fd1850, %fd1609, %fd1367, %fd1613;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r436}, %fd1849;
	}
	and.b32  	%r437, %r436, 2145386496;
	setp.lt.u32	%p201, %r437, 1105199104;
	@%p201 bra 	BB0_305;

	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1849;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd202;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1850, [retval0+0];
	
	//{
	}// Callseq End 29
	ld.local.u32 	%r511, [%rd203];

BB0_305:
	add.s32 	%r140, %r511, 1;
	and.b32  	%r438, %r140, 1;
	shl.b32 	%r439, %r438, 3;
	setp.eq.b32	%p202, %r438, 1;
	selp.f64	%fd1615, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p202;
	mul.wide.u32 	%rd269, %r439, 8;
	add.s64 	%rd271, %rd269, %rd207;
	ld.const.f64 	%fd1616, [%rd271+8];
	mul.rn.f64 	%fd509, %fd1850, %fd1850;
	fma.rn.f64 	%fd1617, %fd1615, %fd509, %fd1616;
	ld.const.f64 	%fd1618, [%rd271+16];
	fma.rn.f64 	%fd1619, %fd1617, %fd509, %fd1618;
	ld.const.f64 	%fd1620, [%rd271+24];
	fma.rn.f64 	%fd1621, %fd1619, %fd509, %fd1620;
	ld.const.f64 	%fd1622, [%rd271+32];
	fma.rn.f64 	%fd1623, %fd1621, %fd509, %fd1622;
	ld.const.f64 	%fd1624, [%rd271+40];
	fma.rn.f64 	%fd1625, %fd1623, %fd509, %fd1624;
	ld.const.f64 	%fd1626, [%rd271+48];
	fma.rn.f64 	%fd510, %fd1625, %fd509, %fd1626;
	fma.rn.f64 	%fd1851, %fd510, %fd1850, %fd1850;
	setp.eq.s32	%p203, %r438, 0;
	@%p203 bra 	BB0_307;

	mov.f64 	%fd1627, 0d3FF0000000000000;
	fma.rn.f64 	%fd1851, %fd510, %fd509, %fd1627;

BB0_307:
	and.b32  	%r440, %r140, 2;
	setp.eq.s32	%p204, %r440, 0;
	@%p204 bra 	BB0_309;

	mov.f64 	%fd1628, 0d0000000000000000;
	mov.f64 	%fd1629, 0dBFF0000000000000;
	fma.rn.f64 	%fd1851, %fd1851, %fd1629, %fd1628;

BB0_309:
	ld.global.f64 	%fd1632, [%rd2+32];
	add.f64 	%fd1633, %fd1632, %fd1632;
	mul.f64 	%fd1634, %fd1632, %fd1633;
	mul.f64 	%fd1635, %fd1848, %fd1851;
	div.rn.f64 	%fd1636, %fd1635, %fd1634;
	add.f64 	%fd516, %fd489, %fd1636;
	mov.f64 	%fd1854, 0d0000000000000000;
	mov.f64 	%fd1853, %fd1854;
	@%p2 bra 	BB0_315;

	ld.global.f64 	%fd517, [%rd2];
	ld.global.f64 	%fd518, [%rd2+8];
	add.f64 	%fd519, %fd462, %fd462;
	ld.global.f64 	%fd520, [%rd2+16];
	ld.global.f64 	%fd521, [%rd2+48];
	mul.lo.s32 	%r448, %r1, %r2;
	mul.lo.s32 	%r449, %r448, %r2;
	mul.wide.s32 	%rd276, %r449, 4;
	add.s64 	%rd280, %rd1, %rd276;
	mov.f64 	%fd1854, 0d0000000000000000;
	mov.u32 	%r512, 0;
	mov.f64 	%fd1853, %fd1854;

BB0_311:
	rem.s32 	%r450, %r512, %r2;
	cvt.rn.f64.s32	%fd1639, %r450;
	sub.f64 	%fd1640, %fd1639, %fd518;
	mul.f64 	%fd1641, %fd434, %fd1640;
	mul.f64 	%fd1642, %fd1640, %fd1641;
	mul.f64 	%fd1643, %fd519, %fd1640;
	div.s32 	%r451, %r512, %r2;
	cvt.rn.f64.s32	%fd1644, %r451;
	sub.f64 	%fd1645, %fd1644, %fd520;
	mul.f64 	%fd1646, %fd1643, %fd1645;
	sub.f64 	%fd1647, %fd1642, %fd1646;
	mul.f64 	%fd1648, %fd516, %fd1645;
	fma.rn.f64 	%fd524, %fd1645, %fd1648, %fd1647;
	neg.f64 	%fd1649, %fd524;
	mov.f64 	%fd1650, 0d4338000000000000;
	mov.f64 	%fd1651, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd1652, %fd1649, %fd1651, %fd1650;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r144, %temp}, %fd1652;
	}
	mov.f64 	%fd1653, 0dC338000000000000;
	add.rn.f64 	%fd1654, %fd1652, %fd1653;
	mov.f64 	%fd1655, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd1656, %fd1654, %fd1655, %fd1649;
	mov.f64 	%fd1657, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd1658, %fd1654, %fd1657, %fd1656;
	mov.f64 	%fd1659, 0d3E928AF3FCA213EA;
	mov.f64 	%fd1660, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd1661, %fd1660, %fd1658, %fd1659;
	mov.f64 	%fd1662, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd1663, %fd1661, %fd1658, %fd1662;
	mov.f64 	%fd1664, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd1665, %fd1663, %fd1658, %fd1664;
	mov.f64 	%fd1666, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd1667, %fd1665, %fd1658, %fd1666;
	mov.f64 	%fd1668, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd1669, %fd1667, %fd1658, %fd1668;
	mov.f64 	%fd1670, 0d3F81111111122322;
	fma.rn.f64 	%fd1671, %fd1669, %fd1658, %fd1670;
	mov.f64 	%fd1672, 0d3FA55555555502A1;
	fma.rn.f64 	%fd1673, %fd1671, %fd1658, %fd1672;
	mov.f64 	%fd1674, 0d3FC5555555555511;
	fma.rn.f64 	%fd1675, %fd1673, %fd1658, %fd1674;
	mov.f64 	%fd1676, 0d3FE000000000000B;
	fma.rn.f64 	%fd1677, %fd1675, %fd1658, %fd1676;
	mov.f64 	%fd1678, 0d3FF0000000000000;
	fma.rn.f64 	%fd1679, %fd1677, %fd1658, %fd1678;
	fma.rn.f64 	%fd1680, %fd1679, %fd1658, %fd1678;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r145, %temp}, %fd1680;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r146}, %fd1680;
	}
	shl.b32 	%r452, %r144, 20;
	add.s32 	%r453, %r146, %r452;
	mov.b64 	%fd1852, {%r145, %r453};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r454}, %fd1649;
	}
	mov.b32 	 %f10, %r454;
	abs.f32 	%f5, %f10;
	setp.lt.f32	%p206, %f5, 0f4086232B;
	@%p206 bra 	BB0_314;

	setp.gt.f64	%p207, %fd524, 0d8000000000000000;
	mov.f64 	%fd1681, 0d7FF0000000000000;
	sub.f64 	%fd1682, %fd1681, %fd524;
	selp.f64	%fd1852, 0d0000000000000000, %fd1682, %p207;
	setp.geu.f32	%p208, %f5, 0f40874800;
	@%p208 bra 	BB0_314;

	shr.u32 	%r455, %r144, 31;
	add.s32 	%r456, %r144, %r455;
	shr.s32 	%r457, %r456, 1;
	shl.b32 	%r458, %r457, 20;
	add.s32 	%r459, %r458, %r146;
	mov.b64 	%fd1683, {%r145, %r459};
	sub.s32 	%r460, %r144, %r457;
	shl.b32 	%r461, %r460, 20;
	add.s32 	%r462, %r461, 1072693248;
	mov.u32 	%r463, 0;
	mov.b64 	%fd1684, {%r463, %r462};
	mul.f64 	%fd1852, %fd1683, %fd1684;

BB0_314:
	fma.rn.f64 	%fd1685, %fd517, %fd1852, %fd521;
	add.f64 	%fd1853, %fd1853, %fd1685;
	ld.global.u32 	%r464, [%rd280];
	cvt.rn.f64.s32	%fd1686, %r464;
	sub.f64 	%fd1687, %fd1685, %fd1686;
	fma.rn.f64 	%fd1854, %fd1687, %fd1687, %fd1854;
	add.s64 	%rd280, %rd280, 4;
	add.s32 	%r512, %r512, 1;
	setp.lt.s32	%p209, %r512, %r3;
	@%p209 bra 	BB0_311;

BB0_315:
	st.global.f64 	[%rd2], %fd1853;
	div.rn.f64 	%fd1688, %fd1854, %fd1703;
	mov.f64 	%fd1689, 0d3FF0000000000000;
	sub.f64 	%fd1690, %fd1689, %fd1688;
	st.global.f64 	[%rd2+48], %fd1690;

BB0_316:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot1;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB1_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB1_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd92, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	neg.s32 	%r25, %r24;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd90, %rd45, 120;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r7;

BB1_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r39;
	mov.u64 	%rd7, %rd91;
	ld.const.u64 	%rd48, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd48;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd46, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd92], %rd46;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r26, %r9, %r7;
	mul.wide.s32 	%rd51, %r26, 8;
	add.s64 	%rd92, %rd1, %rd51;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd93, %rd13;
	add.s64 	%rd90, %rd90, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd91, %rd13;
	mov.u32 	%r39, %r9;
	@%p3 bra 	BB1_3;

BB1_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB1_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r10;
	shl.b64 	%rd52, %rd96, %r10;
	shr.u64 	%rd53, %rd95, %r28;
	or.b64  	%rd96, %rd52, %rd53;
	shl.b64 	%rd54, %rd95, %r10;
	ld.local.u64 	%rd55, [%rd1+8];
	shr.u64 	%rd56, %rd55, %r28;
	or.b64  	%rd95, %rd56, %rd54;

BB1_6:
	cvta.to.local.u64 	%rd57, %rd37;
	shr.u64 	%rd58, %rd96, 62;
	cvt.u32.u64	%r29, %rd58;
	shr.u64 	%rd59, %rd95, 62;
	shl.b64 	%rd60, %rd96, 2;
	or.b64  	%rd98, %rd60, %rd59;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd61, %rd96, 61;
	cvt.u32.u64	%r30, %rd61;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.ne.s32	%p5, %r40, 0;
	selp.b32	%r34, %r33, %r32, %p5;
	st.local.u32 	[%rd57], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB1_8;

	mov.u64 	%rd65, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd65;
	mov.b64         {a2,a3}, %rd65;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB1_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB1_10;

	shl.b64 	%rd68, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd69, %rd97, %r36;
	or.b64  	%rd98, %rd69, %rd68;

BB1_10:
	mov.u64 	%rd73, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd73;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd70, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB1_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd70;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd70;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd74, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB1_12:
	cvt.u64.u32	%rd80, %r40;
	shl.b64 	%rd81, %rd80, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd82, %r38;
	shl.b64 	%rd83, %rd82, 52;
	add.s64 	%rd84, %rd99, 1;
	shr.u64 	%rd85, %rd84, 10;
	add.s64 	%rd86, %rd85, 1;
	shr.u64 	%rd87, %rd86, 1;
	add.s64 	%rd88, %rd87, %rd83;
	or.b64  	%rd89, %rd88, %rd81;
	mov.b64 	 %fd4, %rd89;

BB1_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


