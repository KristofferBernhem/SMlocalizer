//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-20732876
// Cuda compilation tools, release 8.0, V8.0.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	gaussFitter
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry gaussFitter(
	.param .u64 gaussFitter_param_0,
	.param .u32 gaussFitter_param_1,
	.param .u64 gaussFitter_param_2,
	.param .u32 gaussFitter_param_3,
	.param .u32 gaussFitter_param_4,
	.param .u64 gaussFitter_param_5,
	.param .u32 gaussFitter_param_6,
	.param .u64 gaussFitter_param_7,
	.param .u32 gaussFitter_param_8,
	.param .f64 gaussFitter_param_9,
	.param .u32 gaussFitter_param_10
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<144>;
	.reg .b16 	%rs<16>;
	.reg .f32 	%f<356>;
	.reg .b32 	%r<356>;
	.reg .f64 	%fd<995>;
	.reg .b64 	%rd<204>;


	mov.u64 	%rd203, __local_depot0;
	cvta.local.u64 	%SP, %rd203;
	ld.param.u64 	%rd12, [gaussFitter_param_0];
	ld.param.u32 	%r102, [gaussFitter_param_1];
	ld.param.u64 	%rd13, [gaussFitter_param_2];
	ld.param.u32 	%r100, [gaussFitter_param_4];
	ld.param.u64 	%rd14, [gaussFitter_param_5];
	ld.param.u64 	%rd15, [gaussFitter_param_7];
	ld.param.f64 	%fd290, [gaussFitter_param_9];
	ld.param.u32 	%r101, [gaussFitter_param_10];
	cvta.to.global.u64 	%rd1, %rd12;
	mov.u32 	%r103, %ctaid.y;
	mov.u32 	%r104, %nctaid.x;
	mov.u32 	%r105, %ctaid.x;
	mad.lo.s32 	%r1, %r103, %r104, %r105;
	mul.lo.s32 	%r2, %r100, %r100;
	div.s32 	%r106, %r102, %r2;
	setp.ge.s32	%p1, %r1, %r106;
	@%p1 bra 	BB0_216;

	mul.lo.s32 	%r3, %r1, 7;
	mul.lo.s32 	%r4, %r1, %r2;
	setp.eq.s32	%p2, %r2, 0;
	mov.f32 	%f297, 0f00000000;
	mov.f32 	%f294, %f297;
	mov.f32 	%f291, %f297;
	mov.u32 	%r326, 0;
	mov.f32 	%f296, %f297;
	mov.f32 	%f293, %f297;
	mov.f32 	%f290, %f297;
	@%p2 bra 	BB0_3;

BB0_2:
	add.s32 	%r108, %r326, %r4;
	mul.wide.s32 	%rd16, %r108, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ldu.global.u32 	%r109, [%rd17];
	cvt.rn.f32.s32	%f116, %r109;
	add.f32 	%f297, %f297, %f116;
	rem.s32 	%r110, %r326, %r100;
	mul.lo.s32 	%r111, %r110, %r109;
	cvt.rn.f32.s32	%f117, %r111;
	add.f32 	%f291, %f291, %f117;
	div.s32 	%r112, %r326, %r100;
	mul.lo.s32 	%r113, %r109, %r112;
	cvt.rn.f32.s32	%f118, %r113;
	add.f32 	%f294, %f294, %f118;
	add.s32 	%r326, %r326, 1;
	setp.lt.s32	%p3, %r326, %r2;
	mov.f32 	%f290, %f291;
	mov.f32 	%f293, %f294;
	mov.f32 	%f296, %f297;
	@%p3 bra 	BB0_2;

BB0_3:
	cvta.to.global.u64 	%rd18, %rd13;
	mul.wide.s32 	%rd19, %r3, 4;
	add.s64 	%rd2, %rd18, %rd19;
	div.rn.f32 	%f121, %f290, %f296;
	st.global.f32 	[%rd2+4], %f121;
	div.rn.f32 	%f122, %f293, %f296;
	st.global.f32 	[%rd2+8], %f122;
	cvt.rn.f32.s32	%f123, %r2;
	div.rn.f32 	%f10, %f296, %f123;
	mov.f32 	%f300, 0f00000000;
	mov.u32 	%r327, 0;
	mov.f32 	%f299, %f300;
	@%p2 bra 	BB0_5;

BB0_4:
	add.s32 	%r115, %r327, %r4;
	mul.wide.s32 	%rd20, %r115, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u32 	%r116, [%rd21];
	cvt.rn.f32.s32	%f124, %r116;
	sub.f32 	%f125, %f124, %f10;
	fma.rn.f32 	%f300, %f125, %f125, %f300;
	add.s32 	%r327, %r327, 1;
	setp.lt.s32	%p5, %r327, %r2;
	mov.f32 	%f299, %f300;
	@%p5 bra 	BB0_4;

BB0_5:
	cvta.to.global.u64 	%rd22, %rd14;
	cvta.to.global.u64 	%rd23, %rd15;
	add.s64 	%rd25, %rd23, %rd19;
	ld.global.f32 	%f126, [%rd25];
	ld.global.f32 	%f127, [%rd2];
	mul.f32 	%f128, %f127, %f126;
	ld.global.f32 	%f129, [%rd25+24];
	ld.global.f32 	%f130, [%rd25+12];
	st.global.f32 	[%rd25], %f128;
	ld.global.f32 	%f131, [%rd2];
	mul.f32 	%f132, %f131, %f129;
	st.global.f32 	[%rd25+24], %f132;
	ld.global.f32 	%f14, [%rd22];
	ld.global.f32 	%f15, [%rd2];
	ld.global.f32 	%f16, [%rd22+4];
	ld.global.f32 	%f17, [%rd22+48];
	ld.global.f32 	%f18, [%rd22+52];
	add.f32 	%f133, %f130, %f130;
	ld.global.f32 	%f340, [%rd2+12];
	sub.f32 	%f341, %f340, %f133;
	add.f32 	%f134, %f340, %f133;
	setp.gtu.f32	%p6, %f341, %f134;
	@%p6 bra 	BB0_18;
	bra.uni 	BB0_6;

BB0_18:
	ld.global.f32 	%f321, [%rd2+16];
	mov.f32 	%f336, %f340;
	bra.uni 	BB0_19;

BB0_6:
	ld.global.f32 	%f325, [%rd2+16];
	ld.global.f32 	%f301, [%rd25+16];
	cvt.f64.f32	%fd1, %f15;
	mov.f32 	%f311, 0f3F800000;

BB0_7:
	mov.f32 	%f328, %f341;
	mov.f32 	%f332, %f340;
	mov.f32 	%f337, %f332;
	mov.f32 	%f26, %f328;
	mov.f32 	%f317, %f325;
	mov.f32 	%f322, %f317;
	mov.f32 	%f305, %f311;
	mov.f32 	%f308, %f305;
	add.f32 	%f136, %f26, %f26;
	mul.f32 	%f137, %f26, %f136;
	rcp.rn.f32 	%f28, %f137;
	add.f32 	%f138, %f301, %f301;
	sub.f32 	%f326, %f322, %f138;
	add.f32 	%f139, %f322, %f138;
	setp.gtu.f32	%p7, %f326, %f139;
	mov.f32 	%f310, %f308;
	mov.f32 	%f324, %f322;
	mov.f32 	%f339, %f337;
	@%p7 bra 	BB0_17;

BB0_8:
	mov.f32 	%f329, %f337;
	mov.f32 	%f32, %f329;
	mov.f32 	%f313, %f326;
	mov.f32 	%f314, %f322;
	mov.f32 	%f30, %f314;
	mov.f32 	%f33, %f313;
	mov.f32 	%f302, %f308;
	mov.f32 	%f34, %f302;
	mov.f32 	%f312, 0f00000000;
	@%p2 bra 	BB0_14;

	add.f32 	%f142, %f33, %f33;
	mul.f32 	%f143, %f33, %f142;
	rcp.rn.f32 	%f35, %f143;
	ld.global.f32 	%f36, [%rd2+4];
	ld.global.f32 	%f37, [%rd2+8];
	mov.f32 	%f312, 0f00000000;
	mov.u32 	%r328, 0;

BB0_10:
	rem.s32 	%r124, %r328, %r100;
	cvt.rn.f32.s32	%f144, %r124;
	sub.f32 	%f145, %f144, %f36;
	mul.f32 	%f146, %f28, %f145;
	mul.f32 	%f147, %f145, 0f80000000;
	div.s32 	%r125, %r328, %r100;
	cvt.rn.f32.s32	%f148, %r125;
	sub.f32 	%f149, %f148, %f37;
	mul.f32 	%f150, %f147, %f149;
	fma.rn.f32 	%f151, %f145, %f146, %f150;
	mul.f32 	%f152, %f35, %f149;
	fma.rn.f32 	%f39, %f149, %f152, %f151;
	cvt.f64.f32	%fd2, %f39;
	neg.f64 	%fd291, %fd2;
	mov.f64 	%fd292, 0d4338000000000000;
	mov.f64 	%fd293, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd294, %fd291, %fd293, %fd292;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd294;
	}
	mov.f64 	%fd295, 0dC338000000000000;
	add.rn.f64 	%fd296, %fd294, %fd295;
	mov.f64 	%fd297, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd298, %fd296, %fd297, %fd291;
	mov.f64 	%fd299, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd300, %fd296, %fd299, %fd298;
	mov.f64 	%fd301, 0d3E928AF3FCA213EA;
	mov.f64 	%fd302, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd303, %fd302, %fd300, %fd301;
	mov.f64 	%fd304, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd305, %fd303, %fd300, %fd304;
	mov.f64 	%fd306, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd307, %fd305, %fd300, %fd306;
	mov.f64 	%fd308, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd309, %fd307, %fd300, %fd308;
	mov.f64 	%fd310, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd311, %fd309, %fd300, %fd310;
	mov.f64 	%fd312, 0d3F81111111122322;
	fma.rn.f64 	%fd313, %fd311, %fd300, %fd312;
	mov.f64 	%fd314, 0d3FA55555555502A1;
	fma.rn.f64 	%fd315, %fd313, %fd300, %fd314;
	mov.f64 	%fd316, 0d3FC5555555555511;
	fma.rn.f64 	%fd317, %fd315, %fd300, %fd316;
	mov.f64 	%fd318, 0d3FE000000000000B;
	fma.rn.f64 	%fd319, %fd317, %fd300, %fd318;
	mov.f64 	%fd320, 0d3FF0000000000000;
	fma.rn.f64 	%fd321, %fd319, %fd300, %fd320;
	fma.rn.f64 	%fd322, %fd321, %fd300, %fd320;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r11, %temp}, %fd322;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd322;
	}
	shl.b32 	%r126, %r10, 20;
	add.s32 	%r127, %r12, %r126;
	mov.b64 	%fd932, {%r11, %r127};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r128}, %fd291;
	}
	mov.b32 	 %f153, %r128;
	abs.f32 	%f40, %f153;
	setp.lt.f32	%p9, %f40, 0f4086232B;
	@%p9 bra 	BB0_13;

	setp.gt.f32	%p10, %f39, 0f80000000;
	mov.f64 	%fd323, 0d7FF0000000000000;
	sub.f64 	%fd324, %fd323, %fd2;
	selp.f64	%fd932, 0d0000000000000000, %fd324, %p10;
	setp.geu.f32	%p11, %f40, 0f40874800;
	@%p11 bra 	BB0_13;

	shr.u32 	%r129, %r10, 31;
	add.s32 	%r130, %r10, %r129;
	shr.s32 	%r131, %r130, 1;
	shl.b32 	%r132, %r131, 20;
	add.s32 	%r133, %r132, %r12;
	mov.b64 	%fd325, {%r11, %r133};
	sub.s32 	%r134, %r10, %r131;
	shl.b32 	%r135, %r134, 20;
	add.s32 	%r136, %r135, 1072693248;
	mov.u32 	%r137, 0;
	mov.b64 	%fd326, {%r137, %r136};
	mul.f64 	%fd932, %fd325, %fd326;

BB0_13:
	mul.f64 	%fd327, %fd1, %fd932;
	cvt.rn.f32.f64	%f154, %fd327;
	add.s32 	%r138, %r328, %r4;
	mul.wide.s32 	%rd30, %r138, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.u32 	%r139, [%rd31];
	cvt.rn.f32.s32	%f155, %r139;
	sub.f32 	%f156, %f154, %f155;
	fma.rn.f32 	%f312, %f156, %f156, %f312;
	add.s32 	%r328, %r328, 1;
	setp.lt.s32	%p12, %r328, %r2;
	@%p12 bra 	BB0_10;

BB0_14:
	div.rn.f32 	%f43, %f312, %f299;
	setp.geu.f32	%p13, %f43, %f34;
	mov.f32 	%f309, %f34;
	mov.f32 	%f323, %f30;
	mov.f32 	%f338, %f32;
	@%p13 bra 	BB0_16;

	st.global.f32 	[%rd2+12], %f26;
	st.global.f32 	[%rd2+16], %f33;
	ld.global.f32 	%f301, [%rd25+16];
	mov.f32 	%f309, %f43;
	mov.f32 	%f323, %f33;
	mov.f32 	%f338, %f26;

BB0_16:
	mov.f32 	%f330, %f338;
	mov.f32 	%f337, %f330;
	mov.f32 	%f315, %f323;
	mov.f32 	%f322, %f315;
	mov.f32 	%f308, %f309;
	fma.rn.f32 	%f157, %f301, 0f40000000, %f322;
	add.f32 	%f326, %f33, %f301;
	setp.le.f32	%p14, %f326, %f157;
	mov.f32 	%f310, %f308;
	mov.f32 	%f324, %f322;
	mov.f32 	%f339, %f337;
	@%p14 bra 	BB0_8;

BB0_17:
	mov.f32 	%f340, %f339;
	mov.f32 	%f325, %f324;
	mov.f32 	%f311, %f310;
	ld.global.f32 	%f158, [%rd25+12];
	add.f32 	%f341, %f26, %f158;
	fma.rn.f32 	%f159, %f158, 0f40000000, %f340;
	setp.gtu.f32	%p15, %f341, %f159;
	mov.f32 	%f321, %f325;
	mov.f32 	%f336, %f340;
	@%p15 bra 	BB0_19;
	bra.uni 	BB0_7;

BB0_19:
	mul.f32 	%f58, %f15, %f14;
	mul.f32 	%f59, %f15, %f16;
	mul.f32 	%f60, %f15, %f17;
	mul.f32 	%f61, %f15, %f18;
	add.f32 	%f163, %f336, %f336;
	mul.f32 	%f164, %f336, %f163;
	rcp.rn.f32 	%f350, %f164;
	add.f32 	%f165, %f321, %f321;
	mul.f32 	%f166, %f321, %f165;
	rcp.rn.f32 	%f352, %f166;
	mov.u16 	%rs15, 1;
	mov.u32 	%r333, 0;
	mov.u32 	%r331, %r333;
	mov.f32 	%f348, 0f3F800000;
	mov.f32 	%f342, %f348;
	mov.f32 	%f351, 0f00000000;

BB0_20:
	mov.u32 	%r16, %r333;
	mov.f32 	%f68, %f348;
	setp.eq.s32	%p16, %r331, 0;
	selp.f32	%f342, %f68, %f342, %p16;
	add.s32 	%r157, %r331, %r3;
	mul.wide.s32 	%rd38, %r157, 4;
	add.s64 	%rd5, %rd18, %rd38;
	add.s64 	%rd6, %rd23, %rd38;
	@%p16 bra 	BB0_26;
	bra.uni 	BB0_21;

BB0_26:
	ld.global.f32 	%f173, [%rd25];
	ld.global.f32 	%f174, [%rd2];
	add.f32 	%f72, %f174, %f173;
	mov.u16 	%rs14, 0;
	setp.leu.f32	%p22, %f72, %f58;
	@%p22 bra 	BB0_28;

	setp.lt.f32	%p23, %f72, %f59;
	selp.u16	%rs14, 1, 0, %p23;
	bra.uni 	BB0_28;

BB0_21:
	setp.eq.s32	%p17, %r331, 6;
	@%p17 bra 	BB0_24;
	bra.uni 	BB0_22;

BB0_24:
	ld.global.f32 	%f171, [%rd25+24];
	ld.global.f32 	%f172, [%rd2+24];
	add.f32 	%f71, %f172, %f171;
	mov.u16 	%rs14, 0;
	setp.leu.f32	%p20, %f71, %f60;
	@%p20 bra 	BB0_28;

	setp.lt.f32	%p21, %f71, %f61;
	selp.u16	%rs14, 1, 0, %p21;
	bra.uni 	BB0_28;

BB0_22:
	ld.global.f32 	%f167, [%rd5];
	ld.global.f32 	%f168, [%rd6];
	add.f32 	%f70, %f167, %f168;
	shl.b32 	%r158, %r331, 1;
	mul.wide.s32 	%rd40, %r158, 4;
	add.s64 	%rd7, %rd22, %rd40;
	ld.global.f32 	%f169, [%rd7];
	mov.u16 	%rs14, 0;
	setp.leu.f32	%p18, %f70, %f169;
	@%p18 bra 	BB0_28;

	ld.global.f32 	%f170, [%rd7+4];
	setp.lt.f32	%p19, %f70, %f170;
	selp.u16	%rs14, 1, 0, %p19;

BB0_28:
	ld.global.f32 	%f73, [%rd6];
	setp.eq.s16	%p24, %rs14, 0;
	@%p24 bra 	BB0_121;
	bra.uni 	BB0_29;

BB0_121:
	setp.lt.f32	%p83, %f73, 0f00000000;
	@%p83 bra 	BB0_123;
	bra.uni 	BB0_122;

BB0_123:
	div.rn.f32 	%f232, %f73, 0fBFC00000;
	st.global.f32 	[%rd6], %f232;
	bra.uni 	BB0_124;

BB0_29:
	ld.global.f32 	%f175, [%rd5];
	add.f32 	%f176, %f73, %f175;
	st.global.f32 	[%rd5], %f176;
	add.s32 	%r169, %r331, -3;
	setp.gt.u32	%p25, %r169, 2;
	@%p25 bra 	BB0_111;

	ld.global.f32 	%f177, [%rd2+20];
	cvt.f64.f32	%fd933, %f177;
	abs.f64 	%fd328, %fd933;
	setp.neu.f64	%p26, %fd328, 0d7FF0000000000000;
	@%p26 bra 	BB0_32;

	mov.f64 	%fd329, 0d0000000000000000;
	mul.rn.f64 	%fd933, %fd933, %fd329;

BB0_32:
	mul.f64 	%fd330, %fd933, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r334, %fd330;
	add.u64 	%rd49, %SP, 0;
	cvta.to.local.u64 	%rd50, %rd49;
	st.local.u32 	[%rd50], %r334;
	cvt.rn.f64.s32	%fd331, %r334;
	neg.f64 	%fd332, %fd331;
	mov.f64 	%fd333, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd334, %fd332, %fd333, %fd933;
	mov.f64 	%fd335, 0d3C91A62633145C00;
	fma.rn.f64 	%fd336, %fd332, %fd335, %fd334;
	mov.f64 	%fd337, 0d397B839A252049C0;
	fma.rn.f64 	%fd934, %fd332, %fd337, %fd336;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r170}, %fd933;
	}
	and.b32  	%r171, %r170, 2145386496;
	setp.lt.u32	%p27, %r171, 1105199104;
	@%p27 bra 	BB0_34;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd933;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd934, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r334, [%rd50];

BB0_34:
	add.s32 	%r20, %r334, 1;
	and.b32  	%r172, %r20, 1;
	shl.b32 	%r173, %r172, 3;
	setp.eq.b32	%p28, %r172, 1;
	selp.f64	%fd338, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p28;
	mul.wide.u32 	%rd53, %r173, 8;
	mov.u64 	%rd54, __cudart_sin_cos_coeffs;
	add.s64 	%rd55, %rd53, %rd54;
	ld.const.f64 	%fd339, [%rd55+8];
	mul.rn.f64 	%fd13, %fd934, %fd934;
	fma.rn.f64 	%fd340, %fd338, %fd13, %fd339;
	ld.const.f64 	%fd341, [%rd55+16];
	fma.rn.f64 	%fd342, %fd340, %fd13, %fd341;
	ld.const.f64 	%fd343, [%rd55+24];
	fma.rn.f64 	%fd344, %fd342, %fd13, %fd343;
	ld.const.f64 	%fd345, [%rd55+32];
	fma.rn.f64 	%fd346, %fd344, %fd13, %fd345;
	ld.const.f64 	%fd347, [%rd55+40];
	fma.rn.f64 	%fd348, %fd346, %fd13, %fd347;
	ld.const.f64 	%fd349, [%rd55+48];
	fma.rn.f64 	%fd14, %fd348, %fd13, %fd349;
	fma.rn.f64 	%fd935, %fd14, %fd934, %fd934;
	setp.eq.s32	%p29, %r172, 0;
	@%p29 bra 	BB0_36;

	mov.f64 	%fd350, 0d3FF0000000000000;
	fma.rn.f64 	%fd935, %fd14, %fd13, %fd350;

BB0_36:
	and.b32  	%r174, %r20, 2;
	setp.eq.s32	%p30, %r174, 0;
	@%p30 bra 	BB0_38;

	mov.f64 	%fd351, 0d0000000000000000;
	mov.f64 	%fd352, 0dBFF0000000000000;
	fma.rn.f64 	%fd935, %fd935, %fd352, %fd351;

BB0_38:
	ld.global.f32 	%f178, [%rd2+20];
	cvt.f64.f32	%fd936, %f178;
	abs.f64 	%fd353, %fd936;
	setp.neu.f64	%p31, %fd353, 0d7FF0000000000000;
	@%p31 bra 	BB0_40;

	mov.f64 	%fd354, 0d0000000000000000;
	mul.rn.f64 	%fd936, %fd936, %fd354;

BB0_40:
	mul.f64 	%fd355, %fd936, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r335, %fd355;
	st.local.u32 	[%rd50], %r335;
	cvt.rn.f64.s32	%fd356, %r335;
	neg.f64 	%fd357, %fd356;
	fma.rn.f64 	%fd359, %fd357, %fd333, %fd936;
	fma.rn.f64 	%fd361, %fd357, %fd335, %fd359;
	fma.rn.f64 	%fd937, %fd357, %fd337, %fd361;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r175}, %fd936;
	}
	and.b32  	%r176, %r175, 2145386496;
	setp.lt.u32	%p32, %r176, 1105199104;
	@%p32 bra 	BB0_42;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd936;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd937, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r335, [%rd50];

BB0_42:
	add.s32 	%r24, %r335, 1;
	and.b32  	%r177, %r24, 1;
	shl.b32 	%r178, %r177, 3;
	setp.eq.b32	%p33, %r177, 1;
	selp.f64	%fd363, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p33;
	mul.wide.u32 	%rd60, %r178, 8;
	add.s64 	%rd62, %rd60, %rd54;
	ld.const.f64 	%fd364, [%rd62+8];
	mul.rn.f64 	%fd26, %fd937, %fd937;
	fma.rn.f64 	%fd365, %fd363, %fd26, %fd364;
	ld.const.f64 	%fd366, [%rd62+16];
	fma.rn.f64 	%fd367, %fd365, %fd26, %fd366;
	ld.const.f64 	%fd368, [%rd62+24];
	fma.rn.f64 	%fd369, %fd367, %fd26, %fd368;
	ld.const.f64 	%fd370, [%rd62+32];
	fma.rn.f64 	%fd371, %fd369, %fd26, %fd370;
	ld.const.f64 	%fd372, [%rd62+40];
	fma.rn.f64 	%fd373, %fd371, %fd26, %fd372;
	ld.const.f64 	%fd374, [%rd62+48];
	fma.rn.f64 	%fd27, %fd373, %fd26, %fd374;
	fma.rn.f64 	%fd938, %fd27, %fd937, %fd937;
	setp.eq.s32	%p34, %r177, 0;
	@%p34 bra 	BB0_44;

	mov.f64 	%fd375, 0d3FF0000000000000;
	fma.rn.f64 	%fd938, %fd27, %fd26, %fd375;

BB0_44:
	and.b32  	%r179, %r24, 2;
	setp.eq.s32	%p35, %r179, 0;
	@%p35 bra 	BB0_46;

	mov.f64 	%fd376, 0d0000000000000000;
	mov.f64 	%fd377, 0dBFF0000000000000;
	fma.rn.f64 	%fd938, %fd938, %fd377, %fd376;

BB0_46:
	ld.global.f32 	%f179, [%rd2+12];
	add.f32 	%f180, %f179, %f179;
	mul.f32 	%f181, %f179, %f180;
	cvt.f64.f32	%fd378, %f181;
	mul.f64 	%fd379, %fd935, %fd938;
	div.rn.f64 	%fd33, %fd379, %fd378;
	ld.global.f32 	%f182, [%rd2+20];
	cvt.f64.f32	%fd939, %f182;
	abs.f64 	%fd380, %fd939;
	setp.neu.f64	%p36, %fd380, 0d7FF0000000000000;
	@%p36 bra 	BB0_48;

	mov.f64 	%fd381, 0d0000000000000000;
	mul.rn.f64 	%fd939, %fd939, %fd381;

BB0_48:
	mul.f64 	%fd382, %fd939, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r336, %fd382;
	st.local.u32 	[%rd50], %r336;
	cvt.rn.f64.s32	%fd383, %r336;
	neg.f64 	%fd384, %fd383;
	fma.rn.f64 	%fd386, %fd384, %fd333, %fd939;
	fma.rn.f64 	%fd388, %fd384, %fd335, %fd386;
	fma.rn.f64 	%fd940, %fd384, %fd337, %fd388;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r180}, %fd939;
	}
	and.b32  	%r181, %r180, 2145386496;
	setp.lt.u32	%p37, %r181, 1105199104;
	@%p37 bra 	BB0_50;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd939;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd940, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r336, [%rd50];

BB0_50:
	and.b32  	%r182, %r336, 1;
	shl.b32 	%r183, %r182, 3;
	setp.eq.b32	%p38, %r182, 1;
	selp.f64	%fd390, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p38;
	mul.wide.u32 	%rd67, %r183, 8;
	add.s64 	%rd69, %rd67, %rd54;
	ld.const.f64 	%fd391, [%rd69+8];
	mul.rn.f64 	%fd40, %fd940, %fd940;
	fma.rn.f64 	%fd392, %fd390, %fd40, %fd391;
	ld.const.f64 	%fd393, [%rd69+16];
	fma.rn.f64 	%fd394, %fd392, %fd40, %fd393;
	ld.const.f64 	%fd395, [%rd69+24];
	fma.rn.f64 	%fd396, %fd394, %fd40, %fd395;
	ld.const.f64 	%fd397, [%rd69+32];
	fma.rn.f64 	%fd398, %fd396, %fd40, %fd397;
	ld.const.f64 	%fd399, [%rd69+40];
	fma.rn.f64 	%fd400, %fd398, %fd40, %fd399;
	ld.const.f64 	%fd401, [%rd69+48];
	fma.rn.f64 	%fd41, %fd400, %fd40, %fd401;
	fma.rn.f64 	%fd941, %fd41, %fd940, %fd940;
	setp.eq.s32	%p39, %r182, 0;
	@%p39 bra 	BB0_52;

	mov.f64 	%fd402, 0d3FF0000000000000;
	fma.rn.f64 	%fd941, %fd41, %fd40, %fd402;

BB0_52:
	and.b32  	%r184, %r336, 2;
	setp.eq.s32	%p40, %r184, 0;
	@%p40 bra 	BB0_54;

	mov.f64 	%fd403, 0d0000000000000000;
	mov.f64 	%fd404, 0dBFF0000000000000;
	fma.rn.f64 	%fd941, %fd941, %fd404, %fd403;

BB0_54:
	ld.global.f32 	%f183, [%rd2+20];
	cvt.f64.f32	%fd942, %f183;
	abs.f64 	%fd405, %fd942;
	setp.neu.f64	%p41, %fd405, 0d7FF0000000000000;
	@%p41 bra 	BB0_56;

	mov.f64 	%fd406, 0d0000000000000000;
	mul.rn.f64 	%fd942, %fd942, %fd406;

BB0_56:
	mul.f64 	%fd407, %fd942, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r337, %fd407;
	st.local.u32 	[%rd50], %r337;
	cvt.rn.f64.s32	%fd408, %r337;
	neg.f64 	%fd409, %fd408;
	fma.rn.f64 	%fd411, %fd409, %fd333, %fd942;
	fma.rn.f64 	%fd413, %fd409, %fd335, %fd411;
	fma.rn.f64 	%fd943, %fd409, %fd337, %fd413;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r185}, %fd942;
	}
	and.b32  	%r186, %r185, 2145386496;
	setp.lt.u32	%p42, %r186, 1105199104;
	@%p42 bra 	BB0_58;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd942;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd943, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r337, [%rd50];

BB0_58:
	and.b32  	%r187, %r337, 1;
	shl.b32 	%r188, %r187, 3;
	setp.eq.b32	%p43, %r187, 1;
	selp.f64	%fd415, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p43;
	mul.wide.u32 	%rd74, %r188, 8;
	add.s64 	%rd76, %rd74, %rd54;
	ld.const.f64 	%fd416, [%rd76+8];
	mul.rn.f64 	%fd53, %fd943, %fd943;
	fma.rn.f64 	%fd417, %fd415, %fd53, %fd416;
	ld.const.f64 	%fd418, [%rd76+16];
	fma.rn.f64 	%fd419, %fd417, %fd53, %fd418;
	ld.const.f64 	%fd420, [%rd76+24];
	fma.rn.f64 	%fd421, %fd419, %fd53, %fd420;
	ld.const.f64 	%fd422, [%rd76+32];
	fma.rn.f64 	%fd423, %fd421, %fd53, %fd422;
	ld.const.f64 	%fd424, [%rd76+40];
	fma.rn.f64 	%fd425, %fd423, %fd53, %fd424;
	ld.const.f64 	%fd426, [%rd76+48];
	fma.rn.f64 	%fd54, %fd425, %fd53, %fd426;
	fma.rn.f64 	%fd944, %fd54, %fd943, %fd943;
	setp.eq.s32	%p44, %r187, 0;
	@%p44 bra 	BB0_60;

	mov.f64 	%fd427, 0d3FF0000000000000;
	fma.rn.f64 	%fd944, %fd54, %fd53, %fd427;

BB0_60:
	and.b32  	%r189, %r337, 2;
	setp.eq.s32	%p45, %r189, 0;
	@%p45 bra 	BB0_62;

	mov.f64 	%fd428, 0d0000000000000000;
	mov.f64 	%fd429, 0dBFF0000000000000;
	fma.rn.f64 	%fd944, %fd944, %fd429, %fd428;

BB0_62:
	ld.global.f32 	%f184, [%rd2+16];
	add.f32 	%f185, %f184, %f184;
	mul.f32 	%f186, %f184, %f185;
	cvt.f64.f32	%fd430, %f186;
	mul.f64 	%fd431, %fd941, %fd944;
	div.rn.f64 	%fd432, %fd431, %fd430;
	add.f64 	%fd60, %fd33, %fd432;
	ld.global.f32 	%f187, [%rd2+20];
	add.f32 	%f188, %f187, %f187;
	cvt.f64.f32	%fd945, %f188;
	abs.f64 	%fd433, %fd945;
	setp.neu.f64	%p46, %fd433, 0d7FF0000000000000;
	@%p46 bra 	BB0_64;

	mov.f64 	%fd434, 0d0000000000000000;
	mul.rn.f64 	%fd945, %fd945, %fd434;

BB0_64:
	mul.f64 	%fd435, %fd945, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r338, %fd435;
	st.local.u32 	[%rd50], %r338;
	cvt.rn.f64.s32	%fd436, %r338;
	neg.f64 	%fd437, %fd436;
	fma.rn.f64 	%fd439, %fd437, %fd333, %fd945;
	fma.rn.f64 	%fd441, %fd437, %fd335, %fd439;
	fma.rn.f64 	%fd946, %fd437, %fd337, %fd441;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r190}, %fd945;
	}
	and.b32  	%r191, %r190, 2145386496;
	setp.lt.u32	%p47, %r191, 1105199104;
	@%p47 bra 	BB0_66;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd945;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd946, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r338, [%rd50];

BB0_66:
	and.b32  	%r192, %r338, 1;
	shl.b32 	%r193, %r192, 3;
	setp.eq.b32	%p48, %r192, 1;
	selp.f64	%fd443, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p48;
	mul.wide.u32 	%rd81, %r193, 8;
	add.s64 	%rd83, %rd81, %rd54;
	ld.const.f64 	%fd444, [%rd83+8];
	mul.rn.f64 	%fd67, %fd946, %fd946;
	fma.rn.f64 	%fd445, %fd443, %fd67, %fd444;
	ld.const.f64 	%fd446, [%rd83+16];
	fma.rn.f64 	%fd447, %fd445, %fd67, %fd446;
	ld.const.f64 	%fd448, [%rd83+24];
	fma.rn.f64 	%fd449, %fd447, %fd67, %fd448;
	ld.const.f64 	%fd450, [%rd83+32];
	fma.rn.f64 	%fd451, %fd449, %fd67, %fd450;
	ld.const.f64 	%fd452, [%rd83+40];
	fma.rn.f64 	%fd453, %fd451, %fd67, %fd452;
	ld.const.f64 	%fd454, [%rd83+48];
	fma.rn.f64 	%fd68, %fd453, %fd67, %fd454;
	fma.rn.f64 	%fd947, %fd68, %fd946, %fd946;
	setp.eq.s32	%p49, %r192, 0;
	@%p49 bra 	BB0_68;

	mov.f64 	%fd455, 0d3FF0000000000000;
	fma.rn.f64 	%fd947, %fd68, %fd67, %fd455;

BB0_68:
	and.b32  	%r194, %r338, 2;
	setp.eq.s32	%p50, %r194, 0;
	@%p50 bra 	BB0_70;

	mov.f64 	%fd456, 0d0000000000000000;
	mov.f64 	%fd457, 0dBFF0000000000000;
	fma.rn.f64 	%fd947, %fd947, %fd457, %fd456;

BB0_70:
	cvt.rn.f32.f64	%f350, %fd60;
	cvt.rn.f32.f64	%f189, %fd947;
	cvt.f64.f32	%fd458, %f189;
	ld.global.f32 	%f190, [%rd2+12];
	mul.f32 	%f191, %f190, 0fC0800000;
	mul.f32 	%f192, %f190, %f191;
	cvt.f64.f32	%fd459, %f192;
	div.rn.f64 	%fd74, %fd458, %fd459;
	ld.global.f32 	%f193, [%rd2+20];
	add.f32 	%f194, %f193, %f193;
	cvt.f64.f32	%fd948, %f194;
	abs.f64 	%fd460, %fd948;
	setp.neu.f64	%p51, %fd460, 0d7FF0000000000000;
	@%p51 bra 	BB0_72;

	mov.f64 	%fd461, 0d0000000000000000;
	mul.rn.f64 	%fd948, %fd948, %fd461;

BB0_72:
	mul.f64 	%fd462, %fd948, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r339, %fd462;
	st.local.u32 	[%rd50], %r339;
	cvt.rn.f64.s32	%fd463, %r339;
	neg.f64 	%fd464, %fd463;
	fma.rn.f64 	%fd466, %fd464, %fd333, %fd948;
	fma.rn.f64 	%fd468, %fd464, %fd335, %fd466;
	fma.rn.f64 	%fd949, %fd464, %fd337, %fd468;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r195}, %fd948;
	}
	and.b32  	%r196, %r195, 2145386496;
	setp.lt.u32	%p52, %r196, 1105199104;
	@%p52 bra 	BB0_74;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd948;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd949, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r339, [%rd50];

BB0_74:
	and.b32  	%r197, %r339, 1;
	shl.b32 	%r198, %r197, 3;
	setp.eq.b32	%p53, %r197, 1;
	selp.f64	%fd470, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p53;
	mul.wide.u32 	%rd88, %r198, 8;
	add.s64 	%rd90, %rd88, %rd54;
	ld.const.f64 	%fd471, [%rd90+8];
	mul.rn.f64 	%fd81, %fd949, %fd949;
	fma.rn.f64 	%fd472, %fd470, %fd81, %fd471;
	ld.const.f64 	%fd473, [%rd90+16];
	fma.rn.f64 	%fd474, %fd472, %fd81, %fd473;
	ld.const.f64 	%fd475, [%rd90+24];
	fma.rn.f64 	%fd476, %fd474, %fd81, %fd475;
	ld.const.f64 	%fd477, [%rd90+32];
	fma.rn.f64 	%fd478, %fd476, %fd81, %fd477;
	ld.const.f64 	%fd479, [%rd90+40];
	fma.rn.f64 	%fd480, %fd478, %fd81, %fd479;
	ld.const.f64 	%fd481, [%rd90+48];
	fma.rn.f64 	%fd82, %fd480, %fd81, %fd481;
	fma.rn.f64 	%fd950, %fd82, %fd949, %fd949;
	setp.eq.s32	%p54, %r197, 0;
	@%p54 bra 	BB0_76;

	mov.f64 	%fd482, 0d3FF0000000000000;
	fma.rn.f64 	%fd950, %fd82, %fd81, %fd482;

BB0_76:
	and.b32  	%r199, %r339, 2;
	setp.eq.s32	%p55, %r199, 0;
	@%p55 bra 	BB0_78;

	mov.f64 	%fd483, 0d0000000000000000;
	mov.f64 	%fd484, 0dBFF0000000000000;
	fma.rn.f64 	%fd950, %fd950, %fd484, %fd483;

BB0_78:
	ld.global.f32 	%f195, [%rd2+16];
	mul.f32 	%f196, %f195, 0f40800000;
	mul.f32 	%f197, %f195, %f196;
	cvt.f64.f32	%fd485, %f197;
	div.rn.f64 	%fd486, %fd950, %fd485;
	add.f64 	%fd88, %fd74, %fd486;
	ld.global.f32 	%f198, [%rd2+20];
	cvt.f64.f32	%fd951, %f198;
	abs.f64 	%fd487, %fd951;
	setp.neu.f64	%p56, %fd487, 0d7FF0000000000000;
	@%p56 bra 	BB0_80;

	mov.f64 	%fd488, 0d0000000000000000;
	mul.rn.f64 	%fd951, %fd951, %fd488;

BB0_80:
	mul.f64 	%fd489, %fd951, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r340, %fd489;
	st.local.u32 	[%rd50], %r340;
	cvt.rn.f64.s32	%fd490, %r340;
	neg.f64 	%fd491, %fd490;
	fma.rn.f64 	%fd493, %fd491, %fd333, %fd951;
	fma.rn.f64 	%fd495, %fd491, %fd335, %fd493;
	fma.rn.f64 	%fd952, %fd491, %fd337, %fd495;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r200}, %fd951;
	}
	and.b32  	%r201, %r200, 2145386496;
	setp.lt.u32	%p57, %r201, 1105199104;
	@%p57 bra 	BB0_82;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd951;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd952, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r340, [%rd50];

BB0_82:
	and.b32  	%r202, %r340, 1;
	shl.b32 	%r203, %r202, 3;
	setp.eq.b32	%p58, %r202, 1;
	selp.f64	%fd497, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p58;
	mul.wide.u32 	%rd95, %r203, 8;
	add.s64 	%rd97, %rd95, %rd54;
	ld.const.f64 	%fd498, [%rd97+8];
	mul.rn.f64 	%fd95, %fd952, %fd952;
	fma.rn.f64 	%fd499, %fd497, %fd95, %fd498;
	ld.const.f64 	%fd500, [%rd97+16];
	fma.rn.f64 	%fd501, %fd499, %fd95, %fd500;
	ld.const.f64 	%fd502, [%rd97+24];
	fma.rn.f64 	%fd503, %fd501, %fd95, %fd502;
	ld.const.f64 	%fd504, [%rd97+32];
	fma.rn.f64 	%fd505, %fd503, %fd95, %fd504;
	ld.const.f64 	%fd506, [%rd97+40];
	fma.rn.f64 	%fd507, %fd505, %fd95, %fd506;
	ld.const.f64 	%fd508, [%rd97+48];
	fma.rn.f64 	%fd96, %fd507, %fd95, %fd508;
	fma.rn.f64 	%fd953, %fd96, %fd952, %fd952;
	setp.eq.s32	%p59, %r202, 0;
	@%p59 bra 	BB0_84;

	mov.f64 	%fd509, 0d3FF0000000000000;
	fma.rn.f64 	%fd953, %fd96, %fd95, %fd509;

BB0_84:
	and.b32  	%r204, %r340, 2;
	setp.eq.s32	%p60, %r204, 0;
	@%p60 bra 	BB0_86;

	mov.f64 	%fd510, 0d0000000000000000;
	mov.f64 	%fd511, 0dBFF0000000000000;
	fma.rn.f64 	%fd953, %fd953, %fd511, %fd510;

BB0_86:
	ld.global.f32 	%f199, [%rd2+20];
	cvt.f64.f32	%fd954, %f199;
	abs.f64 	%fd512, %fd954;
	setp.neu.f64	%p61, %fd512, 0d7FF0000000000000;
	@%p61 bra 	BB0_88;

	mov.f64 	%fd513, 0d0000000000000000;
	mul.rn.f64 	%fd954, %fd954, %fd513;

BB0_88:
	mul.f64 	%fd514, %fd954, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r341, %fd514;
	st.local.u32 	[%rd50], %r341;
	cvt.rn.f64.s32	%fd515, %r341;
	neg.f64 	%fd516, %fd515;
	fma.rn.f64 	%fd518, %fd516, %fd333, %fd954;
	fma.rn.f64 	%fd520, %fd516, %fd335, %fd518;
	fma.rn.f64 	%fd955, %fd516, %fd337, %fd520;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r205}, %fd954;
	}
	and.b32  	%r206, %r205, 2145386496;
	setp.lt.u32	%p62, %r206, 1105199104;
	@%p62 bra 	BB0_90;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd954;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd955, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r341, [%rd50];

BB0_90:
	and.b32  	%r207, %r341, 1;
	shl.b32 	%r208, %r207, 3;
	setp.eq.b32	%p63, %r207, 1;
	selp.f64	%fd522, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p63;
	mul.wide.u32 	%rd102, %r208, 8;
	add.s64 	%rd104, %rd102, %rd54;
	ld.const.f64 	%fd523, [%rd104+8];
	mul.rn.f64 	%fd108, %fd955, %fd955;
	fma.rn.f64 	%fd524, %fd522, %fd108, %fd523;
	ld.const.f64 	%fd525, [%rd104+16];
	fma.rn.f64 	%fd526, %fd524, %fd108, %fd525;
	ld.const.f64 	%fd527, [%rd104+24];
	fma.rn.f64 	%fd528, %fd526, %fd108, %fd527;
	ld.const.f64 	%fd529, [%rd104+32];
	fma.rn.f64 	%fd530, %fd528, %fd108, %fd529;
	ld.const.f64 	%fd531, [%rd104+40];
	fma.rn.f64 	%fd532, %fd530, %fd108, %fd531;
	ld.const.f64 	%fd533, [%rd104+48];
	fma.rn.f64 	%fd109, %fd532, %fd108, %fd533;
	fma.rn.f64 	%fd956, %fd109, %fd955, %fd955;
	setp.eq.s32	%p64, %r207, 0;
	@%p64 bra 	BB0_92;

	mov.f64 	%fd534, 0d3FF0000000000000;
	fma.rn.f64 	%fd956, %fd109, %fd108, %fd534;

BB0_92:
	and.b32  	%r209, %r341, 2;
	setp.eq.s32	%p65, %r209, 0;
	@%p65 bra 	BB0_94;

	mov.f64 	%fd535, 0d0000000000000000;
	mov.f64 	%fd536, 0dBFF0000000000000;
	fma.rn.f64 	%fd956, %fd956, %fd536, %fd535;

BB0_94:
	cvt.rn.f32.f64	%f351, %fd88;
	ld.global.f32 	%f200, [%rd2+12];
	add.f32 	%f201, %f200, %f200;
	mul.f32 	%f202, %f200, %f201;
	cvt.f64.f32	%fd537, %f202;
	mul.f64 	%fd538, %fd953, %fd956;
	div.rn.f64 	%fd115, %fd538, %fd537;
	ld.global.f32 	%f203, [%rd2+20];
	cvt.f64.f32	%fd957, %f203;
	abs.f64 	%fd539, %fd957;
	setp.neu.f64	%p66, %fd539, 0d7FF0000000000000;
	@%p66 bra 	BB0_96;

	mov.f64 	%fd540, 0d0000000000000000;
	mul.rn.f64 	%fd957, %fd957, %fd540;

BB0_96:
	mul.f64 	%fd541, %fd957, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r342, %fd541;
	st.local.u32 	[%rd50], %r342;
	cvt.rn.f64.s32	%fd542, %r342;
	neg.f64 	%fd543, %fd542;
	fma.rn.f64 	%fd545, %fd543, %fd333, %fd957;
	fma.rn.f64 	%fd547, %fd543, %fd335, %fd545;
	fma.rn.f64 	%fd958, %fd543, %fd337, %fd547;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r210}, %fd957;
	}
	and.b32  	%r211, %r210, 2145386496;
	setp.lt.u32	%p67, %r211, 1105199104;
	@%p67 bra 	BB0_98;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd957;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd958, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r342, [%rd50];

BB0_98:
	add.s32 	%r46, %r342, 1;
	and.b32  	%r212, %r46, 1;
	shl.b32 	%r213, %r212, 3;
	setp.eq.b32	%p68, %r212, 1;
	selp.f64	%fd549, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p68;
	mul.wide.u32 	%rd109, %r213, 8;
	add.s64 	%rd111, %rd109, %rd54;
	ld.const.f64 	%fd550, [%rd111+8];
	mul.rn.f64 	%fd122, %fd958, %fd958;
	fma.rn.f64 	%fd551, %fd549, %fd122, %fd550;
	ld.const.f64 	%fd552, [%rd111+16];
	fma.rn.f64 	%fd553, %fd551, %fd122, %fd552;
	ld.const.f64 	%fd554, [%rd111+24];
	fma.rn.f64 	%fd555, %fd553, %fd122, %fd554;
	ld.const.f64 	%fd556, [%rd111+32];
	fma.rn.f64 	%fd557, %fd555, %fd122, %fd556;
	ld.const.f64 	%fd558, [%rd111+40];
	fma.rn.f64 	%fd559, %fd557, %fd122, %fd558;
	ld.const.f64 	%fd560, [%rd111+48];
	fma.rn.f64 	%fd123, %fd559, %fd122, %fd560;
	fma.rn.f64 	%fd959, %fd123, %fd958, %fd958;
	setp.eq.s32	%p69, %r212, 0;
	@%p69 bra 	BB0_100;

	mov.f64 	%fd561, 0d3FF0000000000000;
	fma.rn.f64 	%fd959, %fd123, %fd122, %fd561;

BB0_100:
	and.b32  	%r214, %r46, 2;
	setp.eq.s32	%p70, %r214, 0;
	@%p70 bra 	BB0_102;

	mov.f64 	%fd562, 0d0000000000000000;
	mov.f64 	%fd563, 0dBFF0000000000000;
	fma.rn.f64 	%fd959, %fd959, %fd563, %fd562;

BB0_102:
	ld.global.f32 	%f204, [%rd2+20];
	cvt.f64.f32	%fd960, %f204;
	abs.f64 	%fd564, %fd960;
	setp.neu.f64	%p71, %fd564, 0d7FF0000000000000;
	@%p71 bra 	BB0_104;

	mov.f64 	%fd565, 0d0000000000000000;
	mul.rn.f64 	%fd960, %fd960, %fd565;

BB0_104:
	mul.f64 	%fd566, %fd960, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r343, %fd566;
	st.local.u32 	[%rd50], %r343;
	cvt.rn.f64.s32	%fd567, %r343;
	neg.f64 	%fd568, %fd567;
	fma.rn.f64 	%fd570, %fd568, %fd333, %fd960;
	fma.rn.f64 	%fd572, %fd568, %fd335, %fd570;
	fma.rn.f64 	%fd961, %fd568, %fd337, %fd572;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r215}, %fd960;
	}
	and.b32  	%r216, %r215, 2145386496;
	setp.lt.u32	%p72, %r216, 1105199104;
	@%p72 bra 	BB0_106;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd960;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd49;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd961, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r343, [%rd50];

BB0_106:
	add.s32 	%r50, %r343, 1;
	and.b32  	%r217, %r50, 1;
	shl.b32 	%r218, %r217, 3;
	setp.eq.b32	%p73, %r217, 1;
	selp.f64	%fd574, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p73;
	mul.wide.u32 	%rd116, %r218, 8;
	add.s64 	%rd118, %rd116, %rd54;
	ld.const.f64 	%fd575, [%rd118+8];
	mul.rn.f64 	%fd135, %fd961, %fd961;
	fma.rn.f64 	%fd576, %fd574, %fd135, %fd575;
	ld.const.f64 	%fd577, [%rd118+16];
	fma.rn.f64 	%fd578, %fd576, %fd135, %fd577;
	ld.const.f64 	%fd579, [%rd118+24];
	fma.rn.f64 	%fd580, %fd578, %fd135, %fd579;
	ld.const.f64 	%fd581, [%rd118+32];
	fma.rn.f64 	%fd582, %fd580, %fd135, %fd581;
	ld.const.f64 	%fd583, [%rd118+40];
	fma.rn.f64 	%fd584, %fd582, %fd135, %fd583;
	ld.const.f64 	%fd585, [%rd118+48];
	fma.rn.f64 	%fd136, %fd584, %fd135, %fd585;
	fma.rn.f64 	%fd962, %fd136, %fd961, %fd961;
	setp.eq.s32	%p74, %r217, 0;
	@%p74 bra 	BB0_108;

	mov.f64 	%fd586, 0d3FF0000000000000;
	fma.rn.f64 	%fd962, %fd136, %fd135, %fd586;

BB0_108:
	and.b32  	%r219, %r50, 2;
	setp.eq.s32	%p75, %r219, 0;
	@%p75 bra 	BB0_110;

	mov.f64 	%fd587, 0d0000000000000000;
	mov.f64 	%fd588, 0dBFF0000000000000;
	fma.rn.f64 	%fd962, %fd962, %fd588, %fd587;

BB0_110:
	ld.global.f32 	%f205, [%rd2+16];
	add.f32 	%f206, %f205, %f205;
	mul.f32 	%f207, %f205, %f206;
	cvt.f64.f32	%fd589, %f207;
	mul.f64 	%fd590, %fd959, %fd962;
	div.rn.f64 	%fd591, %fd590, %fd589;
	add.f64 	%fd592, %fd115, %fd591;
	cvt.rn.f32.f64	%f352, %fd592;

BB0_111:
	mov.f32 	%f353, 0f00000000;
	@%p2 bra 	BB0_117;

	ld.global.f32 	%f210, [%rd2];
	cvt.f64.f32	%fd142, %f210;
	ld.global.f32 	%f80, [%rd2+4];
	ld.global.f32 	%f82, [%rd2+8];
	ld.global.f32 	%f83, [%rd2+24];
	mov.f32 	%f353, 0f00000000;
	mov.u32 	%r344, 0;

BB0_113:
	add.f32 	%f288, %f351, %f351;
	rem.s32 	%r226, %r344, %r100;
	cvt.rn.f32.s32	%f211, %r226;
	sub.f32 	%f212, %f211, %f80;
	mul.f32 	%f213, %f350, %f212;
	mul.f32 	%f214, %f212, %f213;
	mul.f32 	%f215, %f288, %f212;
	div.s32 	%r227, %r344, %r100;
	cvt.rn.f32.s32	%f216, %r227;
	sub.f32 	%f217, %f216, %f82;
	mul.f32 	%f218, %f215, %f217;
	sub.f32 	%f219, %f214, %f218;
	mul.f32 	%f220, %f352, %f217;
	fma.rn.f32 	%f85, %f217, %f220, %f219;
	cvt.f64.f32	%fd143, %f85;
	neg.f64 	%fd593, %fd143;
	mov.f64 	%fd594, 0d4338000000000000;
	mov.f64 	%fd595, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd596, %fd593, %fd595, %fd594;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd596;
	}
	mov.f64 	%fd597, 0dC338000000000000;
	add.rn.f64 	%fd598, %fd596, %fd597;
	mov.f64 	%fd599, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd600, %fd598, %fd599, %fd593;
	mov.f64 	%fd601, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd602, %fd598, %fd601, %fd600;
	mov.f64 	%fd603, 0d3E928AF3FCA213EA;
	mov.f64 	%fd604, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd605, %fd604, %fd602, %fd603;
	mov.f64 	%fd606, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd607, %fd605, %fd602, %fd606;
	mov.f64 	%fd608, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd609, %fd607, %fd602, %fd608;
	mov.f64 	%fd610, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd611, %fd609, %fd602, %fd610;
	mov.f64 	%fd612, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd613, %fd611, %fd602, %fd612;
	mov.f64 	%fd614, 0d3F81111111122322;
	fma.rn.f64 	%fd615, %fd613, %fd602, %fd614;
	mov.f64 	%fd616, 0d3FA55555555502A1;
	fma.rn.f64 	%fd617, %fd615, %fd602, %fd616;
	mov.f64 	%fd618, 0d3FC5555555555511;
	fma.rn.f64 	%fd619, %fd617, %fd602, %fd618;
	mov.f64 	%fd620, 0d3FE000000000000B;
	fma.rn.f64 	%fd621, %fd619, %fd602, %fd620;
	mov.f64 	%fd622, 0d3FF0000000000000;
	fma.rn.f64 	%fd623, %fd621, %fd602, %fd622;
	fma.rn.f64 	%fd624, %fd623, %fd602, %fd622;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd624;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd624;
	}
	shl.b32 	%r228, %r53, 20;
	add.s32 	%r229, %r55, %r228;
	mov.b64 	%fd963, {%r54, %r229};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r230}, %fd593;
	}
	mov.b32 	 %f221, %r230;
	abs.f32 	%f86, %f221;
	setp.lt.f32	%p77, %f86, 0f4086232B;
	@%p77 bra 	BB0_116;

	setp.gt.f32	%p78, %f85, 0f80000000;
	mov.f64 	%fd625, 0d7FF0000000000000;
	sub.f64 	%fd626, %fd625, %fd143;
	selp.f64	%fd963, 0d0000000000000000, %fd626, %p78;
	setp.geu.f32	%p79, %f86, 0f40874800;
	@%p79 bra 	BB0_116;

	shr.u32 	%r231, %r53, 31;
	add.s32 	%r232, %r53, %r231;
	shr.s32 	%r233, %r232, 1;
	shl.b32 	%r234, %r233, 20;
	add.s32 	%r235, %r234, %r55;
	mov.b64 	%fd627, {%r54, %r235};
	sub.s32 	%r236, %r53, %r233;
	shl.b32 	%r237, %r236, 20;
	add.s32 	%r238, %r237, 1072693248;
	mov.u32 	%r239, 0;
	mov.b64 	%fd628, {%r239, %r238};
	mul.f64 	%fd963, %fd627, %fd628;

BB0_116:
	mul.f64 	%fd629, %fd142, %fd963;
	cvt.rn.f32.f64	%f222, %fd629;
	add.f32 	%f223, %f83, %f222;
	add.s32 	%r240, %r344, %r4;
	mul.wide.s32 	%rd122, %r240, 4;
	add.s64 	%rd123, %rd1, %rd122;
	ld.global.u32 	%r241, [%rd123];
	cvt.rn.f32.s32	%f224, %r241;
	sub.f32 	%f225, %f223, %f224;
	fma.rn.f32 	%f353, %f225, %f225, %f353;
	add.s32 	%r344, %r344, 1;
	setp.lt.s32	%p80, %r344, %r2;
	@%p80 bra 	BB0_113;

BB0_117:
	div.rn.f32 	%f89, %f353, %f299;
	setp.lt.f32	%p81, %f89, %f68;
	mov.f32 	%f349, %f89;
	@%p81 bra 	BB0_125;

	ld.global.f32 	%f226, [%rd6];
	ld.global.f32 	%f227, [%rd5];
	sub.f32 	%f228, %f227, %f226;
	st.global.f32 	[%rd5], %f228;
	ld.global.f32 	%f90, [%rd6];
	setp.lt.f32	%p82, %f90, 0f00000000;
	@%p82 bra 	BB0_120;
	bra.uni 	BB0_119;

BB0_120:
	div.rn.f32 	%f230, %f90, 0fBFC00000;
	st.global.f32 	[%rd6], %f230;
	bra.uni 	BB0_124;

BB0_122:
	neg.f32 	%f231, %f73;
	st.global.f32 	[%rd6], %f231;
	bra.uni 	BB0_124;

BB0_119:
	neg.f32 	%f229, %f90;
	st.global.f32 	[%rd6], %f229;

BB0_124:
	mov.f32 	%f349, %f68;

BB0_125:
	mov.f32 	%f348, %f349;
	add.s32 	%r332, %r331, 1;
	setp.lt.s32	%p84, %r331, 6;
	@%p84 bra 	BB0_128;

	mov.u32 	%r332, 0;
	setp.lt.s32	%p85, %r16, 61;
	@%p85 bra 	BB0_128;

	sub.f32 	%f233, %f342, %f348;
	cvt.f64.f32	%fd630, %f233;
	setp.lt.f64	%p86, %fd630, %fd290;
	selp.b16	%rs15, 0, %rs15, %p86;

BB0_128:
	mov.u32 	%r331, %r332;
	add.s32 	%r333, %r16, 1;
	setp.ge.s32	%p87, %r16, %r101;
	selp.b16	%rs15, 0, %rs15, %p87;
	and.b16  	%rs13, %rs15, 255;
	setp.ne.s16	%p88, %rs13, 0;
	@%p88 bra 	BB0_20;

	ld.global.f32 	%f234, [%rd2+20];
	cvt.f64.f32	%fd964, %f234;
	abs.f64 	%fd631, %fd964;
	setp.neu.f64	%p89, %fd631, 0d7FF0000000000000;
	@%p89 bra 	BB0_131;

	mov.f64 	%fd632, 0d0000000000000000;
	mul.rn.f64 	%fd964, %fd964, %fd632;

BB0_131:
	mul.f64 	%fd633, %fd964, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r345, %fd633;
	add.u64 	%rd124, %SP, 0;
	cvta.to.local.u64 	%rd125, %rd124;
	st.local.u32 	[%rd125], %r345;
	cvt.rn.f64.s32	%fd634, %r345;
	neg.f64 	%fd635, %fd634;
	mov.f64 	%fd636, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd637, %fd635, %fd636, %fd964;
	mov.f64 	%fd638, 0d3C91A62633145C00;
	fma.rn.f64 	%fd639, %fd635, %fd638, %fd637;
	mov.f64 	%fd640, 0d397B839A252049C0;
	fma.rn.f64 	%fd965, %fd635, %fd640, %fd639;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r244}, %fd964;
	}
	and.b32  	%r245, %r244, 2145386496;
	setp.lt.u32	%p90, %r245, 1105199104;
	@%p90 bra 	BB0_133;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd964;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd965, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r345, [%rd125];

BB0_133:
	add.s32 	%r63, %r345, 1;
	and.b32  	%r246, %r63, 1;
	shl.b32 	%r247, %r246, 3;
	setp.eq.b32	%p91, %r246, 1;
	selp.f64	%fd641, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p91;
	mul.wide.u32 	%rd128, %r247, 8;
	mov.u64 	%rd129, __cudart_sin_cos_coeffs;
	add.s64 	%rd130, %rd128, %rd129;
	ld.const.f64 	%fd642, [%rd130+8];
	mul.rn.f64 	%fd154, %fd965, %fd965;
	fma.rn.f64 	%fd643, %fd641, %fd154, %fd642;
	ld.const.f64 	%fd644, [%rd130+16];
	fma.rn.f64 	%fd645, %fd643, %fd154, %fd644;
	ld.const.f64 	%fd646, [%rd130+24];
	fma.rn.f64 	%fd647, %fd645, %fd154, %fd646;
	ld.const.f64 	%fd648, [%rd130+32];
	fma.rn.f64 	%fd649, %fd647, %fd154, %fd648;
	ld.const.f64 	%fd650, [%rd130+40];
	fma.rn.f64 	%fd651, %fd649, %fd154, %fd650;
	ld.const.f64 	%fd652, [%rd130+48];
	fma.rn.f64 	%fd155, %fd651, %fd154, %fd652;
	fma.rn.f64 	%fd966, %fd155, %fd965, %fd965;
	setp.eq.s32	%p92, %r246, 0;
	@%p92 bra 	BB0_135;

	mov.f64 	%fd653, 0d3FF0000000000000;
	fma.rn.f64 	%fd966, %fd155, %fd154, %fd653;

BB0_135:
	and.b32  	%r248, %r63, 2;
	setp.eq.s32	%p93, %r248, 0;
	@%p93 bra 	BB0_137;

	mov.f64 	%fd654, 0d0000000000000000;
	mov.f64 	%fd655, 0dBFF0000000000000;
	fma.rn.f64 	%fd966, %fd966, %fd655, %fd654;

BB0_137:
	ld.global.f32 	%f235, [%rd2+20];
	cvt.f64.f32	%fd967, %f235;
	abs.f64 	%fd656, %fd967;
	setp.neu.f64	%p94, %fd656, 0d7FF0000000000000;
	@%p94 bra 	BB0_139;

	mov.f64 	%fd657, 0d0000000000000000;
	mul.rn.f64 	%fd967, %fd967, %fd657;

BB0_139:
	mul.f64 	%fd658, %fd967, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r346, %fd658;
	st.local.u32 	[%rd125], %r346;
	cvt.rn.f64.s32	%fd659, %r346;
	neg.f64 	%fd660, %fd659;
	fma.rn.f64 	%fd662, %fd660, %fd636, %fd967;
	fma.rn.f64 	%fd664, %fd660, %fd638, %fd662;
	fma.rn.f64 	%fd968, %fd660, %fd640, %fd664;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r249}, %fd967;
	}
	and.b32  	%r250, %r249, 2145386496;
	setp.lt.u32	%p95, %r250, 1105199104;
	@%p95 bra 	BB0_141;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd967;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd968, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r346, [%rd125];

BB0_141:
	add.s32 	%r67, %r346, 1;
	and.b32  	%r251, %r67, 1;
	shl.b32 	%r252, %r251, 3;
	setp.eq.b32	%p96, %r251, 1;
	selp.f64	%fd666, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p96;
	mul.wide.u32 	%rd135, %r252, 8;
	add.s64 	%rd137, %rd135, %rd129;
	ld.const.f64 	%fd667, [%rd137+8];
	mul.rn.f64 	%fd167, %fd968, %fd968;
	fma.rn.f64 	%fd668, %fd666, %fd167, %fd667;
	ld.const.f64 	%fd669, [%rd137+16];
	fma.rn.f64 	%fd670, %fd668, %fd167, %fd669;
	ld.const.f64 	%fd671, [%rd137+24];
	fma.rn.f64 	%fd672, %fd670, %fd167, %fd671;
	ld.const.f64 	%fd673, [%rd137+32];
	fma.rn.f64 	%fd674, %fd672, %fd167, %fd673;
	ld.const.f64 	%fd675, [%rd137+40];
	fma.rn.f64 	%fd676, %fd674, %fd167, %fd675;
	ld.const.f64 	%fd677, [%rd137+48];
	fma.rn.f64 	%fd168, %fd676, %fd167, %fd677;
	fma.rn.f64 	%fd969, %fd168, %fd968, %fd968;
	setp.eq.s32	%p97, %r251, 0;
	@%p97 bra 	BB0_143;

	mov.f64 	%fd678, 0d3FF0000000000000;
	fma.rn.f64 	%fd969, %fd168, %fd167, %fd678;

BB0_143:
	and.b32  	%r253, %r67, 2;
	setp.eq.s32	%p98, %r253, 0;
	@%p98 bra 	BB0_145;

	mov.f64 	%fd679, 0d0000000000000000;
	mov.f64 	%fd680, 0dBFF0000000000000;
	fma.rn.f64 	%fd969, %fd969, %fd680, %fd679;

BB0_145:
	ld.global.f32 	%f236, [%rd2+12];
	add.f32 	%f237, %f236, %f236;
	mul.f32 	%f238, %f236, %f237;
	cvt.f64.f32	%fd681, %f238;
	mul.f64 	%fd682, %fd966, %fd969;
	div.rn.f64 	%fd174, %fd682, %fd681;
	ld.global.f32 	%f239, [%rd2+20];
	cvt.f64.f32	%fd970, %f239;
	abs.f64 	%fd683, %fd970;
	setp.neu.f64	%p99, %fd683, 0d7FF0000000000000;
	@%p99 bra 	BB0_147;

	mov.f64 	%fd684, 0d0000000000000000;
	mul.rn.f64 	%fd970, %fd970, %fd684;

BB0_147:
	mul.f64 	%fd685, %fd970, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r347, %fd685;
	st.local.u32 	[%rd125], %r347;
	cvt.rn.f64.s32	%fd686, %r347;
	neg.f64 	%fd687, %fd686;
	fma.rn.f64 	%fd689, %fd687, %fd636, %fd970;
	fma.rn.f64 	%fd691, %fd687, %fd638, %fd689;
	fma.rn.f64 	%fd971, %fd687, %fd640, %fd691;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r254}, %fd970;
	}
	and.b32  	%r255, %r254, 2145386496;
	setp.lt.u32	%p100, %r255, 1105199104;
	@%p100 bra 	BB0_149;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd970;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd971, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r347, [%rd125];

BB0_149:
	and.b32  	%r256, %r347, 1;
	shl.b32 	%r257, %r256, 3;
	setp.eq.b32	%p101, %r256, 1;
	selp.f64	%fd693, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p101;
	mul.wide.u32 	%rd142, %r257, 8;
	add.s64 	%rd144, %rd142, %rd129;
	ld.const.f64 	%fd694, [%rd144+8];
	mul.rn.f64 	%fd181, %fd971, %fd971;
	fma.rn.f64 	%fd695, %fd693, %fd181, %fd694;
	ld.const.f64 	%fd696, [%rd144+16];
	fma.rn.f64 	%fd697, %fd695, %fd181, %fd696;
	ld.const.f64 	%fd698, [%rd144+24];
	fma.rn.f64 	%fd699, %fd697, %fd181, %fd698;
	ld.const.f64 	%fd700, [%rd144+32];
	fma.rn.f64 	%fd701, %fd699, %fd181, %fd700;
	ld.const.f64 	%fd702, [%rd144+40];
	fma.rn.f64 	%fd703, %fd701, %fd181, %fd702;
	ld.const.f64 	%fd704, [%rd144+48];
	fma.rn.f64 	%fd182, %fd703, %fd181, %fd704;
	fma.rn.f64 	%fd972, %fd182, %fd971, %fd971;
	setp.eq.s32	%p102, %r256, 0;
	@%p102 bra 	BB0_151;

	mov.f64 	%fd705, 0d3FF0000000000000;
	fma.rn.f64 	%fd972, %fd182, %fd181, %fd705;

BB0_151:
	and.b32  	%r258, %r347, 2;
	setp.eq.s32	%p103, %r258, 0;
	@%p103 bra 	BB0_153;

	mov.f64 	%fd706, 0d0000000000000000;
	mov.f64 	%fd707, 0dBFF0000000000000;
	fma.rn.f64 	%fd972, %fd972, %fd707, %fd706;

BB0_153:
	ld.global.f32 	%f240, [%rd2+20];
	cvt.f64.f32	%fd973, %f240;
	abs.f64 	%fd708, %fd973;
	setp.neu.f64	%p104, %fd708, 0d7FF0000000000000;
	@%p104 bra 	BB0_155;

	mov.f64 	%fd709, 0d0000000000000000;
	mul.rn.f64 	%fd973, %fd973, %fd709;

BB0_155:
	mul.f64 	%fd710, %fd973, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r348, %fd710;
	st.local.u32 	[%rd125], %r348;
	cvt.rn.f64.s32	%fd711, %r348;
	neg.f64 	%fd712, %fd711;
	fma.rn.f64 	%fd714, %fd712, %fd636, %fd973;
	fma.rn.f64 	%fd716, %fd712, %fd638, %fd714;
	fma.rn.f64 	%fd974, %fd712, %fd640, %fd716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r259}, %fd973;
	}
	and.b32  	%r260, %r259, 2145386496;
	setp.lt.u32	%p105, %r260, 1105199104;
	@%p105 bra 	BB0_157;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd973;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd974, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r348, [%rd125];

BB0_157:
	and.b32  	%r261, %r348, 1;
	shl.b32 	%r262, %r261, 3;
	setp.eq.b32	%p106, %r261, 1;
	selp.f64	%fd718, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p106;
	mul.wide.u32 	%rd149, %r262, 8;
	add.s64 	%rd151, %rd149, %rd129;
	ld.const.f64 	%fd719, [%rd151+8];
	mul.rn.f64 	%fd194, %fd974, %fd974;
	fma.rn.f64 	%fd720, %fd718, %fd194, %fd719;
	ld.const.f64 	%fd721, [%rd151+16];
	fma.rn.f64 	%fd722, %fd720, %fd194, %fd721;
	ld.const.f64 	%fd723, [%rd151+24];
	fma.rn.f64 	%fd724, %fd722, %fd194, %fd723;
	ld.const.f64 	%fd725, [%rd151+32];
	fma.rn.f64 	%fd726, %fd724, %fd194, %fd725;
	ld.const.f64 	%fd727, [%rd151+40];
	fma.rn.f64 	%fd728, %fd726, %fd194, %fd727;
	ld.const.f64 	%fd729, [%rd151+48];
	fma.rn.f64 	%fd195, %fd728, %fd194, %fd729;
	fma.rn.f64 	%fd975, %fd195, %fd974, %fd974;
	setp.eq.s32	%p107, %r261, 0;
	@%p107 bra 	BB0_159;

	mov.f64 	%fd730, 0d3FF0000000000000;
	fma.rn.f64 	%fd975, %fd195, %fd194, %fd730;

BB0_159:
	and.b32  	%r263, %r348, 2;
	setp.eq.s32	%p108, %r263, 0;
	@%p108 bra 	BB0_161;

	mov.f64 	%fd731, 0d0000000000000000;
	mov.f64 	%fd732, 0dBFF0000000000000;
	fma.rn.f64 	%fd975, %fd975, %fd732, %fd731;

BB0_161:
	ld.global.f32 	%f241, [%rd2+16];
	add.f32 	%f242, %f241, %f241;
	mul.f32 	%f243, %f241, %f242;
	cvt.f64.f32	%fd733, %f243;
	mul.f64 	%fd734, %fd972, %fd975;
	div.rn.f64 	%fd735, %fd734, %fd733;
	add.f64 	%fd201, %fd174, %fd735;
	ld.global.f32 	%f244, [%rd2+20];
	add.f32 	%f245, %f244, %f244;
	cvt.f64.f32	%fd976, %f245;
	abs.f64 	%fd736, %fd976;
	setp.neu.f64	%p109, %fd736, 0d7FF0000000000000;
	@%p109 bra 	BB0_163;

	mov.f64 	%fd737, 0d0000000000000000;
	mul.rn.f64 	%fd976, %fd976, %fd737;

BB0_163:
	mul.f64 	%fd738, %fd976, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r349, %fd738;
	st.local.u32 	[%rd125], %r349;
	cvt.rn.f64.s32	%fd739, %r349;
	neg.f64 	%fd740, %fd739;
	fma.rn.f64 	%fd742, %fd740, %fd636, %fd976;
	fma.rn.f64 	%fd744, %fd740, %fd638, %fd742;
	fma.rn.f64 	%fd977, %fd740, %fd640, %fd744;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r264}, %fd976;
	}
	and.b32  	%r265, %r264, 2145386496;
	setp.lt.u32	%p110, %r265, 1105199104;
	@%p110 bra 	BB0_165;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd976;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd977, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r349, [%rd125];

BB0_165:
	and.b32  	%r266, %r349, 1;
	shl.b32 	%r267, %r266, 3;
	setp.eq.b32	%p111, %r266, 1;
	selp.f64	%fd746, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p111;
	mul.wide.u32 	%rd156, %r267, 8;
	add.s64 	%rd158, %rd156, %rd129;
	ld.const.f64 	%fd747, [%rd158+8];
	mul.rn.f64 	%fd208, %fd977, %fd977;
	fma.rn.f64 	%fd748, %fd746, %fd208, %fd747;
	ld.const.f64 	%fd749, [%rd158+16];
	fma.rn.f64 	%fd750, %fd748, %fd208, %fd749;
	ld.const.f64 	%fd751, [%rd158+24];
	fma.rn.f64 	%fd752, %fd750, %fd208, %fd751;
	ld.const.f64 	%fd753, [%rd158+32];
	fma.rn.f64 	%fd754, %fd752, %fd208, %fd753;
	ld.const.f64 	%fd755, [%rd158+40];
	fma.rn.f64 	%fd756, %fd754, %fd208, %fd755;
	ld.const.f64 	%fd757, [%rd158+48];
	fma.rn.f64 	%fd209, %fd756, %fd208, %fd757;
	fma.rn.f64 	%fd978, %fd209, %fd977, %fd977;
	setp.eq.s32	%p112, %r266, 0;
	@%p112 bra 	BB0_167;

	mov.f64 	%fd758, 0d3FF0000000000000;
	fma.rn.f64 	%fd978, %fd209, %fd208, %fd758;

BB0_167:
	and.b32  	%r268, %r349, 2;
	setp.eq.s32	%p113, %r268, 0;
	@%p113 bra 	BB0_169;

	mov.f64 	%fd759, 0d0000000000000000;
	mov.f64 	%fd760, 0dBFF0000000000000;
	fma.rn.f64 	%fd978, %fd978, %fd760, %fd759;

BB0_169:
	cvt.rn.f32.f64	%f95, %fd201;
	cvt.rn.f32.f64	%f246, %fd978;
	cvt.f64.f32	%fd761, %f246;
	ld.global.f32 	%f247, [%rd2+12];
	mul.f32 	%f248, %f247, 0fC0800000;
	mul.f32 	%f249, %f247, %f248;
	cvt.f64.f32	%fd762, %f249;
	div.rn.f64 	%fd215, %fd761, %fd762;
	ld.global.f32 	%f250, [%rd2+20];
	add.f32 	%f251, %f250, %f250;
	cvt.f64.f32	%fd979, %f251;
	abs.f64 	%fd763, %fd979;
	setp.neu.f64	%p114, %fd763, 0d7FF0000000000000;
	@%p114 bra 	BB0_171;

	mov.f64 	%fd764, 0d0000000000000000;
	mul.rn.f64 	%fd979, %fd979, %fd764;

BB0_171:
	mul.f64 	%fd765, %fd979, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r350, %fd765;
	st.local.u32 	[%rd125], %r350;
	cvt.rn.f64.s32	%fd766, %r350;
	neg.f64 	%fd767, %fd766;
	fma.rn.f64 	%fd769, %fd767, %fd636, %fd979;
	fma.rn.f64 	%fd771, %fd767, %fd638, %fd769;
	fma.rn.f64 	%fd980, %fd767, %fd640, %fd771;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r269}, %fd979;
	}
	and.b32  	%r270, %r269, 2145386496;
	setp.lt.u32	%p115, %r270, 1105199104;
	@%p115 bra 	BB0_173;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd979;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd980, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r350, [%rd125];

BB0_173:
	and.b32  	%r271, %r350, 1;
	shl.b32 	%r272, %r271, 3;
	setp.eq.b32	%p116, %r271, 1;
	selp.f64	%fd773, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p116;
	mul.wide.u32 	%rd163, %r272, 8;
	add.s64 	%rd165, %rd163, %rd129;
	ld.const.f64 	%fd774, [%rd165+8];
	mul.rn.f64 	%fd222, %fd980, %fd980;
	fma.rn.f64 	%fd775, %fd773, %fd222, %fd774;
	ld.const.f64 	%fd776, [%rd165+16];
	fma.rn.f64 	%fd777, %fd775, %fd222, %fd776;
	ld.const.f64 	%fd778, [%rd165+24];
	fma.rn.f64 	%fd779, %fd777, %fd222, %fd778;
	ld.const.f64 	%fd780, [%rd165+32];
	fma.rn.f64 	%fd781, %fd779, %fd222, %fd780;
	ld.const.f64 	%fd782, [%rd165+40];
	fma.rn.f64 	%fd783, %fd781, %fd222, %fd782;
	ld.const.f64 	%fd784, [%rd165+48];
	fma.rn.f64 	%fd223, %fd783, %fd222, %fd784;
	fma.rn.f64 	%fd981, %fd223, %fd980, %fd980;
	setp.eq.s32	%p117, %r271, 0;
	@%p117 bra 	BB0_175;

	mov.f64 	%fd785, 0d3FF0000000000000;
	fma.rn.f64 	%fd981, %fd223, %fd222, %fd785;

BB0_175:
	and.b32  	%r273, %r350, 2;
	setp.eq.s32	%p118, %r273, 0;
	@%p118 bra 	BB0_177;

	mov.f64 	%fd786, 0d0000000000000000;
	mov.f64 	%fd787, 0dBFF0000000000000;
	fma.rn.f64 	%fd981, %fd981, %fd787, %fd786;

BB0_177:
	ld.global.f32 	%f252, [%rd2+16];
	mul.f32 	%f253, %f252, 0f40800000;
	mul.f32 	%f254, %f252, %f253;
	cvt.f64.f32	%fd788, %f254;
	div.rn.f64 	%fd789, %fd981, %fd788;
	add.f64 	%fd229, %fd215, %fd789;
	ld.global.f32 	%f255, [%rd2+20];
	cvt.f64.f32	%fd982, %f255;
	abs.f64 	%fd790, %fd982;
	setp.neu.f64	%p119, %fd790, 0d7FF0000000000000;
	@%p119 bra 	BB0_179;

	mov.f64 	%fd791, 0d0000000000000000;
	mul.rn.f64 	%fd982, %fd982, %fd791;

BB0_179:
	mul.f64 	%fd792, %fd982, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r351, %fd792;
	st.local.u32 	[%rd125], %r351;
	cvt.rn.f64.s32	%fd793, %r351;
	neg.f64 	%fd794, %fd793;
	fma.rn.f64 	%fd796, %fd794, %fd636, %fd982;
	fma.rn.f64 	%fd798, %fd794, %fd638, %fd796;
	fma.rn.f64 	%fd983, %fd794, %fd640, %fd798;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r274}, %fd982;
	}
	and.b32  	%r275, %r274, 2145386496;
	setp.lt.u32	%p120, %r275, 1105199104;
	@%p120 bra 	BB0_181;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd982;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd983, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r351, [%rd125];

BB0_181:
	and.b32  	%r276, %r351, 1;
	shl.b32 	%r277, %r276, 3;
	setp.eq.b32	%p121, %r276, 1;
	selp.f64	%fd800, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p121;
	mul.wide.u32 	%rd170, %r277, 8;
	add.s64 	%rd172, %rd170, %rd129;
	ld.const.f64 	%fd801, [%rd172+8];
	mul.rn.f64 	%fd236, %fd983, %fd983;
	fma.rn.f64 	%fd802, %fd800, %fd236, %fd801;
	ld.const.f64 	%fd803, [%rd172+16];
	fma.rn.f64 	%fd804, %fd802, %fd236, %fd803;
	ld.const.f64 	%fd805, [%rd172+24];
	fma.rn.f64 	%fd806, %fd804, %fd236, %fd805;
	ld.const.f64 	%fd807, [%rd172+32];
	fma.rn.f64 	%fd808, %fd806, %fd236, %fd807;
	ld.const.f64 	%fd809, [%rd172+40];
	fma.rn.f64 	%fd810, %fd808, %fd236, %fd809;
	ld.const.f64 	%fd811, [%rd172+48];
	fma.rn.f64 	%fd237, %fd810, %fd236, %fd811;
	fma.rn.f64 	%fd984, %fd237, %fd983, %fd983;
	setp.eq.s32	%p122, %r276, 0;
	@%p122 bra 	BB0_183;

	mov.f64 	%fd812, 0d3FF0000000000000;
	fma.rn.f64 	%fd984, %fd237, %fd236, %fd812;

BB0_183:
	and.b32  	%r278, %r351, 2;
	setp.eq.s32	%p123, %r278, 0;
	@%p123 bra 	BB0_185;

	mov.f64 	%fd813, 0d0000000000000000;
	mov.f64 	%fd814, 0dBFF0000000000000;
	fma.rn.f64 	%fd984, %fd984, %fd814, %fd813;

BB0_185:
	ld.global.f32 	%f256, [%rd2+20];
	cvt.f64.f32	%fd985, %f256;
	abs.f64 	%fd815, %fd985;
	setp.neu.f64	%p124, %fd815, 0d7FF0000000000000;
	@%p124 bra 	BB0_187;

	mov.f64 	%fd816, 0d0000000000000000;
	mul.rn.f64 	%fd985, %fd985, %fd816;

BB0_187:
	mul.f64 	%fd817, %fd985, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r352, %fd817;
	st.local.u32 	[%rd125], %r352;
	cvt.rn.f64.s32	%fd818, %r352;
	neg.f64 	%fd819, %fd818;
	fma.rn.f64 	%fd821, %fd819, %fd636, %fd985;
	fma.rn.f64 	%fd823, %fd819, %fd638, %fd821;
	fma.rn.f64 	%fd986, %fd819, %fd640, %fd823;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r279}, %fd985;
	}
	and.b32  	%r280, %r279, 2145386496;
	setp.lt.u32	%p125, %r280, 1105199104;
	@%p125 bra 	BB0_189;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd985;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd986, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r352, [%rd125];

BB0_189:
	and.b32  	%r281, %r352, 1;
	shl.b32 	%r282, %r281, 3;
	setp.eq.b32	%p126, %r281, 1;
	selp.f64	%fd825, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p126;
	mul.wide.u32 	%rd177, %r282, 8;
	add.s64 	%rd179, %rd177, %rd129;
	ld.const.f64 	%fd826, [%rd179+8];
	mul.rn.f64 	%fd249, %fd986, %fd986;
	fma.rn.f64 	%fd827, %fd825, %fd249, %fd826;
	ld.const.f64 	%fd828, [%rd179+16];
	fma.rn.f64 	%fd829, %fd827, %fd249, %fd828;
	ld.const.f64 	%fd830, [%rd179+24];
	fma.rn.f64 	%fd831, %fd829, %fd249, %fd830;
	ld.const.f64 	%fd832, [%rd179+32];
	fma.rn.f64 	%fd833, %fd831, %fd249, %fd832;
	ld.const.f64 	%fd834, [%rd179+40];
	fma.rn.f64 	%fd835, %fd833, %fd249, %fd834;
	ld.const.f64 	%fd836, [%rd179+48];
	fma.rn.f64 	%fd250, %fd835, %fd249, %fd836;
	fma.rn.f64 	%fd987, %fd250, %fd986, %fd986;
	setp.eq.s32	%p127, %r281, 0;
	@%p127 bra 	BB0_191;

	mov.f64 	%fd837, 0d3FF0000000000000;
	fma.rn.f64 	%fd987, %fd250, %fd249, %fd837;

BB0_191:
	and.b32  	%r283, %r352, 2;
	setp.eq.s32	%p128, %r283, 0;
	@%p128 bra 	BB0_193;

	mov.f64 	%fd838, 0d0000000000000000;
	mov.f64 	%fd839, 0dBFF0000000000000;
	fma.rn.f64 	%fd987, %fd987, %fd839, %fd838;

BB0_193:
	cvt.rn.f32.f64	%f96, %fd229;
	ld.global.f32 	%f257, [%rd2+12];
	add.f32 	%f258, %f257, %f257;
	mul.f32 	%f259, %f257, %f258;
	cvt.f64.f32	%fd840, %f259;
	mul.f64 	%fd841, %fd984, %fd987;
	div.rn.f64 	%fd256, %fd841, %fd840;
	ld.global.f32 	%f260, [%rd2+20];
	cvt.f64.f32	%fd988, %f260;
	abs.f64 	%fd842, %fd988;
	setp.neu.f64	%p129, %fd842, 0d7FF0000000000000;
	@%p129 bra 	BB0_195;

	mov.f64 	%fd843, 0d0000000000000000;
	mul.rn.f64 	%fd988, %fd988, %fd843;

BB0_195:
	mul.f64 	%fd844, %fd988, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r353, %fd844;
	st.local.u32 	[%rd125], %r353;
	cvt.rn.f64.s32	%fd845, %r353;
	neg.f64 	%fd846, %fd845;
	fma.rn.f64 	%fd848, %fd846, %fd636, %fd988;
	fma.rn.f64 	%fd850, %fd846, %fd638, %fd848;
	fma.rn.f64 	%fd989, %fd846, %fd640, %fd850;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r284}, %fd988;
	}
	and.b32  	%r285, %r284, 2145386496;
	setp.lt.u32	%p130, %r285, 1105199104;
	@%p130 bra 	BB0_197;

	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd988;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd989, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r353, [%rd125];

BB0_197:
	add.s32 	%r89, %r353, 1;
	and.b32  	%r286, %r89, 1;
	shl.b32 	%r287, %r286, 3;
	setp.eq.b32	%p131, %r286, 1;
	selp.f64	%fd852, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p131;
	mul.wide.u32 	%rd184, %r287, 8;
	add.s64 	%rd186, %rd184, %rd129;
	ld.const.f64 	%fd853, [%rd186+8];
	mul.rn.f64 	%fd263, %fd989, %fd989;
	fma.rn.f64 	%fd854, %fd852, %fd263, %fd853;
	ld.const.f64 	%fd855, [%rd186+16];
	fma.rn.f64 	%fd856, %fd854, %fd263, %fd855;
	ld.const.f64 	%fd857, [%rd186+24];
	fma.rn.f64 	%fd858, %fd856, %fd263, %fd857;
	ld.const.f64 	%fd859, [%rd186+32];
	fma.rn.f64 	%fd860, %fd858, %fd263, %fd859;
	ld.const.f64 	%fd861, [%rd186+40];
	fma.rn.f64 	%fd862, %fd860, %fd263, %fd861;
	ld.const.f64 	%fd863, [%rd186+48];
	fma.rn.f64 	%fd264, %fd862, %fd263, %fd863;
	fma.rn.f64 	%fd990, %fd264, %fd989, %fd989;
	setp.eq.s32	%p132, %r286, 0;
	@%p132 bra 	BB0_199;

	mov.f64 	%fd864, 0d3FF0000000000000;
	fma.rn.f64 	%fd990, %fd264, %fd263, %fd864;

BB0_199:
	and.b32  	%r288, %r89, 2;
	setp.eq.s32	%p133, %r288, 0;
	@%p133 bra 	BB0_201;

	mov.f64 	%fd865, 0d0000000000000000;
	mov.f64 	%fd866, 0dBFF0000000000000;
	fma.rn.f64 	%fd990, %fd990, %fd866, %fd865;

BB0_201:
	ld.global.f32 	%f261, [%rd2+20];
	cvt.f64.f32	%fd991, %f261;
	abs.f64 	%fd867, %fd991;
	setp.neu.f64	%p134, %fd867, 0d7FF0000000000000;
	@%p134 bra 	BB0_203;

	mov.f64 	%fd868, 0d0000000000000000;
	mul.rn.f64 	%fd991, %fd991, %fd868;

BB0_203:
	mul.f64 	%fd869, %fd991, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r354, %fd869;
	st.local.u32 	[%rd125], %r354;
	cvt.rn.f64.s32	%fd870, %r354;
	neg.f64 	%fd871, %fd870;
	fma.rn.f64 	%fd873, %fd871, %fd636, %fd991;
	fma.rn.f64 	%fd875, %fd871, %fd638, %fd873;
	fma.rn.f64 	%fd992, %fd871, %fd640, %fd875;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r289}, %fd991;
	}
	and.b32  	%r290, %r289, 2145386496;
	setp.lt.u32	%p135, %r290, 1105199104;
	@%p135 bra 	BB0_205;

	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd991;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd124;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd992, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r354, [%rd125];

BB0_205:
	add.s32 	%r93, %r354, 1;
	and.b32  	%r291, %r93, 1;
	shl.b32 	%r292, %r291, 3;
	setp.eq.b32	%p136, %r291, 1;
	selp.f64	%fd877, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p136;
	mul.wide.u32 	%rd191, %r292, 8;
	add.s64 	%rd193, %rd191, %rd129;
	ld.const.f64 	%fd878, [%rd193+8];
	mul.rn.f64 	%fd276, %fd992, %fd992;
	fma.rn.f64 	%fd879, %fd877, %fd276, %fd878;
	ld.const.f64 	%fd880, [%rd193+16];
	fma.rn.f64 	%fd881, %fd879, %fd276, %fd880;
	ld.const.f64 	%fd882, [%rd193+24];
	fma.rn.f64 	%fd883, %fd881, %fd276, %fd882;
	ld.const.f64 	%fd884, [%rd193+32];
	fma.rn.f64 	%fd885, %fd883, %fd276, %fd884;
	ld.const.f64 	%fd886, [%rd193+40];
	fma.rn.f64 	%fd887, %fd885, %fd276, %fd886;
	ld.const.f64 	%fd888, [%rd193+48];
	fma.rn.f64 	%fd277, %fd887, %fd276, %fd888;
	fma.rn.f64 	%fd993, %fd277, %fd992, %fd992;
	setp.eq.s32	%p137, %r291, 0;
	@%p137 bra 	BB0_207;

	mov.f64 	%fd889, 0d3FF0000000000000;
	fma.rn.f64 	%fd993, %fd277, %fd276, %fd889;

BB0_207:
	and.b32  	%r293, %r93, 2;
	setp.eq.s32	%p138, %r293, 0;
	@%p138 bra 	BB0_209;

	mov.f64 	%fd890, 0d0000000000000000;
	mov.f64 	%fd891, 0dBFF0000000000000;
	fma.rn.f64 	%fd993, %fd993, %fd891, %fd890;

BB0_209:
	ld.global.f32 	%f264, [%rd2+16];
	add.f32 	%f265, %f264, %f264;
	mul.f32 	%f266, %f264, %f265;
	cvt.f64.f32	%fd892, %f266;
	mul.f64 	%fd893, %fd990, %fd993;
	div.rn.f64 	%fd894, %fd893, %fd892;
	add.f64 	%fd283, %fd256, %fd894;
	mov.f32 	%f355, 0f00000000;
	mov.f32 	%f354, %f355;
	@%p2 bra 	BB0_215;

	mov.u32 	%r325, %ctaid.x;
	mov.u32 	%r324, %nctaid.x;
	mov.u32 	%r323, %ctaid.y;
	mad.lo.s32 	%r322, %r323, %r324, %r325;
	cvt.rn.f32.f64	%f97, %fd283;
	ld.global.f32 	%f269, [%rd2];
	cvt.f64.f32	%fd284, %f269;
	ld.global.f32 	%f98, [%rd2+4];
	add.f32 	%f99, %f96, %f96;
	ld.global.f32 	%f100, [%rd2+8];
	ld.global.f32 	%f101, [%rd2+24];
	mul.lo.s32 	%r301, %r2, %r322;
	mul.wide.s32 	%rd198, %r301, 4;
	add.s64 	%rd202, %rd1, %rd198;
	mov.f32 	%f355, 0f00000000;
	mov.u32 	%r355, 0;
	mov.f32 	%f354, %f355;

BB0_211:
	rem.s32 	%r302, %r355, %r100;
	cvt.rn.f32.s32	%f270, %r302;
	sub.f32 	%f271, %f270, %f98;
	mul.f32 	%f272, %f95, %f271;
	mul.f32 	%f273, %f271, %f272;
	mul.f32 	%f274, %f99, %f271;
	div.s32 	%r303, %r355, %r100;
	cvt.rn.f32.s32	%f275, %r303;
	sub.f32 	%f276, %f275, %f100;
	mul.f32 	%f277, %f274, %f276;
	sub.f32 	%f278, %f273, %f277;
	mul.f32 	%f279, %f97, %f276;
	fma.rn.f32 	%f104, %f276, %f279, %f278;
	cvt.f64.f32	%fd285, %f104;
	neg.f64 	%fd895, %fd285;
	mov.f64 	%fd896, 0d4338000000000000;
	mov.f64 	%fd897, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd898, %fd895, %fd897, %fd896;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r96, %temp}, %fd898;
	}
	mov.f64 	%fd899, 0dC338000000000000;
	add.rn.f64 	%fd900, %fd898, %fd899;
	mov.f64 	%fd901, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd902, %fd900, %fd901, %fd895;
	mov.f64 	%fd903, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd904, %fd900, %fd903, %fd902;
	mov.f64 	%fd905, 0d3E928AF3FCA213EA;
	mov.f64 	%fd906, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd907, %fd906, %fd904, %fd905;
	mov.f64 	%fd908, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd909, %fd907, %fd904, %fd908;
	mov.f64 	%fd910, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd911, %fd909, %fd904, %fd910;
	mov.f64 	%fd912, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd913, %fd911, %fd904, %fd912;
	mov.f64 	%fd914, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd915, %fd913, %fd904, %fd914;
	mov.f64 	%fd916, 0d3F81111111122322;
	fma.rn.f64 	%fd917, %fd915, %fd904, %fd916;
	mov.f64 	%fd918, 0d3FA55555555502A1;
	fma.rn.f64 	%fd919, %fd917, %fd904, %fd918;
	mov.f64 	%fd920, 0d3FC5555555555511;
	fma.rn.f64 	%fd921, %fd919, %fd904, %fd920;
	mov.f64 	%fd922, 0d3FE000000000000B;
	fma.rn.f64 	%fd923, %fd921, %fd904, %fd922;
	mov.f64 	%fd924, 0d3FF0000000000000;
	fma.rn.f64 	%fd925, %fd923, %fd904, %fd924;
	fma.rn.f64 	%fd926, %fd925, %fd904, %fd924;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r97, %temp}, %fd926;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r98}, %fd926;
	}
	shl.b32 	%r304, %r96, 20;
	add.s32 	%r305, %r98, %r304;
	mov.b64 	%fd994, {%r97, %r305};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r306}, %fd895;
	}
	mov.b32 	 %f280, %r306;
	abs.f32 	%f105, %f280;
	setp.lt.f32	%p140, %f105, 0f4086232B;
	@%p140 bra 	BB0_214;

	setp.gt.f32	%p141, %f104, 0f80000000;
	mov.f64 	%fd927, 0d7FF0000000000000;
	sub.f64 	%fd928, %fd927, %fd285;
	selp.f64	%fd994, 0d0000000000000000, %fd928, %p141;
	setp.geu.f32	%p142, %f105, 0f40874800;
	@%p142 bra 	BB0_214;

	shr.u32 	%r307, %r96, 31;
	add.s32 	%r308, %r96, %r307;
	shr.s32 	%r309, %r308, 1;
	shl.b32 	%r310, %r309, 20;
	add.s32 	%r311, %r310, %r98;
	mov.b64 	%fd929, {%r97, %r311};
	sub.s32 	%r312, %r96, %r309;
	shl.b32 	%r313, %r312, 20;
	add.s32 	%r314, %r313, 1072693248;
	mov.u32 	%r315, 0;
	mov.b64 	%fd930, {%r315, %r314};
	mul.f64 	%fd994, %fd929, %fd930;

BB0_214:
	mul.f64 	%fd931, %fd284, %fd994;
	cvt.rn.f32.f64	%f281, %fd931;
	add.f32 	%f282, %f101, %f281;
	add.f32 	%f354, %f354, %f282;
	ld.global.u32 	%r316, [%rd202];
	cvt.rn.f32.s32	%f283, %r316;
	sub.f32 	%f284, %f282, %f283;
	fma.rn.f32 	%f355, %f284, %f284, %f355;
	add.s64 	%rd202, %rd202, 4;
	add.s32 	%r355, %r355, 1;
	setp.lt.s32	%p143, %r355, %r2;
	@%p143 bra 	BB0_211;

BB0_215:
	st.global.f32 	[%rd2], %f354;
	div.rn.f32 	%f285, %f355, %f299;
	mov.f32 	%f286, 0f3F800000;
	sub.f32 	%f287, %f286, %f285;
	st.global.f32 	[%rd2+24], %f287;

BB0_216:
	ret;
}

	// .globl	findMaxima
.visible .entry findMaxima(
	.param .u64 findMaxima_param_0,
	.param .u32 findMaxima_param_1,
	.param .u32 findMaxima_param_2,
	.param .u32 findMaxima_param_3,
	.param .u32 findMaxima_param_4,
	.param .u32 findMaxima_param_5,
	.param .f64 findMaxima_param_6,
	.param .u32 findMaxima_param_7,
	.param .u32 findMaxima_param_8,
	.param .u64 findMaxima_param_9,
	.param .u32 findMaxima_param_10
)
{
	.reg .pred 	%p<24>;
	.reg .b16 	%rs<18>;
	.reg .b32 	%r<106>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<23>;


	ld.param.u64 	%rd5, [findMaxima_param_0];
	ld.param.u32 	%r42, [findMaxima_param_1];
	ld.param.u32 	%r37, [findMaxima_param_2];
	ld.param.u32 	%r38, [findMaxima_param_3];
	ld.param.u32 	%r39, [findMaxima_param_4];
	ld.param.u32 	%r40, [findMaxima_param_5];
	ld.param.f64 	%fd1, [findMaxima_param_6];
	ld.param.u32 	%r41, [findMaxima_param_8];
	ld.param.u64 	%rd6, [findMaxima_param_9];
	mov.u32 	%r43, %ctaid.x;
	mov.u32 	%r44, %ctaid.y;
	mov.u32 	%r45, %nctaid.x;
	mad.lo.s32 	%r1, %r44, %r45, %r43;
	mul.lo.s32 	%r2, %r38, %r37;
	div.s32 	%r46, %r42, %r2;
	setp.ge.s32	%p1, %r1, %r46;
	@%p1 bra 	BB1_25;

	shr.u32 	%r47, %r39, 31;
	add.s32 	%r48, %r39, %r47;
	shr.s32 	%r3, %r48, 1;
	mad.lo.s32 	%r49, %r1, %r2, %r3;
	mad.lo.s32 	%r98, %r3, %r37, %r49;
	add.s32 	%r50, %r1, 1;
	mul.lo.s32 	%r51, %r50, %r2;
	setp.ge.s32	%p2, %r98, %r51;
	@%p2 bra 	BB1_24;

	mov.u32 	%r99, 0;

BB1_3:
	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.s32 	%rd8, %r98, 4;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.u32 	%r7, [%rd9];
	setp.le.s32	%p3, %r7, %r40;
	@%p3 bra 	BB1_23;

	add.s32 	%r53, %r37, -1;
	mul.lo.s32 	%r54, %r3, %r53;
	sub.s32 	%r100, %r98, %r54;
	add.s32 	%r55, %r37, 1;
	mad.lo.s32 	%r9, %r3, %r55, %r98;
	setp.ge.s32	%p4, %r100, %r9;
	@%p4 bra 	BB1_8;

	rem.s32 	%r56, %r98, %r37;
	add.s32 	%r10, %r56, %r3;
	sub.s32 	%r11, %r37, %r39;

BB1_6:
	mul.wide.s32 	%rd10, %r100, 4;
	add.s64 	%rd11, %rd7, %rd10;
	add.s32 	%r57, %r100, 1;
	rem.s32 	%r58, %r57, %r37;
	setp.eq.s32	%p5, %r58, %r10;
	selp.b32	%r59, %r11, 0, %p5;
	add.s32 	%r100, %r59, %r57;
	ld.global.u32 	%r14, [%rd11];
	setp.le.s32	%p6, %r14, %r7;
	setp.lt.s32	%p7, %r100, %r9;
	and.pred  	%p8, %p7, %p6;
	@%p8 bra 	BB1_6;

	setp.gt.s32	%p9, %r14, %r7;
	@%p9 bra 	BB1_23;

BB1_8:
	rem.s32 	%r15, %r98, %r37;
	div.s32 	%r60, %r98, %r38;
	cvt.s32.s16 	%r61, %r60;
	rem.s32 	%r16, %r61, %r38;
	mov.u16 	%rs16, 1;
	setp.lt.s32	%p10, %r99, 1;
	@%p10 bra 	BB1_13;

	cvt.s32.s16 	%r17, %r15;
	cvt.s32.s16 	%r18, %r16;
	mov.u32 	%r101, 0;
	mov.u16 	%rs16, 1;

BB1_10:
	mad.lo.s32 	%r67, %r1, %r41, %r101;
	cvta.to.global.u64 	%rd12, %rd6;
	mul.wide.s32 	%rd13, %r67, 2;
	add.s64 	%rd2, %rd12, %rd13;
	ld.global.s16 	%r68, [%rd2];
	sub.s32 	%r69, %r17, %r68;
	mul.lo.s32 	%r70, %r69, %r69;
	ld.global.s16 	%r71, [%rd2+2];
	sub.s32 	%r72, %r18, %r71;
	mad.lo.s32 	%r73, %r72, %r72, %r70;
	cvt.rn.f64.s32	%fd2, %r73;
	setp.leu.f64	%p11, %fd2, %fd1;
	@%p11 bra 	BB1_12;

	mov.u32 	%r74, 0;
	st.global.u16 	[%rd2+2], %r74;
	st.global.u16 	[%rd2], %r74;
	mov.u16 	%rs16, 0;

BB1_12:
	add.s32 	%r75, %r101, 1;
	cvt.s32.s16 	%r101, %r75;
	setp.lt.s32	%p12, %r101, %r99;
	@%p12 bra 	BB1_10;

BB1_13:
	mad.lo.s32 	%r21, %r1, %r41, %r99;
	and.b16  	%rs10, %rs16, 255;
	setp.eq.s16	%p13, %rs10, 0;
	@%p13 bra 	BB1_15;
	bra.uni 	BB1_14;

BB1_15:
	mul.lo.s32 	%r103, %r1, %r41;
	cvt.s32.s16 	%r102, %r103;
	setp.ge.s32	%p14, %r102, %r21;
	@%p14 bra 	BB1_23;

BB1_16:
	cvta.to.global.u64 	%rd17, %rd6;
	mul.wide.s32 	%rd18, %r102, 2;
	add.s64 	%rd3, %rd17, %rd18;
	ld.global.u16 	%rs11, [%rd3];
	setp.ne.s16	%p15, %rs11, 0;
	@%p15 bra 	BB1_22;

	cvt.s32.s16 	%r84, %r103;
	add.s32 	%r105, %r84, 1;
	cvt.s32.s16 	%r104, %r105;
	setp.ge.s32	%p16, %r104, %r21;
	@%p16 bra 	BB1_22;

	mov.u16 	%rs17, 1;

BB1_19:
	mov.u32 	%r29, %r104;
	mul.wide.s32 	%rd20, %r29, 2;
	add.s64 	%rd4, %rd17, %rd20;
	ld.global.u16 	%rs5, [%rd4];
	setp.lt.s16	%p17, %rs5, 1;
	@%p17 bra 	BB1_21;

	st.global.u16 	[%rd3], %rs5;
	ld.global.u16 	%rs14, [%rd4+2];
	st.global.u16 	[%rd3+2], %rs14;
	mov.u16 	%rs17, 0;

BB1_21:
	cvt.s32.s16 	%r85, %r105;
	add.s32 	%r105, %r85, 1;
	cvt.s32.s16 	%r104, %r105;
	setp.lt.s32	%p18, %r104, %r21;
	and.b16  	%rs15, %rs17, 255;
	setp.ne.s16	%p19, %rs15, 0;
	and.pred  	%p20, %p18, %p19;
	@%p20 bra 	BB1_19;

BB1_22:
	add.s32 	%r103, %r102, 1;
	cvt.s32.s16 	%r102, %r103;
	setp.lt.s32	%p21, %r102, %r21;
	@%p21 bra 	BB1_16;
	bra.uni 	BB1_23;

BB1_14:
	cvta.to.global.u64 	%rd14, %rd6;
	mul.wide.s32 	%rd15, %r21, 2;
	add.s64 	%rd16, %rd14, %rd15;
	st.global.u16 	[%rd16], %r15;
	st.global.u16 	[%rd16+2], %r16;
	add.s32 	%r99, %r99, 1;

BB1_23:
	add.s32 	%r86, %r98, 1;
	rem.s32 	%r87, %r86, %r37;
	sub.s32 	%r88, %r37, %r3;
	setp.eq.s32	%p22, %r87, %r88;
	selp.b32	%r89, %r39, 0, %p22;
	add.s32 	%r98, %r89, %r86;
	setp.lt.s32	%p23, %r98, %r51;
	@%p23 bra 	BB1_3;

BB1_24:
	cvta.to.global.u64 	%rd21, %rd5;
	ld.global.u32 	%r97, [%rd21+1352];
	cvta.to.global.u64 	%rd22, %rd6;
	st.global.u16 	[%rd22], %r97;

BB1_25:
	ret;
}

	// .globl	gaussFitter2
.visible .entry gaussFitter2(
	.param .u64 gaussFitter2_param_0,
	.param .u32 gaussFitter2_param_1,
	.param .u64 gaussFitter2_param_2,
	.param .u32 gaussFitter2_param_3,
	.param .u32 gaussFitter2_param_4,
	.param .u64 gaussFitter2_param_5,
	.param .u32 gaussFitter2_param_6,
	.param .u64 gaussFitter2_param_7,
	.param .u32 gaussFitter2_param_8,
	.param .f64 gaussFitter2_param_9,
	.param .u32 gaussFitter2_param_10
)
{
	.local .align 4 .b8 	__local_depot2[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<61>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<119>;
	.reg .b32 	%r<236>;
	.reg .f64 	%fd<475>;
	.reg .b64 	%rd<132>;


	mov.u64 	%rd131, __local_depot2;
	cvta.local.u64 	%SP, %rd131;
	ld.param.u64 	%rd2, [gaussFitter2_param_0];
	ld.param.u32 	%r47, [gaussFitter2_param_1];
	ld.param.u64 	%rd3, [gaussFitter2_param_2];
	ld.param.u32 	%r46, [gaussFitter2_param_4];
	ld.param.u64 	%rd4, [gaussFitter2_param_7];
	mov.u32 	%r48, %ctaid.y;
	mov.u32 	%r49, %nctaid.x;
	mov.u32 	%r50, %ctaid.x;
	mad.lo.s32 	%r51, %r48, %r49, %r50;
	mul.lo.s32 	%r52, %r46, %r46;
	div.s32 	%r53, %r47, %r52;
	setp.ge.s32	%p1, %r51, %r53;
	@%p1 bra 	BB2_92;

	setp.eq.s32	%p2, %r52, 0;
	mov.f32 	%f113, 0f00000000;
	mov.f32 	%f110, %f113;
	mov.f32 	%f107, %f113;
	mov.u32 	%r223, 0;
	mov.f32 	%f112, %f113;
	mov.f32 	%f109, %f113;
	mov.f32 	%f106, %f113;
	@%p2 bra 	BB2_3;

BB2_2:
	mad.lo.s32 	%r61, %r51, %r52, %r223;
	cvta.to.global.u64 	%rd5, %rd2;
	mul.wide.s32 	%rd6, %r61, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ldu.global.u32 	%r62, [%rd7];
	cvt.rn.f32.s32	%f35, %r62;
	add.f32 	%f107, %f107, %f35;
	rem.s32 	%r63, %r223, %r46;
	cvt.rn.f32.s32	%f36, %r63;
	fma.rn.f32 	%f113, %f35, %f36, %f113;
	div.s32 	%r64, %r223, %r46;
	cvt.rn.f32.s32	%f37, %r64;
	fma.rn.f32 	%f110, %f37, %f35, %f110;
	add.s32 	%r65, %r223, 1;
	and.b32  	%r223, %r65, 65535;
	setp.lt.u32	%p3, %r223, %r52;
	mov.f32 	%f106, %f107;
	mov.f32 	%f109, %f110;
	mov.f32 	%f112, %f113;
	@%p3 bra 	BB2_2;

BB2_3:
	mul.lo.s32 	%r72, %r51, 7;
	cvta.to.global.u64 	%rd8, %rd3;
	mul.wide.s32 	%rd9, %r72, 4;
	add.s64 	%rd10, %rd8, %rd9;
	div.rn.f32 	%f40, %f112, %f106;
	st.global.f32 	[%rd10+4], %f40;
	div.rn.f32 	%f41, %f109, %f106;
	st.global.f32 	[%rd10+8], %f41;
	cvt.rn.f32.s32	%f42, %r52;
	div.rn.f32 	%f10, %f106, %f42;
	mov.f32 	%f116, 0f00000000;
	mov.u32 	%r224, 0;
	mov.f32 	%f115, %f116;
	@%p2 bra 	BB2_5;

BB2_4:
	mad.lo.s32 	%r78, %r51, %r52, %r224;
	cvta.to.global.u64 	%rd11, %rd2;
	mul.wide.s32 	%rd12, %r78, 4;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.u32 	%r79, [%rd13];
	cvt.rn.f32.s32	%f43, %r79;
	sub.f32 	%f44, %f43, %f10;
	fma.rn.f32 	%f116, %f44, %f44, %f116;
	add.s32 	%r80, %r224, 1;
	and.b32  	%r224, %r80, 65535;
	setp.lt.u32	%p5, %r224, %r52;
	mov.f32 	%f115, %f116;
	@%p5 bra 	BB2_4;

BB2_5:
	cvta.to.global.u64 	%rd17, %rd4;
	add.s64 	%rd18, %rd17, %rd9;
	ld.global.f32 	%f45, [%rd18];
	ld.global.f32 	%f46, [%rd10];
	mul.f32 	%f47, %f46, %f45;
	ld.global.f32 	%f48, [%rd18+24];
	st.global.f32 	[%rd18], %f47;
	ld.global.f32 	%f49, [%rd10];
	mul.f32 	%f50, %f49, %f48;
	st.global.f32 	[%rd18+24], %f50;
	ld.global.f32 	%f51, [%rd10+20];
	cvt.f64.f32	%fd444, %f51;
	abs.f64 	%fd143, %fd444;
	setp.neu.f64	%p6, %fd143, 0d7FF0000000000000;
	@%p6 bra 	BB2_7;

	mov.f64 	%fd144, 0d0000000000000000;
	mul.rn.f64 	%fd444, %fd444, %fd144;

BB2_7:
	mul.f64 	%fd145, %fd444, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r225, %fd145;
	add.u64 	%rd19, %SP, 0;
	cvta.to.local.u64 	%rd20, %rd19;
	st.local.u32 	[%rd20], %r225;
	cvt.rn.f64.s32	%fd146, %r225;
	neg.f64 	%fd147, %fd146;
	mov.f64 	%fd148, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd149, %fd147, %fd148, %fd444;
	mov.f64 	%fd150, 0d3C91A62633145C00;
	fma.rn.f64 	%fd151, %fd147, %fd150, %fd149;
	mov.f64 	%fd152, 0d397B839A252049C0;
	fma.rn.f64 	%fd445, %fd147, %fd152, %fd151;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r86}, %fd444;
	}
	and.b32  	%r87, %r86, 2145386496;
	setp.lt.u32	%p7, %r87, 1105199104;
	@%p7 bra 	BB2_9;

	// Callseq Start 20
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd444;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd445, [retval0+0];
	
	//{
	}// Callseq End 20
	ld.local.u32 	%r225, [%rd20];

BB2_9:
	add.s32 	%r8, %r225, 1;
	and.b32  	%r88, %r8, 1;
	shl.b32 	%r89, %r88, 3;
	setp.eq.b32	%p8, %r88, 1;
	selp.f64	%fd153, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p8;
	mul.wide.u32 	%rd23, %r89, 8;
	mov.u64 	%rd24, __cudart_sin_cos_coeffs;
	add.s64 	%rd25, %rd23, %rd24;
	ld.const.f64 	%fd154, [%rd25+8];
	mul.rn.f64 	%fd7, %fd445, %fd445;
	fma.rn.f64 	%fd155, %fd153, %fd7, %fd154;
	ld.const.f64 	%fd156, [%rd25+16];
	fma.rn.f64 	%fd157, %fd155, %fd7, %fd156;
	ld.const.f64 	%fd158, [%rd25+24];
	fma.rn.f64 	%fd159, %fd157, %fd7, %fd158;
	ld.const.f64 	%fd160, [%rd25+32];
	fma.rn.f64 	%fd161, %fd159, %fd7, %fd160;
	ld.const.f64 	%fd162, [%rd25+40];
	fma.rn.f64 	%fd163, %fd161, %fd7, %fd162;
	ld.const.f64 	%fd164, [%rd25+48];
	fma.rn.f64 	%fd8, %fd163, %fd7, %fd164;
	fma.rn.f64 	%fd446, %fd8, %fd445, %fd445;
	setp.eq.s32	%p9, %r88, 0;
	@%p9 bra 	BB2_11;

	mov.f64 	%fd165, 0d3FF0000000000000;
	fma.rn.f64 	%fd446, %fd8, %fd7, %fd165;

BB2_11:
	and.b32  	%r90, %r8, 2;
	setp.eq.s32	%p10, %r90, 0;
	@%p10 bra 	BB2_13;

	mov.f64 	%fd166, 0d0000000000000000;
	mov.f64 	%fd167, 0dBFF0000000000000;
	fma.rn.f64 	%fd446, %fd446, %fd167, %fd166;

BB2_13:
	ld.global.f32 	%f52, [%rd10+20];
	cvt.f64.f32	%fd447, %f52;
	abs.f64 	%fd168, %fd447;
	setp.neu.f64	%p11, %fd168, 0d7FF0000000000000;
	@%p11 bra 	BB2_15;

	mov.f64 	%fd169, 0d0000000000000000;
	mul.rn.f64 	%fd447, %fd447, %fd169;

BB2_15:
	mul.f64 	%fd170, %fd447, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r226, %fd170;
	st.local.u32 	[%rd20], %r226;
	cvt.rn.f64.s32	%fd171, %r226;
	neg.f64 	%fd172, %fd171;
	fma.rn.f64 	%fd174, %fd172, %fd148, %fd447;
	fma.rn.f64 	%fd176, %fd172, %fd150, %fd174;
	fma.rn.f64 	%fd448, %fd172, %fd152, %fd176;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r96}, %fd447;
	}
	and.b32  	%r97, %r96, 2145386496;
	setp.lt.u32	%p12, %r97, 1105199104;
	@%p12 bra 	BB2_17;

	// Callseq Start 21
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd447;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd448, [retval0+0];
	
	//{
	}// Callseq End 21
	ld.local.u32 	%r226, [%rd20];

BB2_17:
	add.s32 	%r12, %r226, 1;
	and.b32  	%r98, %r12, 1;
	shl.b32 	%r99, %r98, 3;
	setp.eq.b32	%p13, %r98, 1;
	selp.f64	%fd178, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p13;
	mul.wide.u32 	%rd33, %r99, 8;
	add.s64 	%rd35, %rd33, %rd24;
	ld.const.f64 	%fd179, [%rd35+8];
	mul.rn.f64 	%fd20, %fd448, %fd448;
	fma.rn.f64 	%fd180, %fd178, %fd20, %fd179;
	ld.const.f64 	%fd181, [%rd35+16];
	fma.rn.f64 	%fd182, %fd180, %fd20, %fd181;
	ld.const.f64 	%fd183, [%rd35+24];
	fma.rn.f64 	%fd184, %fd182, %fd20, %fd183;
	ld.const.f64 	%fd185, [%rd35+32];
	fma.rn.f64 	%fd186, %fd184, %fd20, %fd185;
	ld.const.f64 	%fd187, [%rd35+40];
	fma.rn.f64 	%fd188, %fd186, %fd20, %fd187;
	ld.const.f64 	%fd189, [%rd35+48];
	fma.rn.f64 	%fd21, %fd188, %fd20, %fd189;
	fma.rn.f64 	%fd449, %fd21, %fd448, %fd448;
	setp.eq.s32	%p14, %r98, 0;
	@%p14 bra 	BB2_19;

	mov.f64 	%fd190, 0d3FF0000000000000;
	fma.rn.f64 	%fd449, %fd21, %fd20, %fd190;

BB2_19:
	and.b32  	%r100, %r12, 2;
	setp.eq.s32	%p15, %r100, 0;
	@%p15 bra 	BB2_21;

	mov.f64 	%fd191, 0d0000000000000000;
	mov.f64 	%fd192, 0dBFF0000000000000;
	fma.rn.f64 	%fd449, %fd449, %fd192, %fd191;

BB2_21:
	ld.global.f32 	%f53, [%rd10+12];
	add.f32 	%f54, %f53, %f53;
	mul.f32 	%f55, %f53, %f54;
	cvt.f64.f32	%fd193, %f55;
	mul.f64 	%fd194, %fd446, %fd449;
	div.rn.f64 	%fd27, %fd194, %fd193;
	ld.global.f32 	%f56, [%rd10+20];
	cvt.f64.f32	%fd450, %f56;
	abs.f64 	%fd195, %fd450;
	setp.neu.f64	%p16, %fd195, 0d7FF0000000000000;
	@%p16 bra 	BB2_23;

	mov.f64 	%fd196, 0d0000000000000000;
	mul.rn.f64 	%fd450, %fd450, %fd196;

BB2_23:
	mul.f64 	%fd197, %fd450, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r227, %fd197;
	st.local.u32 	[%rd20], %r227;
	cvt.rn.f64.s32	%fd198, %r227;
	neg.f64 	%fd199, %fd198;
	fma.rn.f64 	%fd201, %fd199, %fd148, %fd450;
	fma.rn.f64 	%fd203, %fd199, %fd150, %fd201;
	fma.rn.f64 	%fd451, %fd199, %fd152, %fd203;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r106}, %fd450;
	}
	and.b32  	%r107, %r106, 2145386496;
	setp.lt.u32	%p17, %r107, 1105199104;
	@%p17 bra 	BB2_25;

	// Callseq Start 22
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd450;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd451, [retval0+0];
	
	//{
	}// Callseq End 22
	ld.local.u32 	%r227, [%rd20];

BB2_25:
	and.b32  	%r108, %r227, 1;
	shl.b32 	%r109, %r108, 3;
	setp.eq.b32	%p18, %r108, 1;
	selp.f64	%fd205, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p18;
	mul.wide.u32 	%rd43, %r109, 8;
	add.s64 	%rd45, %rd43, %rd24;
	ld.const.f64 	%fd206, [%rd45+8];
	mul.rn.f64 	%fd34, %fd451, %fd451;
	fma.rn.f64 	%fd207, %fd205, %fd34, %fd206;
	ld.const.f64 	%fd208, [%rd45+16];
	fma.rn.f64 	%fd209, %fd207, %fd34, %fd208;
	ld.const.f64 	%fd210, [%rd45+24];
	fma.rn.f64 	%fd211, %fd209, %fd34, %fd210;
	ld.const.f64 	%fd212, [%rd45+32];
	fma.rn.f64 	%fd213, %fd211, %fd34, %fd212;
	ld.const.f64 	%fd214, [%rd45+40];
	fma.rn.f64 	%fd215, %fd213, %fd34, %fd214;
	ld.const.f64 	%fd216, [%rd45+48];
	fma.rn.f64 	%fd35, %fd215, %fd34, %fd216;
	fma.rn.f64 	%fd452, %fd35, %fd451, %fd451;
	setp.eq.s32	%p19, %r108, 0;
	@%p19 bra 	BB2_27;

	mov.f64 	%fd217, 0d3FF0000000000000;
	fma.rn.f64 	%fd452, %fd35, %fd34, %fd217;

BB2_27:
	and.b32  	%r110, %r227, 2;
	setp.eq.s32	%p20, %r110, 0;
	@%p20 bra 	BB2_29;

	mov.f64 	%fd218, 0d0000000000000000;
	mov.f64 	%fd219, 0dBFF0000000000000;
	fma.rn.f64 	%fd452, %fd452, %fd219, %fd218;

BB2_29:
	ld.global.f32 	%f57, [%rd10+20];
	cvt.f64.f32	%fd453, %f57;
	abs.f64 	%fd220, %fd453;
	setp.neu.f64	%p21, %fd220, 0d7FF0000000000000;
	@%p21 bra 	BB2_31;

	mov.f64 	%fd221, 0d0000000000000000;
	mul.rn.f64 	%fd453, %fd453, %fd221;

BB2_31:
	mul.f64 	%fd222, %fd453, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r228, %fd222;
	st.local.u32 	[%rd20], %r228;
	cvt.rn.f64.s32	%fd223, %r228;
	neg.f64 	%fd224, %fd223;
	fma.rn.f64 	%fd226, %fd224, %fd148, %fd453;
	fma.rn.f64 	%fd228, %fd224, %fd150, %fd226;
	fma.rn.f64 	%fd454, %fd224, %fd152, %fd228;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r116}, %fd453;
	}
	and.b32  	%r117, %r116, 2145386496;
	setp.lt.u32	%p22, %r117, 1105199104;
	@%p22 bra 	BB2_33;

	// Callseq Start 23
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd453;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd454, [retval0+0];
	
	//{
	}// Callseq End 23
	ld.local.u32 	%r228, [%rd20];

BB2_33:
	and.b32  	%r118, %r228, 1;
	shl.b32 	%r119, %r118, 3;
	setp.eq.b32	%p23, %r118, 1;
	selp.f64	%fd230, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p23;
	mul.wide.u32 	%rd53, %r119, 8;
	add.s64 	%rd55, %rd53, %rd24;
	ld.const.f64 	%fd231, [%rd55+8];
	mul.rn.f64 	%fd47, %fd454, %fd454;
	fma.rn.f64 	%fd232, %fd230, %fd47, %fd231;
	ld.const.f64 	%fd233, [%rd55+16];
	fma.rn.f64 	%fd234, %fd232, %fd47, %fd233;
	ld.const.f64 	%fd235, [%rd55+24];
	fma.rn.f64 	%fd236, %fd234, %fd47, %fd235;
	ld.const.f64 	%fd237, [%rd55+32];
	fma.rn.f64 	%fd238, %fd236, %fd47, %fd237;
	ld.const.f64 	%fd239, [%rd55+40];
	fma.rn.f64 	%fd240, %fd238, %fd47, %fd239;
	ld.const.f64 	%fd241, [%rd55+48];
	fma.rn.f64 	%fd48, %fd240, %fd47, %fd241;
	fma.rn.f64 	%fd455, %fd48, %fd454, %fd454;
	setp.eq.s32	%p24, %r118, 0;
	@%p24 bra 	BB2_35;

	mov.f64 	%fd242, 0d3FF0000000000000;
	fma.rn.f64 	%fd455, %fd48, %fd47, %fd242;

BB2_35:
	and.b32  	%r120, %r228, 2;
	setp.eq.s32	%p25, %r120, 0;
	@%p25 bra 	BB2_37;

	mov.f64 	%fd243, 0d0000000000000000;
	mov.f64 	%fd244, 0dBFF0000000000000;
	fma.rn.f64 	%fd455, %fd455, %fd244, %fd243;

BB2_37:
	ld.global.f32 	%f58, [%rd10+16];
	add.f32 	%f59, %f58, %f58;
	mul.f32 	%f60, %f58, %f59;
	cvt.f64.f32	%fd245, %f60;
	mul.f64 	%fd246, %fd452, %fd455;
	div.rn.f64 	%fd247, %fd246, %fd245;
	add.f64 	%fd54, %fd27, %fd247;
	ld.global.f32 	%f61, [%rd10+20];
	add.f32 	%f62, %f61, %f61;
	cvt.f64.f32	%fd456, %f62;
	abs.f64 	%fd248, %fd456;
	setp.neu.f64	%p26, %fd248, 0d7FF0000000000000;
	@%p26 bra 	BB2_39;

	mov.f64 	%fd249, 0d0000000000000000;
	mul.rn.f64 	%fd456, %fd456, %fd249;

BB2_39:
	mul.f64 	%fd250, %fd456, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r229, %fd250;
	st.local.u32 	[%rd20], %r229;
	cvt.rn.f64.s32	%fd251, %r229;
	neg.f64 	%fd252, %fd251;
	fma.rn.f64 	%fd254, %fd252, %fd148, %fd456;
	fma.rn.f64 	%fd256, %fd252, %fd150, %fd254;
	fma.rn.f64 	%fd457, %fd252, %fd152, %fd256;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r126}, %fd456;
	}
	and.b32  	%r127, %r126, 2145386496;
	setp.lt.u32	%p27, %r127, 1105199104;
	@%p27 bra 	BB2_41;

	// Callseq Start 24
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd456;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd457, [retval0+0];
	
	//{
	}// Callseq End 24
	ld.local.u32 	%r229, [%rd20];

BB2_41:
	and.b32  	%r128, %r229, 1;
	shl.b32 	%r129, %r128, 3;
	setp.eq.b32	%p28, %r128, 1;
	selp.f64	%fd258, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p28;
	mul.wide.u32 	%rd63, %r129, 8;
	add.s64 	%rd65, %rd63, %rd24;
	ld.const.f64 	%fd259, [%rd65+8];
	mul.rn.f64 	%fd61, %fd457, %fd457;
	fma.rn.f64 	%fd260, %fd258, %fd61, %fd259;
	ld.const.f64 	%fd261, [%rd65+16];
	fma.rn.f64 	%fd262, %fd260, %fd61, %fd261;
	ld.const.f64 	%fd263, [%rd65+24];
	fma.rn.f64 	%fd264, %fd262, %fd61, %fd263;
	ld.const.f64 	%fd265, [%rd65+32];
	fma.rn.f64 	%fd266, %fd264, %fd61, %fd265;
	ld.const.f64 	%fd267, [%rd65+40];
	fma.rn.f64 	%fd268, %fd266, %fd61, %fd267;
	ld.const.f64 	%fd269, [%rd65+48];
	fma.rn.f64 	%fd62, %fd268, %fd61, %fd269;
	fma.rn.f64 	%fd458, %fd62, %fd457, %fd457;
	setp.eq.s32	%p29, %r128, 0;
	@%p29 bra 	BB2_43;

	mov.f64 	%fd270, 0d3FF0000000000000;
	fma.rn.f64 	%fd458, %fd62, %fd61, %fd270;

BB2_43:
	and.b32  	%r130, %r229, 2;
	setp.eq.s32	%p30, %r130, 0;
	@%p30 bra 	BB2_45;

	mov.f64 	%fd271, 0d0000000000000000;
	mov.f64 	%fd272, 0dBFF0000000000000;
	fma.rn.f64 	%fd458, %fd458, %fd272, %fd271;

BB2_45:
	cvt.rn.f32.f64	%f14, %fd54;
	cvt.rn.f32.f64	%f63, %fd458;
	cvt.f64.f32	%fd273, %f63;
	ld.global.f32 	%f64, [%rd10+12];
	mul.f32 	%f65, %f64, 0fC0800000;
	mul.f32 	%f66, %f64, %f65;
	cvt.f64.f32	%fd274, %f66;
	div.rn.f64 	%fd68, %fd273, %fd274;
	ld.global.f32 	%f67, [%rd10+20];
	add.f32 	%f68, %f67, %f67;
	cvt.f64.f32	%fd459, %f68;
	abs.f64 	%fd275, %fd459;
	setp.neu.f64	%p31, %fd275, 0d7FF0000000000000;
	@%p31 bra 	BB2_47;

	mov.f64 	%fd276, 0d0000000000000000;
	mul.rn.f64 	%fd459, %fd459, %fd276;

BB2_47:
	mul.f64 	%fd277, %fd459, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r230, %fd277;
	st.local.u32 	[%rd20], %r230;
	cvt.rn.f64.s32	%fd278, %r230;
	neg.f64 	%fd279, %fd278;
	fma.rn.f64 	%fd281, %fd279, %fd148, %fd459;
	fma.rn.f64 	%fd283, %fd279, %fd150, %fd281;
	fma.rn.f64 	%fd460, %fd279, %fd152, %fd283;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r136}, %fd459;
	}
	and.b32  	%r137, %r136, 2145386496;
	setp.lt.u32	%p32, %r137, 1105199104;
	@%p32 bra 	BB2_49;

	// Callseq Start 25
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd459;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd460, [retval0+0];
	
	//{
	}// Callseq End 25
	ld.local.u32 	%r230, [%rd20];

BB2_49:
	and.b32  	%r138, %r230, 1;
	shl.b32 	%r139, %r138, 3;
	setp.eq.b32	%p33, %r138, 1;
	selp.f64	%fd285, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p33;
	mul.wide.u32 	%rd73, %r139, 8;
	add.s64 	%rd75, %rd73, %rd24;
	ld.const.f64 	%fd286, [%rd75+8];
	mul.rn.f64 	%fd75, %fd460, %fd460;
	fma.rn.f64 	%fd287, %fd285, %fd75, %fd286;
	ld.const.f64 	%fd288, [%rd75+16];
	fma.rn.f64 	%fd289, %fd287, %fd75, %fd288;
	ld.const.f64 	%fd290, [%rd75+24];
	fma.rn.f64 	%fd291, %fd289, %fd75, %fd290;
	ld.const.f64 	%fd292, [%rd75+32];
	fma.rn.f64 	%fd293, %fd291, %fd75, %fd292;
	ld.const.f64 	%fd294, [%rd75+40];
	fma.rn.f64 	%fd295, %fd293, %fd75, %fd294;
	ld.const.f64 	%fd296, [%rd75+48];
	fma.rn.f64 	%fd76, %fd295, %fd75, %fd296;
	fma.rn.f64 	%fd461, %fd76, %fd460, %fd460;
	setp.eq.s32	%p34, %r138, 0;
	@%p34 bra 	BB2_51;

	mov.f64 	%fd297, 0d3FF0000000000000;
	fma.rn.f64 	%fd461, %fd76, %fd75, %fd297;

BB2_51:
	and.b32  	%r140, %r230, 2;
	setp.eq.s32	%p35, %r140, 0;
	@%p35 bra 	BB2_53;

	mov.f64 	%fd298, 0d0000000000000000;
	mov.f64 	%fd299, 0dBFF0000000000000;
	fma.rn.f64 	%fd461, %fd461, %fd299, %fd298;

BB2_53:
	ld.global.f32 	%f69, [%rd10+16];
	mul.f32 	%f70, %f69, 0f40800000;
	mul.f32 	%f71, %f69, %f70;
	cvt.f64.f32	%fd300, %f71;
	div.rn.f64 	%fd301, %fd461, %fd300;
	add.f64 	%fd82, %fd68, %fd301;
	ld.global.f32 	%f72, [%rd10+20];
	cvt.f64.f32	%fd462, %f72;
	abs.f64 	%fd302, %fd462;
	setp.neu.f64	%p36, %fd302, 0d7FF0000000000000;
	@%p36 bra 	BB2_55;

	mov.f64 	%fd303, 0d0000000000000000;
	mul.rn.f64 	%fd462, %fd462, %fd303;

BB2_55:
	mul.f64 	%fd304, %fd462, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r231, %fd304;
	st.local.u32 	[%rd20], %r231;
	cvt.rn.f64.s32	%fd305, %r231;
	neg.f64 	%fd306, %fd305;
	fma.rn.f64 	%fd308, %fd306, %fd148, %fd462;
	fma.rn.f64 	%fd310, %fd306, %fd150, %fd308;
	fma.rn.f64 	%fd463, %fd306, %fd152, %fd310;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r146}, %fd462;
	}
	and.b32  	%r147, %r146, 2145386496;
	setp.lt.u32	%p37, %r147, 1105199104;
	@%p37 bra 	BB2_57;

	// Callseq Start 26
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd462;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd463, [retval0+0];
	
	//{
	}// Callseq End 26
	ld.local.u32 	%r231, [%rd20];

BB2_57:
	and.b32  	%r148, %r231, 1;
	shl.b32 	%r149, %r148, 3;
	setp.eq.b32	%p38, %r148, 1;
	selp.f64	%fd312, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p38;
	mul.wide.u32 	%rd83, %r149, 8;
	add.s64 	%rd85, %rd83, %rd24;
	ld.const.f64 	%fd313, [%rd85+8];
	mul.rn.f64 	%fd89, %fd463, %fd463;
	fma.rn.f64 	%fd314, %fd312, %fd89, %fd313;
	ld.const.f64 	%fd315, [%rd85+16];
	fma.rn.f64 	%fd316, %fd314, %fd89, %fd315;
	ld.const.f64 	%fd317, [%rd85+24];
	fma.rn.f64 	%fd318, %fd316, %fd89, %fd317;
	ld.const.f64 	%fd319, [%rd85+32];
	fma.rn.f64 	%fd320, %fd318, %fd89, %fd319;
	ld.const.f64 	%fd321, [%rd85+40];
	fma.rn.f64 	%fd322, %fd320, %fd89, %fd321;
	ld.const.f64 	%fd323, [%rd85+48];
	fma.rn.f64 	%fd90, %fd322, %fd89, %fd323;
	fma.rn.f64 	%fd464, %fd90, %fd463, %fd463;
	setp.eq.s32	%p39, %r148, 0;
	@%p39 bra 	BB2_59;

	mov.f64 	%fd324, 0d3FF0000000000000;
	fma.rn.f64 	%fd464, %fd90, %fd89, %fd324;

BB2_59:
	and.b32  	%r150, %r231, 2;
	setp.eq.s32	%p40, %r150, 0;
	@%p40 bra 	BB2_61;

	mov.f64 	%fd325, 0d0000000000000000;
	mov.f64 	%fd326, 0dBFF0000000000000;
	fma.rn.f64 	%fd464, %fd464, %fd326, %fd325;

BB2_61:
	ld.global.f32 	%f73, [%rd10+20];
	cvt.f64.f32	%fd465, %f73;
	abs.f64 	%fd327, %fd465;
	setp.neu.f64	%p41, %fd327, 0d7FF0000000000000;
	@%p41 bra 	BB2_63;

	mov.f64 	%fd328, 0d0000000000000000;
	mul.rn.f64 	%fd465, %fd465, %fd328;

BB2_63:
	mul.f64 	%fd329, %fd465, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r232, %fd329;
	st.local.u32 	[%rd20], %r232;
	cvt.rn.f64.s32	%fd330, %r232;
	neg.f64 	%fd331, %fd330;
	fma.rn.f64 	%fd333, %fd331, %fd148, %fd465;
	fma.rn.f64 	%fd335, %fd331, %fd150, %fd333;
	fma.rn.f64 	%fd466, %fd331, %fd152, %fd335;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r156}, %fd465;
	}
	and.b32  	%r157, %r156, 2145386496;
	setp.lt.u32	%p42, %r157, 1105199104;
	@%p42 bra 	BB2_65;

	// Callseq Start 27
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd465;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd466, [retval0+0];
	
	//{
	}// Callseq End 27
	ld.local.u32 	%r232, [%rd20];

BB2_65:
	and.b32  	%r158, %r232, 1;
	shl.b32 	%r159, %r158, 3;
	setp.eq.b32	%p43, %r158, 1;
	selp.f64	%fd337, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p43;
	mul.wide.u32 	%rd93, %r159, 8;
	add.s64 	%rd95, %rd93, %rd24;
	ld.const.f64 	%fd338, [%rd95+8];
	mul.rn.f64 	%fd102, %fd466, %fd466;
	fma.rn.f64 	%fd339, %fd337, %fd102, %fd338;
	ld.const.f64 	%fd340, [%rd95+16];
	fma.rn.f64 	%fd341, %fd339, %fd102, %fd340;
	ld.const.f64 	%fd342, [%rd95+24];
	fma.rn.f64 	%fd343, %fd341, %fd102, %fd342;
	ld.const.f64 	%fd344, [%rd95+32];
	fma.rn.f64 	%fd345, %fd343, %fd102, %fd344;
	ld.const.f64 	%fd346, [%rd95+40];
	fma.rn.f64 	%fd347, %fd345, %fd102, %fd346;
	ld.const.f64 	%fd348, [%rd95+48];
	fma.rn.f64 	%fd103, %fd347, %fd102, %fd348;
	fma.rn.f64 	%fd467, %fd103, %fd466, %fd466;
	setp.eq.s32	%p44, %r158, 0;
	@%p44 bra 	BB2_67;

	mov.f64 	%fd349, 0d3FF0000000000000;
	fma.rn.f64 	%fd467, %fd103, %fd102, %fd349;

BB2_67:
	and.b32  	%r160, %r232, 2;
	setp.eq.s32	%p45, %r160, 0;
	@%p45 bra 	BB2_69;

	mov.f64 	%fd350, 0d0000000000000000;
	mov.f64 	%fd351, 0dBFF0000000000000;
	fma.rn.f64 	%fd467, %fd467, %fd351, %fd350;

BB2_69:
	cvt.rn.f32.f64	%f15, %fd82;
	ld.global.f32 	%f74, [%rd10+12];
	add.f32 	%f75, %f74, %f74;
	mul.f32 	%f76, %f74, %f75;
	cvt.f64.f32	%fd352, %f76;
	mul.f64 	%fd353, %fd464, %fd467;
	div.rn.f64 	%fd109, %fd353, %fd352;
	ld.global.f32 	%f77, [%rd10+20];
	cvt.f64.f32	%fd468, %f77;
	abs.f64 	%fd354, %fd468;
	setp.neu.f64	%p46, %fd354, 0d7FF0000000000000;
	@%p46 bra 	BB2_71;

	mov.f64 	%fd355, 0d0000000000000000;
	mul.rn.f64 	%fd468, %fd468, %fd355;

BB2_71:
	mul.f64 	%fd356, %fd468, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r233, %fd356;
	st.local.u32 	[%rd20], %r233;
	cvt.rn.f64.s32	%fd357, %r233;
	neg.f64 	%fd358, %fd357;
	fma.rn.f64 	%fd360, %fd358, %fd148, %fd468;
	fma.rn.f64 	%fd362, %fd358, %fd150, %fd360;
	fma.rn.f64 	%fd469, %fd358, %fd152, %fd362;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r166}, %fd468;
	}
	and.b32  	%r167, %r166, 2145386496;
	setp.lt.u32	%p47, %r167, 1105199104;
	@%p47 bra 	BB2_73;

	// Callseq Start 28
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd468;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd469, [retval0+0];
	
	//{
	}// Callseq End 28
	ld.local.u32 	%r233, [%rd20];

BB2_73:
	add.s32 	%r34, %r233, 1;
	and.b32  	%r168, %r34, 1;
	shl.b32 	%r169, %r168, 3;
	setp.eq.b32	%p48, %r168, 1;
	selp.f64	%fd364, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p48;
	mul.wide.u32 	%rd103, %r169, 8;
	add.s64 	%rd105, %rd103, %rd24;
	ld.const.f64 	%fd365, [%rd105+8];
	mul.rn.f64 	%fd116, %fd469, %fd469;
	fma.rn.f64 	%fd366, %fd364, %fd116, %fd365;
	ld.const.f64 	%fd367, [%rd105+16];
	fma.rn.f64 	%fd368, %fd366, %fd116, %fd367;
	ld.const.f64 	%fd369, [%rd105+24];
	fma.rn.f64 	%fd370, %fd368, %fd116, %fd369;
	ld.const.f64 	%fd371, [%rd105+32];
	fma.rn.f64 	%fd372, %fd370, %fd116, %fd371;
	ld.const.f64 	%fd373, [%rd105+40];
	fma.rn.f64 	%fd374, %fd372, %fd116, %fd373;
	ld.const.f64 	%fd375, [%rd105+48];
	fma.rn.f64 	%fd117, %fd374, %fd116, %fd375;
	fma.rn.f64 	%fd470, %fd117, %fd469, %fd469;
	setp.eq.s32	%p49, %r168, 0;
	@%p49 bra 	BB2_75;

	mov.f64 	%fd376, 0d3FF0000000000000;
	fma.rn.f64 	%fd470, %fd117, %fd116, %fd376;

BB2_75:
	and.b32  	%r170, %r34, 2;
	setp.eq.s32	%p50, %r170, 0;
	@%p50 bra 	BB2_77;

	mov.f64 	%fd377, 0d0000000000000000;
	mov.f64 	%fd378, 0dBFF0000000000000;
	fma.rn.f64 	%fd470, %fd470, %fd378, %fd377;

BB2_77:
	ld.global.f32 	%f78, [%rd10+20];
	cvt.f64.f32	%fd471, %f78;
	abs.f64 	%fd379, %fd471;
	setp.neu.f64	%p51, %fd379, 0d7FF0000000000000;
	@%p51 bra 	BB2_79;

	mov.f64 	%fd380, 0d0000000000000000;
	mul.rn.f64 	%fd471, %fd471, %fd380;

BB2_79:
	mul.f64 	%fd381, %fd471, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r234, %fd381;
	st.local.u32 	[%rd20], %r234;
	cvt.rn.f64.s32	%fd382, %r234;
	neg.f64 	%fd383, %fd382;
	fma.rn.f64 	%fd385, %fd383, %fd148, %fd471;
	fma.rn.f64 	%fd387, %fd383, %fd150, %fd385;
	fma.rn.f64 	%fd472, %fd383, %fd152, %fd387;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd471;
	}
	and.b32  	%r177, %r176, 2145386496;
	setp.lt.u32	%p52, %r177, 1105199104;
	@%p52 bra 	BB2_81;

	// Callseq Start 29
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd471;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd19;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd472, [retval0+0];
	
	//{
	}// Callseq End 29
	ld.local.u32 	%r234, [%rd20];

BB2_81:
	add.s32 	%r38, %r234, 1;
	and.b32  	%r178, %r38, 1;
	shl.b32 	%r179, %r178, 3;
	setp.eq.b32	%p53, %r178, 1;
	selp.f64	%fd389, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p53;
	mul.wide.u32 	%rd113, %r179, 8;
	add.s64 	%rd115, %rd113, %rd24;
	ld.const.f64 	%fd390, [%rd115+8];
	mul.rn.f64 	%fd129, %fd472, %fd472;
	fma.rn.f64 	%fd391, %fd389, %fd129, %fd390;
	ld.const.f64 	%fd392, [%rd115+16];
	fma.rn.f64 	%fd393, %fd391, %fd129, %fd392;
	ld.const.f64 	%fd394, [%rd115+24];
	fma.rn.f64 	%fd395, %fd393, %fd129, %fd394;
	ld.const.f64 	%fd396, [%rd115+32];
	fma.rn.f64 	%fd397, %fd395, %fd129, %fd396;
	ld.const.f64 	%fd398, [%rd115+40];
	fma.rn.f64 	%fd399, %fd397, %fd129, %fd398;
	ld.const.f64 	%fd400, [%rd115+48];
	fma.rn.f64 	%fd130, %fd399, %fd129, %fd400;
	fma.rn.f64 	%fd473, %fd130, %fd472, %fd472;
	setp.eq.s32	%p54, %r178, 0;
	@%p54 bra 	BB2_83;

	mov.f64 	%fd401, 0d3FF0000000000000;
	fma.rn.f64 	%fd473, %fd130, %fd129, %fd401;

BB2_83:
	and.b32  	%r180, %r38, 2;
	setp.eq.s32	%p55, %r180, 0;
	@%p55 bra 	BB2_85;

	mov.f64 	%fd402, 0d0000000000000000;
	mov.f64 	%fd403, 0dBFF0000000000000;
	fma.rn.f64 	%fd473, %fd473, %fd403, %fd402;

BB2_85:
	ld.global.f32 	%f81, [%rd10+16];
	add.f32 	%f82, %f81, %f81;
	mul.f32 	%f83, %f81, %f82;
	cvt.f64.f32	%fd404, %f83;
	mul.f64 	%fd405, %fd470, %fd473;
	div.rn.f64 	%fd406, %fd405, %fd404;
	add.f64 	%fd136, %fd109, %fd406;
	mov.f32 	%f118, 0f00000000;
	mov.f32 	%f117, %f118;
	@%p2 bra 	BB2_91;

	cvt.rn.f32.f64	%f16, %fd136;
	ld.global.f32 	%f86, [%rd10];
	cvt.f64.f32	%fd137, %f86;
	ld.global.f32 	%f17, [%rd10+4];
	add.f32 	%f18, %f15, %f15;
	ld.global.f32 	%f19, [%rd10+8];
	ld.global.f32 	%f20, [%rd10+24];
	cvta.to.global.u64 	%rd1, %rd2;
	mul.lo.s32 	%r40, %r51, %r52;
	mov.f32 	%f118, 0f00000000;
	mov.u32 	%r235, 0;
	mov.f32 	%f117, %f118;

BB2_87:
	ld.param.u32 	%r222, [gaussFitter2_param_4];
	rem.s32 	%r193, %r235, %r222;
	cvt.u16.u32	%rs1, %r193;
	div.s32 	%r194, %r235, %r222;
	cvt.u16.u32	%rs2, %r194;
	cvt.rn.f32.u16	%f87, %rs1;
	sub.f32 	%f88, %f87, %f17;
	mul.f32 	%f89, %f14, %f88;
	mul.f32 	%f90, %f88, %f89;
	mul.f32 	%f91, %f18, %f88;
	cvt.rn.f32.u16	%f92, %rs2;
	sub.f32 	%f93, %f92, %f19;
	mul.f32 	%f94, %f91, %f93;
	sub.f32 	%f95, %f90, %f94;
	mul.f32 	%f96, %f16, %f93;
	fma.rn.f32 	%f23, %f93, %f96, %f95;
	cvt.f64.f32	%fd138, %f23;
	neg.f64 	%fd407, %fd138;
	mov.f64 	%fd408, 0d4338000000000000;
	mov.f64 	%fd409, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd410, %fd407, %fd409, %fd408;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r42, %temp}, %fd410;
	}
	mov.f64 	%fd411, 0dC338000000000000;
	add.rn.f64 	%fd412, %fd410, %fd411;
	mov.f64 	%fd413, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd414, %fd412, %fd413, %fd407;
	mov.f64 	%fd415, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd416, %fd412, %fd415, %fd414;
	mov.f64 	%fd417, 0d3E928AF3FCA213EA;
	mov.f64 	%fd418, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd419, %fd418, %fd416, %fd417;
	mov.f64 	%fd420, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd421, %fd419, %fd416, %fd420;
	mov.f64 	%fd422, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd423, %fd421, %fd416, %fd422;
	mov.f64 	%fd424, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd425, %fd423, %fd416, %fd424;
	mov.f64 	%fd426, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd427, %fd425, %fd416, %fd426;
	mov.f64 	%fd428, 0d3F81111111122322;
	fma.rn.f64 	%fd429, %fd427, %fd416, %fd428;
	mov.f64 	%fd430, 0d3FA55555555502A1;
	fma.rn.f64 	%fd431, %fd429, %fd416, %fd430;
	mov.f64 	%fd432, 0d3FC5555555555511;
	fma.rn.f64 	%fd433, %fd431, %fd416, %fd432;
	mov.f64 	%fd434, 0d3FE000000000000B;
	fma.rn.f64 	%fd435, %fd433, %fd416, %fd434;
	mov.f64 	%fd436, 0d3FF0000000000000;
	fma.rn.f64 	%fd437, %fd435, %fd416, %fd436;
	fma.rn.f64 	%fd438, %fd437, %fd416, %fd436;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r43, %temp}, %fd438;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r44}, %fd438;
	}
	shl.b32 	%r195, %r42, 20;
	add.s32 	%r196, %r44, %r195;
	mov.b64 	%fd474, {%r43, %r196};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r197}, %fd407;
	}
	mov.b32 	 %f97, %r197;
	abs.f32 	%f24, %f97;
	setp.lt.f32	%p57, %f24, 0f4086232B;
	@%p57 bra 	BB2_90;

	setp.gt.f32	%p58, %f23, 0f80000000;
	mov.f64 	%fd439, 0d7FF0000000000000;
	sub.f64 	%fd440, %fd439, %fd138;
	selp.f64	%fd474, 0d0000000000000000, %fd440, %p58;
	setp.geu.f32	%p59, %f24, 0f40874800;
	@%p59 bra 	BB2_90;

	shr.u32 	%r198, %r42, 31;
	add.s32 	%r199, %r42, %r198;
	shr.s32 	%r200, %r199, 1;
	shl.b32 	%r201, %r200, 20;
	add.s32 	%r202, %r201, %r44;
	mov.b64 	%fd441, {%r43, %r202};
	sub.s32 	%r203, %r42, %r200;
	shl.b32 	%r204, %r203, 20;
	add.s32 	%r205, %r204, 1072693248;
	mov.u32 	%r206, 0;
	mov.b64 	%fd442, {%r206, %r205};
	mul.f64 	%fd474, %fd441, %fd442;

BB2_90:
	ld.param.u32 	%r216, [gaussFitter2_param_4];
	mul.lo.s32 	%r215, %r216, %r216;
	mul.f64 	%fd443, %fd137, %fd474;
	cvt.rn.f32.f64	%f98, %fd443;
	add.f32 	%f99, %f20, %f98;
	add.f32 	%f117, %f117, %f99;
	add.s32 	%r207, %r235, %r40;
	mul.wide.s32 	%rd122, %r207, 4;
	add.s64 	%rd123, %rd1, %rd122;
	ld.global.u32 	%r208, [%rd123];
	cvt.rn.f32.s32	%f100, %r208;
	sub.f32 	%f101, %f99, %f100;
	fma.rn.f32 	%f118, %f101, %f101, %f118;
	add.s32 	%r209, %r235, 1;
	and.b32  	%r235, %r209, 65535;
	setp.lt.u32	%p60, %r235, %r215;
	@%p60 bra 	BB2_87;

BB2_91:
	ld.param.u64 	%rd130, [gaussFitter2_param_2];
	mov.u32 	%r221, %ctaid.x;
	mov.u32 	%r220, %nctaid.x;
	mov.u32 	%r219, %ctaid.y;
	mad.lo.s32 	%r218, %r219, %r220, %r221;
	mul.lo.s32 	%r217, %r218, 7;
	mul.wide.s32 	%rd129, %r217, 4;
	cvta.to.global.u64 	%rd128, %rd130;
	add.s64 	%rd127, %rd128, %rd129;
	st.global.f32 	[%rd127], %f117;
	div.rn.f32 	%f102, %f118, %f115;
	mov.f32 	%f103, 0f3F800000;
	sub.f32 	%f104, %f103, %f102;
	st.global.f32 	[%rd127+24], %f104;

BB2_92:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot3[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot3;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB3_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB3_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd92, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	neg.s32 	%r25, %r24;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd90, %rd45, 120;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r7;

BB3_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r39;
	mov.u64 	%rd7, %rd91;
	ld.const.u64 	%rd48, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd48;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd46, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd92], %rd46;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r26, %r9, %r7;
	mul.wide.s32 	%rd51, %r26, 8;
	add.s64 	%rd92, %rd1, %rd51;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd93, %rd13;
	add.s64 	%rd90, %rd90, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd91, %rd13;
	mov.u32 	%r39, %r9;
	@%p3 bra 	BB3_3;

BB3_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB3_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r10;
	shl.b64 	%rd52, %rd96, %r10;
	shr.u64 	%rd53, %rd95, %r28;
	or.b64  	%rd96, %rd52, %rd53;
	shl.b64 	%rd54, %rd95, %r10;
	ld.local.u64 	%rd55, [%rd1+8];
	shr.u64 	%rd56, %rd55, %r28;
	or.b64  	%rd95, %rd56, %rd54;

BB3_6:
	cvta.to.local.u64 	%rd57, %rd37;
	shr.u64 	%rd58, %rd96, 62;
	cvt.u32.u64	%r29, %rd58;
	shr.u64 	%rd59, %rd95, 62;
	shl.b64 	%rd60, %rd96, 2;
	or.b64  	%rd98, %rd60, %rd59;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd61, %rd96, 61;
	cvt.u32.u64	%r30, %rd61;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.ne.s32	%p5, %r40, 0;
	selp.b32	%r34, %r33, %r32, %p5;
	st.local.u32 	[%rd57], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB3_8;

	mov.u64 	%rd65, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd65;
	mov.b64         {a2,a3}, %rd65;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB3_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB3_10;

	shl.b64 	%rd68, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd69, %rd97, %r36;
	or.b64  	%rd98, %rd69, %rd68;

BB3_10:
	mov.u64 	%rd73, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd73;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd70, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB3_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd70;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd70;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd74, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB3_12:
	cvt.u64.u32	%rd80, %r40;
	shl.b64 	%rd81, %rd80, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd82, %r38;
	shl.b64 	%rd83, %rd82, 52;
	add.s64 	%rd84, %rd99, 1;
	shr.u64 	%rd85, %rd84, 10;
	add.s64 	%rd86, %rd85, 1;
	shr.u64 	%rd87, %rd86, 1;
	add.s64 	%rd88, %rd87, %rd83;
	or.b64  	%rd89, %rd88, %rd81;
	mov.b64 	 %fd4, %rd89;

BB3_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


