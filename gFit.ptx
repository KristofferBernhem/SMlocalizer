//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-20732876
// Cuda compilation tools, release 8.0, V8.0.26
// Based on LLVM 3.4svn
//

.version 5.0
.target sm_20
.address_size 64

	// .globl	gaussFitter
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry gaussFitter(
	.param .u64 gaussFitter_param_0,
	.param .u32 gaussFitter_param_1,
	.param .u64 gaussFitter_param_2,
	.param .u32 gaussFitter_param_3,
	.param .u32 gaussFitter_param_4,
	.param .u64 gaussFitter_param_5,
	.param .u32 gaussFitter_param_6,
	.param .u64 gaussFitter_param_7,
	.param .u32 gaussFitter_param_8,
	.param .f64 gaussFitter_param_9,
	.param .u32 gaussFitter_param_10
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<144>;
	.reg .b16 	%rs<16>;
	.reg .f32 	%f<314>;
	.reg .b32 	%r<342>;
	.reg .f64 	%fd<995>;
	.reg .b64 	%rd<199>;


	mov.u64 	%rd198, __local_depot0;
	cvta.local.u64 	%SP, %rd198;
	ld.param.u64 	%rd12, [gaussFitter_param_0];
	ld.param.u32 	%r102, [gaussFitter_param_1];
	ld.param.u64 	%rd13, [gaussFitter_param_2];
	ld.param.u32 	%r100, [gaussFitter_param_4];
	ld.param.u64 	%rd14, [gaussFitter_param_5];
	ld.param.u64 	%rd15, [gaussFitter_param_7];
	ld.param.f64 	%fd289, [gaussFitter_param_9];
	ld.param.u32 	%r101, [gaussFitter_param_10];
	cvta.to.global.u64 	%rd1, %rd12;
	mov.u32 	%r103, %ctaid.y;
	mov.u32 	%r104, %nctaid.x;
	mov.u32 	%r105, %ctaid.x;
	mad.lo.s32 	%r1, %r103, %r104, %r105;
	mul.lo.s32 	%r2, %r100, %r100;
	div.s32 	%r106, %r102, %r2;
	setp.ge.s32	%p1, %r1, %r106;
	@%p1 bra 	BB0_213;

	mul.lo.s32 	%r3, %r1, 7;
	mul.lo.s32 	%r4, %r1, %r2;
	setp.eq.s32	%p2, %r2, 0;
	mov.f32 	%f290, 0f00000000;
	mov.f32 	%f287, %f290;
	mov.f32 	%f284, %f290;
	mov.u32 	%r312, 0;
	mov.f32 	%f289, %f290;
	mov.f32 	%f286, %f290;
	mov.f32 	%f283, %f290;
	@%p2 bra 	BB0_3;

BB0_2:
	add.s32 	%r108, %r312, %r4;
	mul.wide.s32 	%rd16, %r108, 4;
	add.s64 	%rd17, %rd1, %rd16;
	ldu.global.u32 	%r109, [%rd17];
	cvt.rn.f32.s32	%f110, %r109;
	add.f32 	%f290, %f290, %f110;
	rem.s32 	%r110, %r312, %r100;
	mul.lo.s32 	%r111, %r110, %r109;
	cvt.rn.f32.s32	%f111, %r111;
	add.f32 	%f284, %f284, %f111;
	div.s32 	%r112, %r312, %r100;
	mul.lo.s32 	%r113, %r109, %r112;
	cvt.rn.f32.s32	%f112, %r113;
	add.f32 	%f287, %f287, %f112;
	add.s32 	%r312, %r312, 1;
	setp.lt.s32	%p3, %r312, %r2;
	mov.f32 	%f283, %f284;
	mov.f32 	%f286, %f287;
	mov.f32 	%f289, %f290;
	@%p3 bra 	BB0_2;

BB0_3:
	cvta.to.global.u64 	%rd18, %rd13;
	mul.wide.s32 	%rd19, %r3, 4;
	add.s64 	%rd2, %rd18, %rd19;
	div.rn.f32 	%f115, %f283, %f289;
	st.global.f32 	[%rd2+4], %f115;
	div.rn.f32 	%f116, %f286, %f289;
	st.global.f32 	[%rd2+8], %f116;
	cvt.rn.f32.s32	%f117, %r2;
	div.rn.f32 	%f10, %f289, %f117;
	mov.f32 	%f293, 0f00000000;
	mov.u32 	%r313, 0;
	mov.f32 	%f292, %f293;
	@%p2 bra 	BB0_5;

BB0_4:
	add.s32 	%r115, %r313, %r4;
	mul.wide.s32 	%rd20, %r115, 4;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u32 	%r116, [%rd21];
	cvt.rn.f32.s32	%f118, %r116;
	sub.f32 	%f119, %f118, %f10;
	fma.rn.f32 	%f293, %f119, %f119, %f293;
	add.s32 	%r313, %r313, 1;
	setp.lt.s32	%p5, %r313, %r2;
	mov.f32 	%f292, %f293;
	@%p5 bra 	BB0_4;

BB0_5:
	cvta.to.global.u64 	%rd22, %rd14;
	cvta.to.global.u64 	%rd23, %rd15;
	add.s64 	%rd25, %rd23, %rd19;
	ld.global.f32 	%f120, [%rd25];
	ld.global.f32 	%f121, [%rd2];
	mul.f32 	%f122, %f121, %f120;
	ld.global.f32 	%f123, [%rd25+24];
	ld.global.f32 	%f19, [%rd25+12];
	st.global.f32 	[%rd25], %f122;
	ld.global.f32 	%f124, [%rd2];
	mul.f32 	%f125, %f124, %f123;
	st.global.f32 	[%rd25+24], %f125;
	ld.global.f32 	%f126, [%rd22];
	ld.global.f32 	%f14, [%rd2];
	mul.f32 	%f299, %f14, %f126;
	ld.global.f32 	%f127, [%rd22+4];
	mul.f32 	%f298, %f14, %f127;
	ld.global.f32 	%f17, [%rd22+48];
	ld.global.f32 	%f18, [%rd22+52];
	ld.global.f32 	%f128, [%rd2+12];
	fma.rn.f32 	%f294, %f19, 0fC0400000, %f128;
	fma.rn.f32 	%f21, %f19, 0f40000000, %f128;
	setp.gtu.f32	%p6, %f294, %f21;
	@%p6 bra 	BB0_16;

	ld.global.f32 	%f22, [%rd25+16];
	ld.global.f32 	%f130, [%rd2+16];
	fma.rn.f32 	%f23, %f22, 0fC0400000, %f130;
	fma.rn.f32 	%f24, %f22, 0f40000000, %f130;
	mov.f32 	%f296, 0f3F800000;

BB0_7:
	add.f32 	%f131, %f294, %f294;
	mul.f32 	%f132, %f294, %f131;
	rcp.rn.f32 	%f29, %f132;
	setp.gtu.f32	%p7, %f23, %f24;
	mov.f32 	%f295, %f23;
	@%p7 bra 	BB0_15;

BB0_8:
	mov.f32 	%f30, %f295;
	mov.f32 	%f297, 0f00000000;
	@%p2 bra 	BB0_14;

	add.f32 	%f135, %f30, %f30;
	mul.f32 	%f136, %f30, %f135;
	rcp.rn.f32 	%f34, %f136;
	ld.global.f32 	%f35, [%rd2+4];
	ld.global.f32 	%f36, [%rd2+8];
	mov.f32 	%f297, 0f00000000;
	mov.u32 	%r314, 0;

BB0_10:
	rem.s32 	%r124, %r314, %r100;
	cvt.rn.f32.s32	%f137, %r124;
	sub.f32 	%f138, %f137, %f35;
	mul.f32 	%f139, %f29, %f138;
	mul.f32 	%f140, %f138, 0f80000000;
	div.s32 	%r125, %r314, %r100;
	cvt.rn.f32.s32	%f141, %r125;
	sub.f32 	%f142, %f141, %f36;
	mul.f32 	%f143, %f140, %f142;
	fma.rn.f32 	%f144, %f138, %f139, %f143;
	mul.f32 	%f145, %f34, %f142;
	fma.rn.f32 	%f38, %f142, %f145, %f144;
	cvt.f64.f32	%fd1, %f38;
	neg.f64 	%fd290, %fd1;
	mov.f64 	%fd291, 0d4338000000000000;
	mov.f64 	%fd292, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd293, %fd290, %fd292, %fd291;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r10, %temp}, %fd293;
	}
	mov.f64 	%fd294, 0dC338000000000000;
	add.rn.f64 	%fd295, %fd293, %fd294;
	mov.f64 	%fd296, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd297, %fd295, %fd296, %fd290;
	mov.f64 	%fd298, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd299, %fd295, %fd298, %fd297;
	mov.f64 	%fd300, 0d3E928AF3FCA213EA;
	mov.f64 	%fd301, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd302, %fd301, %fd299, %fd300;
	mov.f64 	%fd303, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd304, %fd302, %fd299, %fd303;
	mov.f64 	%fd305, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd306, %fd304, %fd299, %fd305;
	mov.f64 	%fd307, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd308, %fd306, %fd299, %fd307;
	mov.f64 	%fd309, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd310, %fd308, %fd299, %fd309;
	mov.f64 	%fd311, 0d3F81111111122322;
	fma.rn.f64 	%fd312, %fd310, %fd299, %fd311;
	mov.f64 	%fd313, 0d3FA55555555502A1;
	fma.rn.f64 	%fd314, %fd312, %fd299, %fd313;
	mov.f64 	%fd315, 0d3FC5555555555511;
	fma.rn.f64 	%fd316, %fd314, %fd299, %fd315;
	mov.f64 	%fd317, 0d3FE000000000000B;
	fma.rn.f64 	%fd318, %fd316, %fd299, %fd317;
	mov.f64 	%fd319, 0d3FF0000000000000;
	fma.rn.f64 	%fd320, %fd318, %fd299, %fd319;
	fma.rn.f64 	%fd321, %fd320, %fd299, %fd319;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r11, %temp}, %fd321;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r12}, %fd321;
	}
	shl.b32 	%r126, %r10, 20;
	add.s32 	%r127, %r12, %r126;
	mov.b64 	%fd932, {%r11, %r127};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r128}, %fd290;
	}
	mov.b32 	 %f146, %r128;
	abs.f32 	%f39, %f146;
	setp.lt.f32	%p9, %f39, 0f4086232B;
	@%p9 bra 	BB0_13;

	setp.gt.f32	%p10, %f38, 0f80000000;
	mov.f64 	%fd322, 0d7FF0000000000000;
	sub.f64 	%fd323, %fd322, %fd1;
	selp.f64	%fd932, 0d0000000000000000, %fd323, %p10;
	setp.geu.f32	%p11, %f39, 0f40874800;
	@%p11 bra 	BB0_13;

	shr.u32 	%r129, %r10, 31;
	add.s32 	%r130, %r10, %r129;
	shr.s32 	%r131, %r130, 1;
	shl.b32 	%r132, %r131, 20;
	add.s32 	%r133, %r132, %r12;
	mov.b64 	%fd324, {%r11, %r133};
	sub.s32 	%r134, %r10, %r131;
	shl.b32 	%r135, %r134, 20;
	add.s32 	%r136, %r135, 1072693248;
	mov.u32 	%r137, 0;
	mov.b64 	%fd325, {%r137, %r136};
	mul.f64 	%fd932, %fd324, %fd325;

BB0_13:
	cvt.f64.f32	%fd326, %f14;
	mul.f64 	%fd327, %fd326, %fd932;
	cvt.rn.f32.f64	%f147, %fd327;
	add.s32 	%r138, %r314, %r4;
	mul.wide.s32 	%rd30, %r138, 4;
	add.s64 	%rd31, %rd1, %rd30;
	ld.global.u32 	%r139, [%rd31];
	cvt.rn.f32.s32	%f148, %r139;
	sub.f32 	%f149, %f147, %f148;
	fma.rn.f32 	%f297, %f149, %f149, %f297;
	add.s32 	%r314, %r314, 1;
	setp.lt.s32	%p12, %r314, %r2;
	@%p12 bra 	BB0_10;

BB0_14:
	div.rn.f32 	%f150, %f297, %f292;
	setp.lt.f32	%p13, %f150, %f296;
	selp.f32	%f296, %f150, %f296, %p13;
	selp.f32	%f299, %f294, %f299, %p13;
	selp.f32	%f298, %f30, %f298, %p13;
	add.f32 	%f45, %f30, %f22;
	setp.le.f32	%p14, %f45, %f24;
	mov.f32 	%f295, %f45;
	@%p14 bra 	BB0_8;

BB0_15:
	add.f32 	%f294, %f294, %f19;
	setp.le.f32	%p15, %f294, %f21;
	@%p15 bra 	BB0_7;

BB0_16:
	mul.f32 	%f52, %f14, %f17;
	mul.f32 	%f53, %f14, %f18;
	st.global.f32 	[%rd2+12], %f299;
	st.global.f32 	[%rd2+16], %f298;
	ld.global.f32 	%f154, [%rd22];
	mul.f32 	%f54, %f14, %f154;
	ld.global.f32 	%f155, [%rd22+4];
	mul.f32 	%f55, %f14, %f155;
	add.f32 	%f156, %f299, %f299;
	mul.f32 	%f157, %f299, %f156;
	rcp.rn.f32 	%f308, %f157;
	add.f32 	%f158, %f298, %f298;
	mul.f32 	%f159, %f298, %f158;
	rcp.rn.f32 	%f310, %f159;
	mov.u16 	%rs15, 1;
	mov.u32 	%r319, 0;
	mov.u32 	%r317, %r319;
	mov.f32 	%f306, 0f3F800000;
	mov.f32 	%f300, %f306;
	mov.f32 	%f309, 0f00000000;

BB0_17:
	mov.u32 	%r16, %r319;
	mov.f32 	%f62, %f306;
	setp.eq.s32	%p16, %r317, 0;
	selp.f32	%f300, %f62, %f300, %p16;
	add.s32 	%r143, %r317, %r3;
	mul.wide.s32 	%rd33, %r143, 4;
	add.s64 	%rd5, %rd18, %rd33;
	add.s64 	%rd6, %rd23, %rd33;
	@%p16 bra 	BB0_23;
	bra.uni 	BB0_18;

BB0_23:
	ld.global.f32 	%f166, [%rd25];
	ld.global.f32 	%f167, [%rd2];
	add.f32 	%f66, %f167, %f166;
	mov.u16 	%rs14, 0;
	setp.leu.f32	%p22, %f66, %f54;
	@%p22 bra 	BB0_25;

	setp.lt.f32	%p23, %f66, %f55;
	selp.u16	%rs14, 1, 0, %p23;
	bra.uni 	BB0_25;

BB0_18:
	setp.eq.s32	%p17, %r317, 6;
	@%p17 bra 	BB0_21;
	bra.uni 	BB0_19;

BB0_21:
	ld.global.f32 	%f164, [%rd25+24];
	ld.global.f32 	%f165, [%rd2+24];
	add.f32 	%f65, %f165, %f164;
	mov.u16 	%rs14, 0;
	setp.leu.f32	%p20, %f65, %f52;
	@%p20 bra 	BB0_25;

	setp.lt.f32	%p21, %f65, %f53;
	selp.u16	%rs14, 1, 0, %p21;
	bra.uni 	BB0_25;

BB0_19:
	ld.global.f32 	%f160, [%rd5];
	ld.global.f32 	%f161, [%rd6];
	add.f32 	%f64, %f160, %f161;
	shl.b32 	%r144, %r317, 1;
	mul.wide.s32 	%rd35, %r144, 4;
	add.s64 	%rd7, %rd22, %rd35;
	ld.global.f32 	%f162, [%rd7];
	mov.u16 	%rs14, 0;
	setp.leu.f32	%p18, %f64, %f162;
	@%p18 bra 	BB0_25;

	ld.global.f32 	%f163, [%rd7+4];
	setp.lt.f32	%p19, %f64, %f163;
	selp.u16	%rs14, 1, 0, %p19;

BB0_25:
	ld.global.f32 	%f67, [%rd6];
	setp.eq.s16	%p24, %rs14, 0;
	@%p24 bra 	BB0_118;
	bra.uni 	BB0_26;

BB0_118:
	setp.lt.f32	%p83, %f67, 0f00000000;
	@%p83 bra 	BB0_120;
	bra.uni 	BB0_119;

BB0_120:
	div.rn.f32 	%f225, %f67, 0fBFC00000;
	st.global.f32 	[%rd6], %f225;
	bra.uni 	BB0_121;

BB0_26:
	ld.global.f32 	%f168, [%rd5];
	add.f32 	%f169, %f67, %f168;
	st.global.f32 	[%rd5], %f169;
	add.s32 	%r155, %r317, -3;
	setp.gt.u32	%p25, %r155, 2;
	@%p25 bra 	BB0_108;

	ld.global.f32 	%f170, [%rd2+20];
	cvt.f64.f32	%fd933, %f170;
	abs.f64 	%fd328, %fd933;
	setp.neu.f64	%p26, %fd328, 0d7FF0000000000000;
	@%p26 bra 	BB0_29;

	mov.f64 	%fd329, 0d0000000000000000;
	mul.rn.f64 	%fd933, %fd933, %fd329;

BB0_29:
	mul.f64 	%fd330, %fd933, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r320, %fd330;
	add.u64 	%rd44, %SP, 0;
	cvta.to.local.u64 	%rd45, %rd44;
	st.local.u32 	[%rd45], %r320;
	cvt.rn.f64.s32	%fd331, %r320;
	neg.f64 	%fd332, %fd331;
	mov.f64 	%fd333, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd334, %fd332, %fd333, %fd933;
	mov.f64 	%fd335, 0d3C91A62633145C00;
	fma.rn.f64 	%fd336, %fd332, %fd335, %fd334;
	mov.f64 	%fd337, 0d397B839A252049C0;
	fma.rn.f64 	%fd934, %fd332, %fd337, %fd336;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r156}, %fd933;
	}
	and.b32  	%r157, %r156, 2145386496;
	setp.lt.u32	%p27, %r157, 1105199104;
	@%p27 bra 	BB0_31;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd933;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd934, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r320, [%rd45];

BB0_31:
	add.s32 	%r20, %r320, 1;
	and.b32  	%r158, %r20, 1;
	shl.b32 	%r159, %r158, 3;
	setp.eq.b32	%p28, %r158, 1;
	selp.f64	%fd338, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p28;
	mul.wide.u32 	%rd48, %r159, 8;
	mov.u64 	%rd49, __cudart_sin_cos_coeffs;
	add.s64 	%rd50, %rd48, %rd49;
	ld.const.f64 	%fd339, [%rd50+8];
	mul.rn.f64 	%fd12, %fd934, %fd934;
	fma.rn.f64 	%fd340, %fd338, %fd12, %fd339;
	ld.const.f64 	%fd341, [%rd50+16];
	fma.rn.f64 	%fd342, %fd340, %fd12, %fd341;
	ld.const.f64 	%fd343, [%rd50+24];
	fma.rn.f64 	%fd344, %fd342, %fd12, %fd343;
	ld.const.f64 	%fd345, [%rd50+32];
	fma.rn.f64 	%fd346, %fd344, %fd12, %fd345;
	ld.const.f64 	%fd347, [%rd50+40];
	fma.rn.f64 	%fd348, %fd346, %fd12, %fd347;
	ld.const.f64 	%fd349, [%rd50+48];
	fma.rn.f64 	%fd13, %fd348, %fd12, %fd349;
	fma.rn.f64 	%fd935, %fd13, %fd934, %fd934;
	setp.eq.s32	%p29, %r158, 0;
	@%p29 bra 	BB0_33;

	mov.f64 	%fd350, 0d3FF0000000000000;
	fma.rn.f64 	%fd935, %fd13, %fd12, %fd350;

BB0_33:
	and.b32  	%r160, %r20, 2;
	setp.eq.s32	%p30, %r160, 0;
	@%p30 bra 	BB0_35;

	mov.f64 	%fd351, 0d0000000000000000;
	mov.f64 	%fd352, 0dBFF0000000000000;
	fma.rn.f64 	%fd935, %fd935, %fd352, %fd351;

BB0_35:
	ld.global.f32 	%f171, [%rd2+20];
	cvt.f64.f32	%fd936, %f171;
	abs.f64 	%fd353, %fd936;
	setp.neu.f64	%p31, %fd353, 0d7FF0000000000000;
	@%p31 bra 	BB0_37;

	mov.f64 	%fd354, 0d0000000000000000;
	mul.rn.f64 	%fd936, %fd936, %fd354;

BB0_37:
	mul.f64 	%fd355, %fd936, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r321, %fd355;
	st.local.u32 	[%rd45], %r321;
	cvt.rn.f64.s32	%fd356, %r321;
	neg.f64 	%fd357, %fd356;
	fma.rn.f64 	%fd359, %fd357, %fd333, %fd936;
	fma.rn.f64 	%fd361, %fd357, %fd335, %fd359;
	fma.rn.f64 	%fd937, %fd357, %fd337, %fd361;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r161}, %fd936;
	}
	and.b32  	%r162, %r161, 2145386496;
	setp.lt.u32	%p32, %r162, 1105199104;
	@%p32 bra 	BB0_39;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd936;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd937, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r321, [%rd45];

BB0_39:
	add.s32 	%r24, %r321, 1;
	and.b32  	%r163, %r24, 1;
	shl.b32 	%r164, %r163, 3;
	setp.eq.b32	%p33, %r163, 1;
	selp.f64	%fd363, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p33;
	mul.wide.u32 	%rd55, %r164, 8;
	add.s64 	%rd57, %rd55, %rd49;
	ld.const.f64 	%fd364, [%rd57+8];
	mul.rn.f64 	%fd25, %fd937, %fd937;
	fma.rn.f64 	%fd365, %fd363, %fd25, %fd364;
	ld.const.f64 	%fd366, [%rd57+16];
	fma.rn.f64 	%fd367, %fd365, %fd25, %fd366;
	ld.const.f64 	%fd368, [%rd57+24];
	fma.rn.f64 	%fd369, %fd367, %fd25, %fd368;
	ld.const.f64 	%fd370, [%rd57+32];
	fma.rn.f64 	%fd371, %fd369, %fd25, %fd370;
	ld.const.f64 	%fd372, [%rd57+40];
	fma.rn.f64 	%fd373, %fd371, %fd25, %fd372;
	ld.const.f64 	%fd374, [%rd57+48];
	fma.rn.f64 	%fd26, %fd373, %fd25, %fd374;
	fma.rn.f64 	%fd938, %fd26, %fd937, %fd937;
	setp.eq.s32	%p34, %r163, 0;
	@%p34 bra 	BB0_41;

	mov.f64 	%fd375, 0d3FF0000000000000;
	fma.rn.f64 	%fd938, %fd26, %fd25, %fd375;

BB0_41:
	and.b32  	%r165, %r24, 2;
	setp.eq.s32	%p35, %r165, 0;
	@%p35 bra 	BB0_43;

	mov.f64 	%fd376, 0d0000000000000000;
	mov.f64 	%fd377, 0dBFF0000000000000;
	fma.rn.f64 	%fd938, %fd938, %fd377, %fd376;

BB0_43:
	ld.global.f32 	%f172, [%rd2+12];
	add.f32 	%f173, %f172, %f172;
	mul.f32 	%f174, %f172, %f173;
	cvt.f64.f32	%fd378, %f174;
	mul.f64 	%fd379, %fd935, %fd938;
	div.rn.f64 	%fd32, %fd379, %fd378;
	ld.global.f32 	%f175, [%rd2+20];
	cvt.f64.f32	%fd939, %f175;
	abs.f64 	%fd380, %fd939;
	setp.neu.f64	%p36, %fd380, 0d7FF0000000000000;
	@%p36 bra 	BB0_45;

	mov.f64 	%fd381, 0d0000000000000000;
	mul.rn.f64 	%fd939, %fd939, %fd381;

BB0_45:
	mul.f64 	%fd382, %fd939, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r322, %fd382;
	st.local.u32 	[%rd45], %r322;
	cvt.rn.f64.s32	%fd383, %r322;
	neg.f64 	%fd384, %fd383;
	fma.rn.f64 	%fd386, %fd384, %fd333, %fd939;
	fma.rn.f64 	%fd388, %fd384, %fd335, %fd386;
	fma.rn.f64 	%fd940, %fd384, %fd337, %fd388;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r166}, %fd939;
	}
	and.b32  	%r167, %r166, 2145386496;
	setp.lt.u32	%p37, %r167, 1105199104;
	@%p37 bra 	BB0_47;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd939;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd940, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r322, [%rd45];

BB0_47:
	and.b32  	%r168, %r322, 1;
	shl.b32 	%r169, %r168, 3;
	setp.eq.b32	%p38, %r168, 1;
	selp.f64	%fd390, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p38;
	mul.wide.u32 	%rd62, %r169, 8;
	add.s64 	%rd64, %rd62, %rd49;
	ld.const.f64 	%fd391, [%rd64+8];
	mul.rn.f64 	%fd39, %fd940, %fd940;
	fma.rn.f64 	%fd392, %fd390, %fd39, %fd391;
	ld.const.f64 	%fd393, [%rd64+16];
	fma.rn.f64 	%fd394, %fd392, %fd39, %fd393;
	ld.const.f64 	%fd395, [%rd64+24];
	fma.rn.f64 	%fd396, %fd394, %fd39, %fd395;
	ld.const.f64 	%fd397, [%rd64+32];
	fma.rn.f64 	%fd398, %fd396, %fd39, %fd397;
	ld.const.f64 	%fd399, [%rd64+40];
	fma.rn.f64 	%fd400, %fd398, %fd39, %fd399;
	ld.const.f64 	%fd401, [%rd64+48];
	fma.rn.f64 	%fd40, %fd400, %fd39, %fd401;
	fma.rn.f64 	%fd941, %fd40, %fd940, %fd940;
	setp.eq.s32	%p39, %r168, 0;
	@%p39 bra 	BB0_49;

	mov.f64 	%fd402, 0d3FF0000000000000;
	fma.rn.f64 	%fd941, %fd40, %fd39, %fd402;

BB0_49:
	and.b32  	%r170, %r322, 2;
	setp.eq.s32	%p40, %r170, 0;
	@%p40 bra 	BB0_51;

	mov.f64 	%fd403, 0d0000000000000000;
	mov.f64 	%fd404, 0dBFF0000000000000;
	fma.rn.f64 	%fd941, %fd941, %fd404, %fd403;

BB0_51:
	ld.global.f32 	%f176, [%rd2+20];
	cvt.f64.f32	%fd942, %f176;
	abs.f64 	%fd405, %fd942;
	setp.neu.f64	%p41, %fd405, 0d7FF0000000000000;
	@%p41 bra 	BB0_53;

	mov.f64 	%fd406, 0d0000000000000000;
	mul.rn.f64 	%fd942, %fd942, %fd406;

BB0_53:
	mul.f64 	%fd407, %fd942, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r323, %fd407;
	st.local.u32 	[%rd45], %r323;
	cvt.rn.f64.s32	%fd408, %r323;
	neg.f64 	%fd409, %fd408;
	fma.rn.f64 	%fd411, %fd409, %fd333, %fd942;
	fma.rn.f64 	%fd413, %fd409, %fd335, %fd411;
	fma.rn.f64 	%fd943, %fd409, %fd337, %fd413;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r171}, %fd942;
	}
	and.b32  	%r172, %r171, 2145386496;
	setp.lt.u32	%p42, %r172, 1105199104;
	@%p42 bra 	BB0_55;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd942;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd943, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r323, [%rd45];

BB0_55:
	and.b32  	%r173, %r323, 1;
	shl.b32 	%r174, %r173, 3;
	setp.eq.b32	%p43, %r173, 1;
	selp.f64	%fd415, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p43;
	mul.wide.u32 	%rd69, %r174, 8;
	add.s64 	%rd71, %rd69, %rd49;
	ld.const.f64 	%fd416, [%rd71+8];
	mul.rn.f64 	%fd52, %fd943, %fd943;
	fma.rn.f64 	%fd417, %fd415, %fd52, %fd416;
	ld.const.f64 	%fd418, [%rd71+16];
	fma.rn.f64 	%fd419, %fd417, %fd52, %fd418;
	ld.const.f64 	%fd420, [%rd71+24];
	fma.rn.f64 	%fd421, %fd419, %fd52, %fd420;
	ld.const.f64 	%fd422, [%rd71+32];
	fma.rn.f64 	%fd423, %fd421, %fd52, %fd422;
	ld.const.f64 	%fd424, [%rd71+40];
	fma.rn.f64 	%fd425, %fd423, %fd52, %fd424;
	ld.const.f64 	%fd426, [%rd71+48];
	fma.rn.f64 	%fd53, %fd425, %fd52, %fd426;
	fma.rn.f64 	%fd944, %fd53, %fd943, %fd943;
	setp.eq.s32	%p44, %r173, 0;
	@%p44 bra 	BB0_57;

	mov.f64 	%fd427, 0d3FF0000000000000;
	fma.rn.f64 	%fd944, %fd53, %fd52, %fd427;

BB0_57:
	and.b32  	%r175, %r323, 2;
	setp.eq.s32	%p45, %r175, 0;
	@%p45 bra 	BB0_59;

	mov.f64 	%fd428, 0d0000000000000000;
	mov.f64 	%fd429, 0dBFF0000000000000;
	fma.rn.f64 	%fd944, %fd944, %fd429, %fd428;

BB0_59:
	ld.global.f32 	%f177, [%rd2+16];
	add.f32 	%f178, %f177, %f177;
	mul.f32 	%f179, %f177, %f178;
	cvt.f64.f32	%fd430, %f179;
	mul.f64 	%fd431, %fd941, %fd944;
	div.rn.f64 	%fd432, %fd431, %fd430;
	add.f64 	%fd59, %fd32, %fd432;
	ld.global.f32 	%f180, [%rd2+20];
	add.f32 	%f181, %f180, %f180;
	cvt.f64.f32	%fd945, %f181;
	abs.f64 	%fd433, %fd945;
	setp.neu.f64	%p46, %fd433, 0d7FF0000000000000;
	@%p46 bra 	BB0_61;

	mov.f64 	%fd434, 0d0000000000000000;
	mul.rn.f64 	%fd945, %fd945, %fd434;

BB0_61:
	mul.f64 	%fd435, %fd945, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r324, %fd435;
	st.local.u32 	[%rd45], %r324;
	cvt.rn.f64.s32	%fd436, %r324;
	neg.f64 	%fd437, %fd436;
	fma.rn.f64 	%fd439, %fd437, %fd333, %fd945;
	fma.rn.f64 	%fd441, %fd437, %fd335, %fd439;
	fma.rn.f64 	%fd946, %fd437, %fd337, %fd441;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r176}, %fd945;
	}
	and.b32  	%r177, %r176, 2145386496;
	setp.lt.u32	%p47, %r177, 1105199104;
	@%p47 bra 	BB0_63;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd945;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd946, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r324, [%rd45];

BB0_63:
	and.b32  	%r178, %r324, 1;
	shl.b32 	%r179, %r178, 3;
	setp.eq.b32	%p48, %r178, 1;
	selp.f64	%fd443, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p48;
	mul.wide.u32 	%rd76, %r179, 8;
	add.s64 	%rd78, %rd76, %rd49;
	ld.const.f64 	%fd444, [%rd78+8];
	mul.rn.f64 	%fd66, %fd946, %fd946;
	fma.rn.f64 	%fd445, %fd443, %fd66, %fd444;
	ld.const.f64 	%fd446, [%rd78+16];
	fma.rn.f64 	%fd447, %fd445, %fd66, %fd446;
	ld.const.f64 	%fd448, [%rd78+24];
	fma.rn.f64 	%fd449, %fd447, %fd66, %fd448;
	ld.const.f64 	%fd450, [%rd78+32];
	fma.rn.f64 	%fd451, %fd449, %fd66, %fd450;
	ld.const.f64 	%fd452, [%rd78+40];
	fma.rn.f64 	%fd453, %fd451, %fd66, %fd452;
	ld.const.f64 	%fd454, [%rd78+48];
	fma.rn.f64 	%fd67, %fd453, %fd66, %fd454;
	fma.rn.f64 	%fd947, %fd67, %fd946, %fd946;
	setp.eq.s32	%p49, %r178, 0;
	@%p49 bra 	BB0_65;

	mov.f64 	%fd455, 0d3FF0000000000000;
	fma.rn.f64 	%fd947, %fd67, %fd66, %fd455;

BB0_65:
	and.b32  	%r180, %r324, 2;
	setp.eq.s32	%p50, %r180, 0;
	@%p50 bra 	BB0_67;

	mov.f64 	%fd456, 0d0000000000000000;
	mov.f64 	%fd457, 0dBFF0000000000000;
	fma.rn.f64 	%fd947, %fd947, %fd457, %fd456;

BB0_67:
	cvt.rn.f32.f64	%f308, %fd59;
	cvt.rn.f32.f64	%f182, %fd947;
	cvt.f64.f32	%fd458, %f182;
	ld.global.f32 	%f183, [%rd2+12];
	mul.f32 	%f184, %f183, 0fC0800000;
	mul.f32 	%f185, %f183, %f184;
	cvt.f64.f32	%fd459, %f185;
	div.rn.f64 	%fd73, %fd458, %fd459;
	ld.global.f32 	%f186, [%rd2+20];
	add.f32 	%f187, %f186, %f186;
	cvt.f64.f32	%fd948, %f187;
	abs.f64 	%fd460, %fd948;
	setp.neu.f64	%p51, %fd460, 0d7FF0000000000000;
	@%p51 bra 	BB0_69;

	mov.f64 	%fd461, 0d0000000000000000;
	mul.rn.f64 	%fd948, %fd948, %fd461;

BB0_69:
	mul.f64 	%fd462, %fd948, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r325, %fd462;
	st.local.u32 	[%rd45], %r325;
	cvt.rn.f64.s32	%fd463, %r325;
	neg.f64 	%fd464, %fd463;
	fma.rn.f64 	%fd466, %fd464, %fd333, %fd948;
	fma.rn.f64 	%fd468, %fd464, %fd335, %fd466;
	fma.rn.f64 	%fd949, %fd464, %fd337, %fd468;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r181}, %fd948;
	}
	and.b32  	%r182, %r181, 2145386496;
	setp.lt.u32	%p52, %r182, 1105199104;
	@%p52 bra 	BB0_71;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd948;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd949, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r325, [%rd45];

BB0_71:
	and.b32  	%r183, %r325, 1;
	shl.b32 	%r184, %r183, 3;
	setp.eq.b32	%p53, %r183, 1;
	selp.f64	%fd470, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p53;
	mul.wide.u32 	%rd83, %r184, 8;
	add.s64 	%rd85, %rd83, %rd49;
	ld.const.f64 	%fd471, [%rd85+8];
	mul.rn.f64 	%fd80, %fd949, %fd949;
	fma.rn.f64 	%fd472, %fd470, %fd80, %fd471;
	ld.const.f64 	%fd473, [%rd85+16];
	fma.rn.f64 	%fd474, %fd472, %fd80, %fd473;
	ld.const.f64 	%fd475, [%rd85+24];
	fma.rn.f64 	%fd476, %fd474, %fd80, %fd475;
	ld.const.f64 	%fd477, [%rd85+32];
	fma.rn.f64 	%fd478, %fd476, %fd80, %fd477;
	ld.const.f64 	%fd479, [%rd85+40];
	fma.rn.f64 	%fd480, %fd478, %fd80, %fd479;
	ld.const.f64 	%fd481, [%rd85+48];
	fma.rn.f64 	%fd81, %fd480, %fd80, %fd481;
	fma.rn.f64 	%fd950, %fd81, %fd949, %fd949;
	setp.eq.s32	%p54, %r183, 0;
	@%p54 bra 	BB0_73;

	mov.f64 	%fd482, 0d3FF0000000000000;
	fma.rn.f64 	%fd950, %fd81, %fd80, %fd482;

BB0_73:
	and.b32  	%r185, %r325, 2;
	setp.eq.s32	%p55, %r185, 0;
	@%p55 bra 	BB0_75;

	mov.f64 	%fd483, 0d0000000000000000;
	mov.f64 	%fd484, 0dBFF0000000000000;
	fma.rn.f64 	%fd950, %fd950, %fd484, %fd483;

BB0_75:
	ld.global.f32 	%f188, [%rd2+16];
	mul.f32 	%f189, %f188, 0f40800000;
	mul.f32 	%f190, %f188, %f189;
	cvt.f64.f32	%fd485, %f190;
	div.rn.f64 	%fd486, %fd950, %fd485;
	add.f64 	%fd87, %fd73, %fd486;
	ld.global.f32 	%f191, [%rd2+20];
	cvt.f64.f32	%fd951, %f191;
	abs.f64 	%fd487, %fd951;
	setp.neu.f64	%p56, %fd487, 0d7FF0000000000000;
	@%p56 bra 	BB0_77;

	mov.f64 	%fd488, 0d0000000000000000;
	mul.rn.f64 	%fd951, %fd951, %fd488;

BB0_77:
	mul.f64 	%fd489, %fd951, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r326, %fd489;
	st.local.u32 	[%rd45], %r326;
	cvt.rn.f64.s32	%fd490, %r326;
	neg.f64 	%fd491, %fd490;
	fma.rn.f64 	%fd493, %fd491, %fd333, %fd951;
	fma.rn.f64 	%fd495, %fd491, %fd335, %fd493;
	fma.rn.f64 	%fd952, %fd491, %fd337, %fd495;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r186}, %fd951;
	}
	and.b32  	%r187, %r186, 2145386496;
	setp.lt.u32	%p57, %r187, 1105199104;
	@%p57 bra 	BB0_79;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd951;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd952, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r326, [%rd45];

BB0_79:
	and.b32  	%r188, %r326, 1;
	shl.b32 	%r189, %r188, 3;
	setp.eq.b32	%p58, %r188, 1;
	selp.f64	%fd497, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p58;
	mul.wide.u32 	%rd90, %r189, 8;
	add.s64 	%rd92, %rd90, %rd49;
	ld.const.f64 	%fd498, [%rd92+8];
	mul.rn.f64 	%fd94, %fd952, %fd952;
	fma.rn.f64 	%fd499, %fd497, %fd94, %fd498;
	ld.const.f64 	%fd500, [%rd92+16];
	fma.rn.f64 	%fd501, %fd499, %fd94, %fd500;
	ld.const.f64 	%fd502, [%rd92+24];
	fma.rn.f64 	%fd503, %fd501, %fd94, %fd502;
	ld.const.f64 	%fd504, [%rd92+32];
	fma.rn.f64 	%fd505, %fd503, %fd94, %fd504;
	ld.const.f64 	%fd506, [%rd92+40];
	fma.rn.f64 	%fd507, %fd505, %fd94, %fd506;
	ld.const.f64 	%fd508, [%rd92+48];
	fma.rn.f64 	%fd95, %fd507, %fd94, %fd508;
	fma.rn.f64 	%fd953, %fd95, %fd952, %fd952;
	setp.eq.s32	%p59, %r188, 0;
	@%p59 bra 	BB0_81;

	mov.f64 	%fd509, 0d3FF0000000000000;
	fma.rn.f64 	%fd953, %fd95, %fd94, %fd509;

BB0_81:
	and.b32  	%r190, %r326, 2;
	setp.eq.s32	%p60, %r190, 0;
	@%p60 bra 	BB0_83;

	mov.f64 	%fd510, 0d0000000000000000;
	mov.f64 	%fd511, 0dBFF0000000000000;
	fma.rn.f64 	%fd953, %fd953, %fd511, %fd510;

BB0_83:
	ld.global.f32 	%f192, [%rd2+20];
	cvt.f64.f32	%fd954, %f192;
	abs.f64 	%fd512, %fd954;
	setp.neu.f64	%p61, %fd512, 0d7FF0000000000000;
	@%p61 bra 	BB0_85;

	mov.f64 	%fd513, 0d0000000000000000;
	mul.rn.f64 	%fd954, %fd954, %fd513;

BB0_85:
	mul.f64 	%fd514, %fd954, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r327, %fd514;
	st.local.u32 	[%rd45], %r327;
	cvt.rn.f64.s32	%fd515, %r327;
	neg.f64 	%fd516, %fd515;
	fma.rn.f64 	%fd518, %fd516, %fd333, %fd954;
	fma.rn.f64 	%fd520, %fd516, %fd335, %fd518;
	fma.rn.f64 	%fd955, %fd516, %fd337, %fd520;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r191}, %fd954;
	}
	and.b32  	%r192, %r191, 2145386496;
	setp.lt.u32	%p62, %r192, 1105199104;
	@%p62 bra 	BB0_87;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd954;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd955, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r327, [%rd45];

BB0_87:
	and.b32  	%r193, %r327, 1;
	shl.b32 	%r194, %r193, 3;
	setp.eq.b32	%p63, %r193, 1;
	selp.f64	%fd522, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p63;
	mul.wide.u32 	%rd97, %r194, 8;
	add.s64 	%rd99, %rd97, %rd49;
	ld.const.f64 	%fd523, [%rd99+8];
	mul.rn.f64 	%fd107, %fd955, %fd955;
	fma.rn.f64 	%fd524, %fd522, %fd107, %fd523;
	ld.const.f64 	%fd525, [%rd99+16];
	fma.rn.f64 	%fd526, %fd524, %fd107, %fd525;
	ld.const.f64 	%fd527, [%rd99+24];
	fma.rn.f64 	%fd528, %fd526, %fd107, %fd527;
	ld.const.f64 	%fd529, [%rd99+32];
	fma.rn.f64 	%fd530, %fd528, %fd107, %fd529;
	ld.const.f64 	%fd531, [%rd99+40];
	fma.rn.f64 	%fd532, %fd530, %fd107, %fd531;
	ld.const.f64 	%fd533, [%rd99+48];
	fma.rn.f64 	%fd108, %fd532, %fd107, %fd533;
	fma.rn.f64 	%fd956, %fd108, %fd955, %fd955;
	setp.eq.s32	%p64, %r193, 0;
	@%p64 bra 	BB0_89;

	mov.f64 	%fd534, 0d3FF0000000000000;
	fma.rn.f64 	%fd956, %fd108, %fd107, %fd534;

BB0_89:
	and.b32  	%r195, %r327, 2;
	setp.eq.s32	%p65, %r195, 0;
	@%p65 bra 	BB0_91;

	mov.f64 	%fd535, 0d0000000000000000;
	mov.f64 	%fd536, 0dBFF0000000000000;
	fma.rn.f64 	%fd956, %fd956, %fd536, %fd535;

BB0_91:
	cvt.rn.f32.f64	%f309, %fd87;
	ld.global.f32 	%f193, [%rd2+12];
	add.f32 	%f194, %f193, %f193;
	mul.f32 	%f195, %f193, %f194;
	cvt.f64.f32	%fd537, %f195;
	mul.f64 	%fd538, %fd953, %fd956;
	div.rn.f64 	%fd114, %fd538, %fd537;
	ld.global.f32 	%f196, [%rd2+20];
	cvt.f64.f32	%fd957, %f196;
	abs.f64 	%fd539, %fd957;
	setp.neu.f64	%p66, %fd539, 0d7FF0000000000000;
	@%p66 bra 	BB0_93;

	mov.f64 	%fd540, 0d0000000000000000;
	mul.rn.f64 	%fd957, %fd957, %fd540;

BB0_93:
	mul.f64 	%fd541, %fd957, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r328, %fd541;
	st.local.u32 	[%rd45], %r328;
	cvt.rn.f64.s32	%fd542, %r328;
	neg.f64 	%fd543, %fd542;
	fma.rn.f64 	%fd545, %fd543, %fd333, %fd957;
	fma.rn.f64 	%fd547, %fd543, %fd335, %fd545;
	fma.rn.f64 	%fd958, %fd543, %fd337, %fd547;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r196}, %fd957;
	}
	and.b32  	%r197, %r196, 2145386496;
	setp.lt.u32	%p67, %r197, 1105199104;
	@%p67 bra 	BB0_95;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd957;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd958, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r328, [%rd45];

BB0_95:
	add.s32 	%r46, %r328, 1;
	and.b32  	%r198, %r46, 1;
	shl.b32 	%r199, %r198, 3;
	setp.eq.b32	%p68, %r198, 1;
	selp.f64	%fd549, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p68;
	mul.wide.u32 	%rd104, %r199, 8;
	add.s64 	%rd106, %rd104, %rd49;
	ld.const.f64 	%fd550, [%rd106+8];
	mul.rn.f64 	%fd121, %fd958, %fd958;
	fma.rn.f64 	%fd551, %fd549, %fd121, %fd550;
	ld.const.f64 	%fd552, [%rd106+16];
	fma.rn.f64 	%fd553, %fd551, %fd121, %fd552;
	ld.const.f64 	%fd554, [%rd106+24];
	fma.rn.f64 	%fd555, %fd553, %fd121, %fd554;
	ld.const.f64 	%fd556, [%rd106+32];
	fma.rn.f64 	%fd557, %fd555, %fd121, %fd556;
	ld.const.f64 	%fd558, [%rd106+40];
	fma.rn.f64 	%fd559, %fd557, %fd121, %fd558;
	ld.const.f64 	%fd560, [%rd106+48];
	fma.rn.f64 	%fd122, %fd559, %fd121, %fd560;
	fma.rn.f64 	%fd959, %fd122, %fd958, %fd958;
	setp.eq.s32	%p69, %r198, 0;
	@%p69 bra 	BB0_97;

	mov.f64 	%fd561, 0d3FF0000000000000;
	fma.rn.f64 	%fd959, %fd122, %fd121, %fd561;

BB0_97:
	and.b32  	%r200, %r46, 2;
	setp.eq.s32	%p70, %r200, 0;
	@%p70 bra 	BB0_99;

	mov.f64 	%fd562, 0d0000000000000000;
	mov.f64 	%fd563, 0dBFF0000000000000;
	fma.rn.f64 	%fd959, %fd959, %fd563, %fd562;

BB0_99:
	ld.global.f32 	%f197, [%rd2+20];
	cvt.f64.f32	%fd960, %f197;
	abs.f64 	%fd564, %fd960;
	setp.neu.f64	%p71, %fd564, 0d7FF0000000000000;
	@%p71 bra 	BB0_101;

	mov.f64 	%fd565, 0d0000000000000000;
	mul.rn.f64 	%fd960, %fd960, %fd565;

BB0_101:
	mul.f64 	%fd566, %fd960, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r329, %fd566;
	st.local.u32 	[%rd45], %r329;
	cvt.rn.f64.s32	%fd567, %r329;
	neg.f64 	%fd568, %fd567;
	fma.rn.f64 	%fd570, %fd568, %fd333, %fd960;
	fma.rn.f64 	%fd572, %fd568, %fd335, %fd570;
	fma.rn.f64 	%fd961, %fd568, %fd337, %fd572;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r201}, %fd960;
	}
	and.b32  	%r202, %r201, 2145386496;
	setp.lt.u32	%p72, %r202, 1105199104;
	@%p72 bra 	BB0_103;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd960;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd44;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd961, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r329, [%rd45];

BB0_103:
	add.s32 	%r50, %r329, 1;
	and.b32  	%r203, %r50, 1;
	shl.b32 	%r204, %r203, 3;
	setp.eq.b32	%p73, %r203, 1;
	selp.f64	%fd574, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p73;
	mul.wide.u32 	%rd111, %r204, 8;
	add.s64 	%rd113, %rd111, %rd49;
	ld.const.f64 	%fd575, [%rd113+8];
	mul.rn.f64 	%fd134, %fd961, %fd961;
	fma.rn.f64 	%fd576, %fd574, %fd134, %fd575;
	ld.const.f64 	%fd577, [%rd113+16];
	fma.rn.f64 	%fd578, %fd576, %fd134, %fd577;
	ld.const.f64 	%fd579, [%rd113+24];
	fma.rn.f64 	%fd580, %fd578, %fd134, %fd579;
	ld.const.f64 	%fd581, [%rd113+32];
	fma.rn.f64 	%fd582, %fd580, %fd134, %fd581;
	ld.const.f64 	%fd583, [%rd113+40];
	fma.rn.f64 	%fd584, %fd582, %fd134, %fd583;
	ld.const.f64 	%fd585, [%rd113+48];
	fma.rn.f64 	%fd135, %fd584, %fd134, %fd585;
	fma.rn.f64 	%fd962, %fd135, %fd961, %fd961;
	setp.eq.s32	%p74, %r203, 0;
	@%p74 bra 	BB0_105;

	mov.f64 	%fd586, 0d3FF0000000000000;
	fma.rn.f64 	%fd962, %fd135, %fd134, %fd586;

BB0_105:
	and.b32  	%r205, %r50, 2;
	setp.eq.s32	%p75, %r205, 0;
	@%p75 bra 	BB0_107;

	mov.f64 	%fd587, 0d0000000000000000;
	mov.f64 	%fd588, 0dBFF0000000000000;
	fma.rn.f64 	%fd962, %fd962, %fd588, %fd587;

BB0_107:
	ld.global.f32 	%f198, [%rd2+16];
	add.f32 	%f199, %f198, %f198;
	mul.f32 	%f200, %f198, %f199;
	cvt.f64.f32	%fd589, %f200;
	mul.f64 	%fd590, %fd959, %fd962;
	div.rn.f64 	%fd591, %fd590, %fd589;
	add.f64 	%fd592, %fd114, %fd591;
	cvt.rn.f32.f64	%f310, %fd592;

BB0_108:
	mov.f32 	%f311, 0f00000000;
	@%p2 bra 	BB0_114;

	ld.global.f32 	%f203, [%rd2];
	cvt.f64.f32	%fd141, %f203;
	ld.global.f32 	%f74, [%rd2+4];
	ld.global.f32 	%f76, [%rd2+8];
	ld.global.f32 	%f77, [%rd2+24];
	mov.f32 	%f311, 0f00000000;
	mov.u32 	%r330, 0;

BB0_110:
	add.f32 	%f281, %f309, %f309;
	rem.s32 	%r212, %r330, %r100;
	cvt.rn.f32.s32	%f204, %r212;
	sub.f32 	%f205, %f204, %f74;
	mul.f32 	%f206, %f308, %f205;
	mul.f32 	%f207, %f205, %f206;
	mul.f32 	%f208, %f281, %f205;
	div.s32 	%r213, %r330, %r100;
	cvt.rn.f32.s32	%f209, %r213;
	sub.f32 	%f210, %f209, %f76;
	mul.f32 	%f211, %f208, %f210;
	sub.f32 	%f212, %f207, %f211;
	mul.f32 	%f213, %f310, %f210;
	fma.rn.f32 	%f79, %f210, %f213, %f212;
	cvt.f64.f32	%fd142, %f79;
	neg.f64 	%fd593, %fd142;
	mov.f64 	%fd594, 0d4338000000000000;
	mov.f64 	%fd595, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd596, %fd593, %fd595, %fd594;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r53, %temp}, %fd596;
	}
	mov.f64 	%fd597, 0dC338000000000000;
	add.rn.f64 	%fd598, %fd596, %fd597;
	mov.f64 	%fd599, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd600, %fd598, %fd599, %fd593;
	mov.f64 	%fd601, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd602, %fd598, %fd601, %fd600;
	mov.f64 	%fd603, 0d3E928AF3FCA213EA;
	mov.f64 	%fd604, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd605, %fd604, %fd602, %fd603;
	mov.f64 	%fd606, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd607, %fd605, %fd602, %fd606;
	mov.f64 	%fd608, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd609, %fd607, %fd602, %fd608;
	mov.f64 	%fd610, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd611, %fd609, %fd602, %fd610;
	mov.f64 	%fd612, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd613, %fd611, %fd602, %fd612;
	mov.f64 	%fd614, 0d3F81111111122322;
	fma.rn.f64 	%fd615, %fd613, %fd602, %fd614;
	mov.f64 	%fd616, 0d3FA55555555502A1;
	fma.rn.f64 	%fd617, %fd615, %fd602, %fd616;
	mov.f64 	%fd618, 0d3FC5555555555511;
	fma.rn.f64 	%fd619, %fd617, %fd602, %fd618;
	mov.f64 	%fd620, 0d3FE000000000000B;
	fma.rn.f64 	%fd621, %fd619, %fd602, %fd620;
	mov.f64 	%fd622, 0d3FF0000000000000;
	fma.rn.f64 	%fd623, %fd621, %fd602, %fd622;
	fma.rn.f64 	%fd624, %fd623, %fd602, %fd622;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r54, %temp}, %fd624;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r55}, %fd624;
	}
	shl.b32 	%r214, %r53, 20;
	add.s32 	%r215, %r55, %r214;
	mov.b64 	%fd963, {%r54, %r215};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r216}, %fd593;
	}
	mov.b32 	 %f214, %r216;
	abs.f32 	%f80, %f214;
	setp.lt.f32	%p77, %f80, 0f4086232B;
	@%p77 bra 	BB0_113;

	setp.gt.f32	%p78, %f79, 0f80000000;
	mov.f64 	%fd625, 0d7FF0000000000000;
	sub.f64 	%fd626, %fd625, %fd142;
	selp.f64	%fd963, 0d0000000000000000, %fd626, %p78;
	setp.geu.f32	%p79, %f80, 0f40874800;
	@%p79 bra 	BB0_113;

	shr.u32 	%r217, %r53, 31;
	add.s32 	%r218, %r53, %r217;
	shr.s32 	%r219, %r218, 1;
	shl.b32 	%r220, %r219, 20;
	add.s32 	%r221, %r220, %r55;
	mov.b64 	%fd627, {%r54, %r221};
	sub.s32 	%r222, %r53, %r219;
	shl.b32 	%r223, %r222, 20;
	add.s32 	%r224, %r223, 1072693248;
	mov.u32 	%r225, 0;
	mov.b64 	%fd628, {%r225, %r224};
	mul.f64 	%fd963, %fd627, %fd628;

BB0_113:
	mul.f64 	%fd629, %fd141, %fd963;
	cvt.rn.f32.f64	%f215, %fd629;
	add.f32 	%f216, %f77, %f215;
	add.s32 	%r226, %r330, %r4;
	mul.wide.s32 	%rd117, %r226, 4;
	add.s64 	%rd118, %rd1, %rd117;
	ld.global.u32 	%r227, [%rd118];
	cvt.rn.f32.s32	%f217, %r227;
	sub.f32 	%f218, %f216, %f217;
	fma.rn.f32 	%f311, %f218, %f218, %f311;
	add.s32 	%r330, %r330, 1;
	setp.lt.s32	%p80, %r330, %r2;
	@%p80 bra 	BB0_110;

BB0_114:
	div.rn.f32 	%f83, %f311, %f292;
	setp.lt.f32	%p81, %f83, %f62;
	mov.f32 	%f307, %f83;
	@%p81 bra 	BB0_122;

	ld.global.f32 	%f219, [%rd6];
	ld.global.f32 	%f220, [%rd5];
	sub.f32 	%f221, %f220, %f219;
	st.global.f32 	[%rd5], %f221;
	ld.global.f32 	%f84, [%rd6];
	setp.lt.f32	%p82, %f84, 0f00000000;
	@%p82 bra 	BB0_117;
	bra.uni 	BB0_116;

BB0_117:
	div.rn.f32 	%f223, %f84, 0fBFC00000;
	st.global.f32 	[%rd6], %f223;
	bra.uni 	BB0_121;

BB0_119:
	neg.f32 	%f224, %f67;
	st.global.f32 	[%rd6], %f224;
	bra.uni 	BB0_121;

BB0_116:
	neg.f32 	%f222, %f84;
	st.global.f32 	[%rd6], %f222;

BB0_121:
	mov.f32 	%f307, %f62;

BB0_122:
	mov.f32 	%f306, %f307;
	add.s32 	%r318, %r317, 1;
	setp.lt.s32	%p84, %r317, 6;
	@%p84 bra 	BB0_125;

	mov.u32 	%r318, 0;
	setp.lt.s32	%p85, %r16, 41;
	@%p85 bra 	BB0_125;

	sub.f32 	%f226, %f300, %f306;
	cvt.f64.f32	%fd630, %f226;
	setp.lt.f64	%p86, %fd630, %fd289;
	selp.b16	%rs15, 0, %rs15, %p86;

BB0_125:
	mov.u32 	%r317, %r318;
	add.s32 	%r319, %r16, 1;
	setp.ge.s32	%p87, %r16, %r101;
	selp.b16	%rs15, 0, %rs15, %p87;
	and.b16  	%rs13, %rs15, 255;
	setp.ne.s16	%p88, %rs13, 0;
	@%p88 bra 	BB0_17;

	ld.global.f32 	%f227, [%rd2+20];
	cvt.f64.f32	%fd964, %f227;
	abs.f64 	%fd631, %fd964;
	setp.neu.f64	%p89, %fd631, 0d7FF0000000000000;
	@%p89 bra 	BB0_128;

	mov.f64 	%fd632, 0d0000000000000000;
	mul.rn.f64 	%fd964, %fd964, %fd632;

BB0_128:
	mul.f64 	%fd633, %fd964, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r331, %fd633;
	add.u64 	%rd119, %SP, 0;
	cvta.to.local.u64 	%rd120, %rd119;
	st.local.u32 	[%rd120], %r331;
	cvt.rn.f64.s32	%fd634, %r331;
	neg.f64 	%fd635, %fd634;
	mov.f64 	%fd636, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd637, %fd635, %fd636, %fd964;
	mov.f64 	%fd638, 0d3C91A62633145C00;
	fma.rn.f64 	%fd639, %fd635, %fd638, %fd637;
	mov.f64 	%fd640, 0d397B839A252049C0;
	fma.rn.f64 	%fd965, %fd635, %fd640, %fd639;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r230}, %fd964;
	}
	and.b32  	%r231, %r230, 2145386496;
	setp.lt.u32	%p90, %r231, 1105199104;
	@%p90 bra 	BB0_130;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd964;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd965, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r331, [%rd120];

BB0_130:
	add.s32 	%r63, %r331, 1;
	and.b32  	%r232, %r63, 1;
	shl.b32 	%r233, %r232, 3;
	setp.eq.b32	%p91, %r232, 1;
	selp.f64	%fd641, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p91;
	mul.wide.u32 	%rd123, %r233, 8;
	mov.u64 	%rd124, __cudart_sin_cos_coeffs;
	add.s64 	%rd125, %rd123, %rd124;
	ld.const.f64 	%fd642, [%rd125+8];
	mul.rn.f64 	%fd153, %fd965, %fd965;
	fma.rn.f64 	%fd643, %fd641, %fd153, %fd642;
	ld.const.f64 	%fd644, [%rd125+16];
	fma.rn.f64 	%fd645, %fd643, %fd153, %fd644;
	ld.const.f64 	%fd646, [%rd125+24];
	fma.rn.f64 	%fd647, %fd645, %fd153, %fd646;
	ld.const.f64 	%fd648, [%rd125+32];
	fma.rn.f64 	%fd649, %fd647, %fd153, %fd648;
	ld.const.f64 	%fd650, [%rd125+40];
	fma.rn.f64 	%fd651, %fd649, %fd153, %fd650;
	ld.const.f64 	%fd652, [%rd125+48];
	fma.rn.f64 	%fd154, %fd651, %fd153, %fd652;
	fma.rn.f64 	%fd966, %fd154, %fd965, %fd965;
	setp.eq.s32	%p92, %r232, 0;
	@%p92 bra 	BB0_132;

	mov.f64 	%fd653, 0d3FF0000000000000;
	fma.rn.f64 	%fd966, %fd154, %fd153, %fd653;

BB0_132:
	and.b32  	%r234, %r63, 2;
	setp.eq.s32	%p93, %r234, 0;
	@%p93 bra 	BB0_134;

	mov.f64 	%fd654, 0d0000000000000000;
	mov.f64 	%fd655, 0dBFF0000000000000;
	fma.rn.f64 	%fd966, %fd966, %fd655, %fd654;

BB0_134:
	ld.global.f32 	%f228, [%rd2+20];
	cvt.f64.f32	%fd967, %f228;
	abs.f64 	%fd656, %fd967;
	setp.neu.f64	%p94, %fd656, 0d7FF0000000000000;
	@%p94 bra 	BB0_136;

	mov.f64 	%fd657, 0d0000000000000000;
	mul.rn.f64 	%fd967, %fd967, %fd657;

BB0_136:
	mul.f64 	%fd658, %fd967, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r332, %fd658;
	st.local.u32 	[%rd120], %r332;
	cvt.rn.f64.s32	%fd659, %r332;
	neg.f64 	%fd660, %fd659;
	fma.rn.f64 	%fd662, %fd660, %fd636, %fd967;
	fma.rn.f64 	%fd664, %fd660, %fd638, %fd662;
	fma.rn.f64 	%fd968, %fd660, %fd640, %fd664;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r235}, %fd967;
	}
	and.b32  	%r236, %r235, 2145386496;
	setp.lt.u32	%p95, %r236, 1105199104;
	@%p95 bra 	BB0_138;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd967;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd968, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r332, [%rd120];

BB0_138:
	add.s32 	%r67, %r332, 1;
	and.b32  	%r237, %r67, 1;
	shl.b32 	%r238, %r237, 3;
	setp.eq.b32	%p96, %r237, 1;
	selp.f64	%fd666, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p96;
	mul.wide.u32 	%rd130, %r238, 8;
	add.s64 	%rd132, %rd130, %rd124;
	ld.const.f64 	%fd667, [%rd132+8];
	mul.rn.f64 	%fd166, %fd968, %fd968;
	fma.rn.f64 	%fd668, %fd666, %fd166, %fd667;
	ld.const.f64 	%fd669, [%rd132+16];
	fma.rn.f64 	%fd670, %fd668, %fd166, %fd669;
	ld.const.f64 	%fd671, [%rd132+24];
	fma.rn.f64 	%fd672, %fd670, %fd166, %fd671;
	ld.const.f64 	%fd673, [%rd132+32];
	fma.rn.f64 	%fd674, %fd672, %fd166, %fd673;
	ld.const.f64 	%fd675, [%rd132+40];
	fma.rn.f64 	%fd676, %fd674, %fd166, %fd675;
	ld.const.f64 	%fd677, [%rd132+48];
	fma.rn.f64 	%fd167, %fd676, %fd166, %fd677;
	fma.rn.f64 	%fd969, %fd167, %fd968, %fd968;
	setp.eq.s32	%p97, %r237, 0;
	@%p97 bra 	BB0_140;

	mov.f64 	%fd678, 0d3FF0000000000000;
	fma.rn.f64 	%fd969, %fd167, %fd166, %fd678;

BB0_140:
	and.b32  	%r239, %r67, 2;
	setp.eq.s32	%p98, %r239, 0;
	@%p98 bra 	BB0_142;

	mov.f64 	%fd679, 0d0000000000000000;
	mov.f64 	%fd680, 0dBFF0000000000000;
	fma.rn.f64 	%fd969, %fd969, %fd680, %fd679;

BB0_142:
	ld.global.f32 	%f229, [%rd2+12];
	add.f32 	%f230, %f229, %f229;
	mul.f32 	%f231, %f229, %f230;
	cvt.f64.f32	%fd681, %f231;
	mul.f64 	%fd682, %fd966, %fd969;
	div.rn.f64 	%fd173, %fd682, %fd681;
	ld.global.f32 	%f232, [%rd2+20];
	cvt.f64.f32	%fd970, %f232;
	abs.f64 	%fd683, %fd970;
	setp.neu.f64	%p99, %fd683, 0d7FF0000000000000;
	@%p99 bra 	BB0_144;

	mov.f64 	%fd684, 0d0000000000000000;
	mul.rn.f64 	%fd970, %fd970, %fd684;

BB0_144:
	mul.f64 	%fd685, %fd970, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r333, %fd685;
	st.local.u32 	[%rd120], %r333;
	cvt.rn.f64.s32	%fd686, %r333;
	neg.f64 	%fd687, %fd686;
	fma.rn.f64 	%fd689, %fd687, %fd636, %fd970;
	fma.rn.f64 	%fd691, %fd687, %fd638, %fd689;
	fma.rn.f64 	%fd971, %fd687, %fd640, %fd691;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r240}, %fd970;
	}
	and.b32  	%r241, %r240, 2145386496;
	setp.lt.u32	%p100, %r241, 1105199104;
	@%p100 bra 	BB0_146;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd970;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd971, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r333, [%rd120];

BB0_146:
	and.b32  	%r242, %r333, 1;
	shl.b32 	%r243, %r242, 3;
	setp.eq.b32	%p101, %r242, 1;
	selp.f64	%fd693, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p101;
	mul.wide.u32 	%rd137, %r243, 8;
	add.s64 	%rd139, %rd137, %rd124;
	ld.const.f64 	%fd694, [%rd139+8];
	mul.rn.f64 	%fd180, %fd971, %fd971;
	fma.rn.f64 	%fd695, %fd693, %fd180, %fd694;
	ld.const.f64 	%fd696, [%rd139+16];
	fma.rn.f64 	%fd697, %fd695, %fd180, %fd696;
	ld.const.f64 	%fd698, [%rd139+24];
	fma.rn.f64 	%fd699, %fd697, %fd180, %fd698;
	ld.const.f64 	%fd700, [%rd139+32];
	fma.rn.f64 	%fd701, %fd699, %fd180, %fd700;
	ld.const.f64 	%fd702, [%rd139+40];
	fma.rn.f64 	%fd703, %fd701, %fd180, %fd702;
	ld.const.f64 	%fd704, [%rd139+48];
	fma.rn.f64 	%fd181, %fd703, %fd180, %fd704;
	fma.rn.f64 	%fd972, %fd181, %fd971, %fd971;
	setp.eq.s32	%p102, %r242, 0;
	@%p102 bra 	BB0_148;

	mov.f64 	%fd705, 0d3FF0000000000000;
	fma.rn.f64 	%fd972, %fd181, %fd180, %fd705;

BB0_148:
	and.b32  	%r244, %r333, 2;
	setp.eq.s32	%p103, %r244, 0;
	@%p103 bra 	BB0_150;

	mov.f64 	%fd706, 0d0000000000000000;
	mov.f64 	%fd707, 0dBFF0000000000000;
	fma.rn.f64 	%fd972, %fd972, %fd707, %fd706;

BB0_150:
	ld.global.f32 	%f233, [%rd2+20];
	cvt.f64.f32	%fd973, %f233;
	abs.f64 	%fd708, %fd973;
	setp.neu.f64	%p104, %fd708, 0d7FF0000000000000;
	@%p104 bra 	BB0_152;

	mov.f64 	%fd709, 0d0000000000000000;
	mul.rn.f64 	%fd973, %fd973, %fd709;

BB0_152:
	mul.f64 	%fd710, %fd973, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r334, %fd710;
	st.local.u32 	[%rd120], %r334;
	cvt.rn.f64.s32	%fd711, %r334;
	neg.f64 	%fd712, %fd711;
	fma.rn.f64 	%fd714, %fd712, %fd636, %fd973;
	fma.rn.f64 	%fd716, %fd712, %fd638, %fd714;
	fma.rn.f64 	%fd974, %fd712, %fd640, %fd716;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r245}, %fd973;
	}
	and.b32  	%r246, %r245, 2145386496;
	setp.lt.u32	%p105, %r246, 1105199104;
	@%p105 bra 	BB0_154;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd973;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd974, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r334, [%rd120];

BB0_154:
	and.b32  	%r247, %r334, 1;
	shl.b32 	%r248, %r247, 3;
	setp.eq.b32	%p106, %r247, 1;
	selp.f64	%fd718, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p106;
	mul.wide.u32 	%rd144, %r248, 8;
	add.s64 	%rd146, %rd144, %rd124;
	ld.const.f64 	%fd719, [%rd146+8];
	mul.rn.f64 	%fd193, %fd974, %fd974;
	fma.rn.f64 	%fd720, %fd718, %fd193, %fd719;
	ld.const.f64 	%fd721, [%rd146+16];
	fma.rn.f64 	%fd722, %fd720, %fd193, %fd721;
	ld.const.f64 	%fd723, [%rd146+24];
	fma.rn.f64 	%fd724, %fd722, %fd193, %fd723;
	ld.const.f64 	%fd725, [%rd146+32];
	fma.rn.f64 	%fd726, %fd724, %fd193, %fd725;
	ld.const.f64 	%fd727, [%rd146+40];
	fma.rn.f64 	%fd728, %fd726, %fd193, %fd727;
	ld.const.f64 	%fd729, [%rd146+48];
	fma.rn.f64 	%fd194, %fd728, %fd193, %fd729;
	fma.rn.f64 	%fd975, %fd194, %fd974, %fd974;
	setp.eq.s32	%p107, %r247, 0;
	@%p107 bra 	BB0_156;

	mov.f64 	%fd730, 0d3FF0000000000000;
	fma.rn.f64 	%fd975, %fd194, %fd193, %fd730;

BB0_156:
	and.b32  	%r249, %r334, 2;
	setp.eq.s32	%p108, %r249, 0;
	@%p108 bra 	BB0_158;

	mov.f64 	%fd731, 0d0000000000000000;
	mov.f64 	%fd732, 0dBFF0000000000000;
	fma.rn.f64 	%fd975, %fd975, %fd732, %fd731;

BB0_158:
	ld.global.f32 	%f234, [%rd2+16];
	add.f32 	%f235, %f234, %f234;
	mul.f32 	%f236, %f234, %f235;
	cvt.f64.f32	%fd733, %f236;
	mul.f64 	%fd734, %fd972, %fd975;
	div.rn.f64 	%fd735, %fd734, %fd733;
	add.f64 	%fd200, %fd173, %fd735;
	ld.global.f32 	%f237, [%rd2+20];
	add.f32 	%f238, %f237, %f237;
	cvt.f64.f32	%fd976, %f238;
	abs.f64 	%fd736, %fd976;
	setp.neu.f64	%p109, %fd736, 0d7FF0000000000000;
	@%p109 bra 	BB0_160;

	mov.f64 	%fd737, 0d0000000000000000;
	mul.rn.f64 	%fd976, %fd976, %fd737;

BB0_160:
	mul.f64 	%fd738, %fd976, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r335, %fd738;
	st.local.u32 	[%rd120], %r335;
	cvt.rn.f64.s32	%fd739, %r335;
	neg.f64 	%fd740, %fd739;
	fma.rn.f64 	%fd742, %fd740, %fd636, %fd976;
	fma.rn.f64 	%fd744, %fd740, %fd638, %fd742;
	fma.rn.f64 	%fd977, %fd740, %fd640, %fd744;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r250}, %fd976;
	}
	and.b32  	%r251, %r250, 2145386496;
	setp.lt.u32	%p110, %r251, 1105199104;
	@%p110 bra 	BB0_162;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd976;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd977, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r335, [%rd120];

BB0_162:
	and.b32  	%r252, %r335, 1;
	shl.b32 	%r253, %r252, 3;
	setp.eq.b32	%p111, %r252, 1;
	selp.f64	%fd746, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p111;
	mul.wide.u32 	%rd151, %r253, 8;
	add.s64 	%rd153, %rd151, %rd124;
	ld.const.f64 	%fd747, [%rd153+8];
	mul.rn.f64 	%fd207, %fd977, %fd977;
	fma.rn.f64 	%fd748, %fd746, %fd207, %fd747;
	ld.const.f64 	%fd749, [%rd153+16];
	fma.rn.f64 	%fd750, %fd748, %fd207, %fd749;
	ld.const.f64 	%fd751, [%rd153+24];
	fma.rn.f64 	%fd752, %fd750, %fd207, %fd751;
	ld.const.f64 	%fd753, [%rd153+32];
	fma.rn.f64 	%fd754, %fd752, %fd207, %fd753;
	ld.const.f64 	%fd755, [%rd153+40];
	fma.rn.f64 	%fd756, %fd754, %fd207, %fd755;
	ld.const.f64 	%fd757, [%rd153+48];
	fma.rn.f64 	%fd208, %fd756, %fd207, %fd757;
	fma.rn.f64 	%fd978, %fd208, %fd977, %fd977;
	setp.eq.s32	%p112, %r252, 0;
	@%p112 bra 	BB0_164;

	mov.f64 	%fd758, 0d3FF0000000000000;
	fma.rn.f64 	%fd978, %fd208, %fd207, %fd758;

BB0_164:
	and.b32  	%r254, %r335, 2;
	setp.eq.s32	%p113, %r254, 0;
	@%p113 bra 	BB0_166;

	mov.f64 	%fd759, 0d0000000000000000;
	mov.f64 	%fd760, 0dBFF0000000000000;
	fma.rn.f64 	%fd978, %fd978, %fd760, %fd759;

BB0_166:
	cvt.rn.f32.f64	%f89, %fd200;
	cvt.rn.f32.f64	%f239, %fd978;
	cvt.f64.f32	%fd761, %f239;
	ld.global.f32 	%f240, [%rd2+12];
	mul.f32 	%f241, %f240, 0fC0800000;
	mul.f32 	%f242, %f240, %f241;
	cvt.f64.f32	%fd762, %f242;
	div.rn.f64 	%fd214, %fd761, %fd762;
	ld.global.f32 	%f243, [%rd2+20];
	add.f32 	%f244, %f243, %f243;
	cvt.f64.f32	%fd979, %f244;
	abs.f64 	%fd763, %fd979;
	setp.neu.f64	%p114, %fd763, 0d7FF0000000000000;
	@%p114 bra 	BB0_168;

	mov.f64 	%fd764, 0d0000000000000000;
	mul.rn.f64 	%fd979, %fd979, %fd764;

BB0_168:
	mul.f64 	%fd765, %fd979, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r336, %fd765;
	st.local.u32 	[%rd120], %r336;
	cvt.rn.f64.s32	%fd766, %r336;
	neg.f64 	%fd767, %fd766;
	fma.rn.f64 	%fd769, %fd767, %fd636, %fd979;
	fma.rn.f64 	%fd771, %fd767, %fd638, %fd769;
	fma.rn.f64 	%fd980, %fd767, %fd640, %fd771;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r255}, %fd979;
	}
	and.b32  	%r256, %r255, 2145386496;
	setp.lt.u32	%p115, %r256, 1105199104;
	@%p115 bra 	BB0_170;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd979;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd980, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r336, [%rd120];

BB0_170:
	and.b32  	%r257, %r336, 1;
	shl.b32 	%r258, %r257, 3;
	setp.eq.b32	%p116, %r257, 1;
	selp.f64	%fd773, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p116;
	mul.wide.u32 	%rd158, %r258, 8;
	add.s64 	%rd160, %rd158, %rd124;
	ld.const.f64 	%fd774, [%rd160+8];
	mul.rn.f64 	%fd221, %fd980, %fd980;
	fma.rn.f64 	%fd775, %fd773, %fd221, %fd774;
	ld.const.f64 	%fd776, [%rd160+16];
	fma.rn.f64 	%fd777, %fd775, %fd221, %fd776;
	ld.const.f64 	%fd778, [%rd160+24];
	fma.rn.f64 	%fd779, %fd777, %fd221, %fd778;
	ld.const.f64 	%fd780, [%rd160+32];
	fma.rn.f64 	%fd781, %fd779, %fd221, %fd780;
	ld.const.f64 	%fd782, [%rd160+40];
	fma.rn.f64 	%fd783, %fd781, %fd221, %fd782;
	ld.const.f64 	%fd784, [%rd160+48];
	fma.rn.f64 	%fd222, %fd783, %fd221, %fd784;
	fma.rn.f64 	%fd981, %fd222, %fd980, %fd980;
	setp.eq.s32	%p117, %r257, 0;
	@%p117 bra 	BB0_172;

	mov.f64 	%fd785, 0d3FF0000000000000;
	fma.rn.f64 	%fd981, %fd222, %fd221, %fd785;

BB0_172:
	and.b32  	%r259, %r336, 2;
	setp.eq.s32	%p118, %r259, 0;
	@%p118 bra 	BB0_174;

	mov.f64 	%fd786, 0d0000000000000000;
	mov.f64 	%fd787, 0dBFF0000000000000;
	fma.rn.f64 	%fd981, %fd981, %fd787, %fd786;

BB0_174:
	ld.global.f32 	%f245, [%rd2+16];
	mul.f32 	%f246, %f245, 0f40800000;
	mul.f32 	%f247, %f245, %f246;
	cvt.f64.f32	%fd788, %f247;
	div.rn.f64 	%fd789, %fd981, %fd788;
	add.f64 	%fd228, %fd214, %fd789;
	ld.global.f32 	%f248, [%rd2+20];
	cvt.f64.f32	%fd982, %f248;
	abs.f64 	%fd790, %fd982;
	setp.neu.f64	%p119, %fd790, 0d7FF0000000000000;
	@%p119 bra 	BB0_176;

	mov.f64 	%fd791, 0d0000000000000000;
	mul.rn.f64 	%fd982, %fd982, %fd791;

BB0_176:
	mul.f64 	%fd792, %fd982, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r337, %fd792;
	st.local.u32 	[%rd120], %r337;
	cvt.rn.f64.s32	%fd793, %r337;
	neg.f64 	%fd794, %fd793;
	fma.rn.f64 	%fd796, %fd794, %fd636, %fd982;
	fma.rn.f64 	%fd798, %fd794, %fd638, %fd796;
	fma.rn.f64 	%fd983, %fd794, %fd640, %fd798;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r260}, %fd982;
	}
	and.b32  	%r261, %r260, 2145386496;
	setp.lt.u32	%p120, %r261, 1105199104;
	@%p120 bra 	BB0_178;

	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd982;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd983, [retval0+0];
	
	//{
	}// Callseq End 16
	ld.local.u32 	%r337, [%rd120];

BB0_178:
	and.b32  	%r262, %r337, 1;
	shl.b32 	%r263, %r262, 3;
	setp.eq.b32	%p121, %r262, 1;
	selp.f64	%fd800, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p121;
	mul.wide.u32 	%rd165, %r263, 8;
	add.s64 	%rd167, %rd165, %rd124;
	ld.const.f64 	%fd801, [%rd167+8];
	mul.rn.f64 	%fd235, %fd983, %fd983;
	fma.rn.f64 	%fd802, %fd800, %fd235, %fd801;
	ld.const.f64 	%fd803, [%rd167+16];
	fma.rn.f64 	%fd804, %fd802, %fd235, %fd803;
	ld.const.f64 	%fd805, [%rd167+24];
	fma.rn.f64 	%fd806, %fd804, %fd235, %fd805;
	ld.const.f64 	%fd807, [%rd167+32];
	fma.rn.f64 	%fd808, %fd806, %fd235, %fd807;
	ld.const.f64 	%fd809, [%rd167+40];
	fma.rn.f64 	%fd810, %fd808, %fd235, %fd809;
	ld.const.f64 	%fd811, [%rd167+48];
	fma.rn.f64 	%fd236, %fd810, %fd235, %fd811;
	fma.rn.f64 	%fd984, %fd236, %fd983, %fd983;
	setp.eq.s32	%p122, %r262, 0;
	@%p122 bra 	BB0_180;

	mov.f64 	%fd812, 0d3FF0000000000000;
	fma.rn.f64 	%fd984, %fd236, %fd235, %fd812;

BB0_180:
	and.b32  	%r264, %r337, 2;
	setp.eq.s32	%p123, %r264, 0;
	@%p123 bra 	BB0_182;

	mov.f64 	%fd813, 0d0000000000000000;
	mov.f64 	%fd814, 0dBFF0000000000000;
	fma.rn.f64 	%fd984, %fd984, %fd814, %fd813;

BB0_182:
	ld.global.f32 	%f249, [%rd2+20];
	cvt.f64.f32	%fd985, %f249;
	abs.f64 	%fd815, %fd985;
	setp.neu.f64	%p124, %fd815, 0d7FF0000000000000;
	@%p124 bra 	BB0_184;

	mov.f64 	%fd816, 0d0000000000000000;
	mul.rn.f64 	%fd985, %fd985, %fd816;

BB0_184:
	mul.f64 	%fd817, %fd985, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r338, %fd817;
	st.local.u32 	[%rd120], %r338;
	cvt.rn.f64.s32	%fd818, %r338;
	neg.f64 	%fd819, %fd818;
	fma.rn.f64 	%fd821, %fd819, %fd636, %fd985;
	fma.rn.f64 	%fd823, %fd819, %fd638, %fd821;
	fma.rn.f64 	%fd986, %fd819, %fd640, %fd823;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r265}, %fd985;
	}
	and.b32  	%r266, %r265, 2145386496;
	setp.lt.u32	%p125, %r266, 1105199104;
	@%p125 bra 	BB0_186;

	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd985;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd986, [retval0+0];
	
	//{
	}// Callseq End 17
	ld.local.u32 	%r338, [%rd120];

BB0_186:
	and.b32  	%r267, %r338, 1;
	shl.b32 	%r268, %r267, 3;
	setp.eq.b32	%p126, %r267, 1;
	selp.f64	%fd825, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p126;
	mul.wide.u32 	%rd172, %r268, 8;
	add.s64 	%rd174, %rd172, %rd124;
	ld.const.f64 	%fd826, [%rd174+8];
	mul.rn.f64 	%fd248, %fd986, %fd986;
	fma.rn.f64 	%fd827, %fd825, %fd248, %fd826;
	ld.const.f64 	%fd828, [%rd174+16];
	fma.rn.f64 	%fd829, %fd827, %fd248, %fd828;
	ld.const.f64 	%fd830, [%rd174+24];
	fma.rn.f64 	%fd831, %fd829, %fd248, %fd830;
	ld.const.f64 	%fd832, [%rd174+32];
	fma.rn.f64 	%fd833, %fd831, %fd248, %fd832;
	ld.const.f64 	%fd834, [%rd174+40];
	fma.rn.f64 	%fd835, %fd833, %fd248, %fd834;
	ld.const.f64 	%fd836, [%rd174+48];
	fma.rn.f64 	%fd249, %fd835, %fd248, %fd836;
	fma.rn.f64 	%fd987, %fd249, %fd986, %fd986;
	setp.eq.s32	%p127, %r267, 0;
	@%p127 bra 	BB0_188;

	mov.f64 	%fd837, 0d3FF0000000000000;
	fma.rn.f64 	%fd987, %fd249, %fd248, %fd837;

BB0_188:
	and.b32  	%r269, %r338, 2;
	setp.eq.s32	%p128, %r269, 0;
	@%p128 bra 	BB0_190;

	mov.f64 	%fd838, 0d0000000000000000;
	mov.f64 	%fd839, 0dBFF0000000000000;
	fma.rn.f64 	%fd987, %fd987, %fd839, %fd838;

BB0_190:
	cvt.rn.f32.f64	%f90, %fd228;
	ld.global.f32 	%f250, [%rd2+12];
	add.f32 	%f251, %f250, %f250;
	mul.f32 	%f252, %f250, %f251;
	cvt.f64.f32	%fd840, %f252;
	mul.f64 	%fd841, %fd984, %fd987;
	div.rn.f64 	%fd255, %fd841, %fd840;
	ld.global.f32 	%f253, [%rd2+20];
	cvt.f64.f32	%fd988, %f253;
	abs.f64 	%fd842, %fd988;
	setp.neu.f64	%p129, %fd842, 0d7FF0000000000000;
	@%p129 bra 	BB0_192;

	mov.f64 	%fd843, 0d0000000000000000;
	mul.rn.f64 	%fd988, %fd988, %fd843;

BB0_192:
	mul.f64 	%fd844, %fd988, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r339, %fd844;
	st.local.u32 	[%rd120], %r339;
	cvt.rn.f64.s32	%fd845, %r339;
	neg.f64 	%fd846, %fd845;
	fma.rn.f64 	%fd848, %fd846, %fd636, %fd988;
	fma.rn.f64 	%fd850, %fd846, %fd638, %fd848;
	fma.rn.f64 	%fd989, %fd846, %fd640, %fd850;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r270}, %fd988;
	}
	and.b32  	%r271, %r270, 2145386496;
	setp.lt.u32	%p130, %r271, 1105199104;
	@%p130 bra 	BB0_194;

	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd988;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd989, [retval0+0];
	
	//{
	}// Callseq End 18
	ld.local.u32 	%r339, [%rd120];

BB0_194:
	add.s32 	%r89, %r339, 1;
	and.b32  	%r272, %r89, 1;
	shl.b32 	%r273, %r272, 3;
	setp.eq.b32	%p131, %r272, 1;
	selp.f64	%fd852, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p131;
	mul.wide.u32 	%rd179, %r273, 8;
	add.s64 	%rd181, %rd179, %rd124;
	ld.const.f64 	%fd853, [%rd181+8];
	mul.rn.f64 	%fd262, %fd989, %fd989;
	fma.rn.f64 	%fd854, %fd852, %fd262, %fd853;
	ld.const.f64 	%fd855, [%rd181+16];
	fma.rn.f64 	%fd856, %fd854, %fd262, %fd855;
	ld.const.f64 	%fd857, [%rd181+24];
	fma.rn.f64 	%fd858, %fd856, %fd262, %fd857;
	ld.const.f64 	%fd859, [%rd181+32];
	fma.rn.f64 	%fd860, %fd858, %fd262, %fd859;
	ld.const.f64 	%fd861, [%rd181+40];
	fma.rn.f64 	%fd862, %fd860, %fd262, %fd861;
	ld.const.f64 	%fd863, [%rd181+48];
	fma.rn.f64 	%fd263, %fd862, %fd262, %fd863;
	fma.rn.f64 	%fd990, %fd263, %fd989, %fd989;
	setp.eq.s32	%p132, %r272, 0;
	@%p132 bra 	BB0_196;

	mov.f64 	%fd864, 0d3FF0000000000000;
	fma.rn.f64 	%fd990, %fd263, %fd262, %fd864;

BB0_196:
	and.b32  	%r274, %r89, 2;
	setp.eq.s32	%p133, %r274, 0;
	@%p133 bra 	BB0_198;

	mov.f64 	%fd865, 0d0000000000000000;
	mov.f64 	%fd866, 0dBFF0000000000000;
	fma.rn.f64 	%fd990, %fd990, %fd866, %fd865;

BB0_198:
	ld.global.f32 	%f254, [%rd2+20];
	cvt.f64.f32	%fd991, %f254;
	abs.f64 	%fd867, %fd991;
	setp.neu.f64	%p134, %fd867, 0d7FF0000000000000;
	@%p134 bra 	BB0_200;

	mov.f64 	%fd868, 0d0000000000000000;
	mul.rn.f64 	%fd991, %fd991, %fd868;

BB0_200:
	mul.f64 	%fd869, %fd991, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r340, %fd869;
	st.local.u32 	[%rd120], %r340;
	cvt.rn.f64.s32	%fd870, %r340;
	neg.f64 	%fd871, %fd870;
	fma.rn.f64 	%fd873, %fd871, %fd636, %fd991;
	fma.rn.f64 	%fd875, %fd871, %fd638, %fd873;
	fma.rn.f64 	%fd992, %fd871, %fd640, %fd875;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r275}, %fd991;
	}
	and.b32  	%r276, %r275, 2145386496;
	setp.lt.u32	%p135, %r276, 1105199104;
	@%p135 bra 	BB0_202;

	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd991;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd992, [retval0+0];
	
	//{
	}// Callseq End 19
	ld.local.u32 	%r340, [%rd120];

BB0_202:
	add.s32 	%r93, %r340, 1;
	and.b32  	%r277, %r93, 1;
	shl.b32 	%r278, %r277, 3;
	setp.eq.b32	%p136, %r277, 1;
	selp.f64	%fd877, 0dBDA8FF8320FD8164, 0d3DE5DB65F9785EBA, %p136;
	mul.wide.u32 	%rd186, %r278, 8;
	add.s64 	%rd188, %rd186, %rd124;
	ld.const.f64 	%fd878, [%rd188+8];
	mul.rn.f64 	%fd275, %fd992, %fd992;
	fma.rn.f64 	%fd879, %fd877, %fd275, %fd878;
	ld.const.f64 	%fd880, [%rd188+16];
	fma.rn.f64 	%fd881, %fd879, %fd275, %fd880;
	ld.const.f64 	%fd882, [%rd188+24];
	fma.rn.f64 	%fd883, %fd881, %fd275, %fd882;
	ld.const.f64 	%fd884, [%rd188+32];
	fma.rn.f64 	%fd885, %fd883, %fd275, %fd884;
	ld.const.f64 	%fd886, [%rd188+40];
	fma.rn.f64 	%fd887, %fd885, %fd275, %fd886;
	ld.const.f64 	%fd888, [%rd188+48];
	fma.rn.f64 	%fd276, %fd887, %fd275, %fd888;
	fma.rn.f64 	%fd993, %fd276, %fd992, %fd992;
	setp.eq.s32	%p137, %r277, 0;
	@%p137 bra 	BB0_204;

	mov.f64 	%fd889, 0d3FF0000000000000;
	fma.rn.f64 	%fd993, %fd276, %fd275, %fd889;

BB0_204:
	and.b32  	%r279, %r93, 2;
	setp.eq.s32	%p138, %r279, 0;
	@%p138 bra 	BB0_206;

	mov.f64 	%fd890, 0d0000000000000000;
	mov.f64 	%fd891, 0dBFF0000000000000;
	fma.rn.f64 	%fd993, %fd993, %fd891, %fd890;

BB0_206:
	ld.global.f32 	%f257, [%rd2+16];
	add.f32 	%f258, %f257, %f257;
	mul.f32 	%f259, %f257, %f258;
	cvt.f64.f32	%fd892, %f259;
	mul.f64 	%fd893, %fd990, %fd993;
	div.rn.f64 	%fd894, %fd893, %fd892;
	add.f64 	%fd282, %fd255, %fd894;
	mov.f32 	%f313, 0f00000000;
	mov.f32 	%f312, %f313;
	@%p2 bra 	BB0_212;

	mov.u32 	%r311, %ctaid.x;
	mov.u32 	%r310, %nctaid.x;
	mov.u32 	%r309, %ctaid.y;
	mad.lo.s32 	%r308, %r309, %r310, %r311;
	cvt.rn.f32.f64	%f91, %fd282;
	ld.global.f32 	%f262, [%rd2];
	cvt.f64.f32	%fd283, %f262;
	ld.global.f32 	%f92, [%rd2+4];
	add.f32 	%f93, %f90, %f90;
	ld.global.f32 	%f94, [%rd2+8];
	ld.global.f32 	%f95, [%rd2+24];
	mul.lo.s32 	%r287, %r2, %r308;
	mul.wide.s32 	%rd193, %r287, 4;
	add.s64 	%rd197, %rd1, %rd193;
	mov.f32 	%f313, 0f00000000;
	mov.u32 	%r341, 0;
	mov.f32 	%f312, %f313;

BB0_208:
	rem.s32 	%r288, %r341, %r100;
	cvt.rn.f32.s32	%f263, %r288;
	sub.f32 	%f264, %f263, %f92;
	mul.f32 	%f265, %f89, %f264;
	mul.f32 	%f266, %f264, %f265;
	mul.f32 	%f267, %f93, %f264;
	div.s32 	%r289, %r341, %r100;
	cvt.rn.f32.s32	%f268, %r289;
	sub.f32 	%f269, %f268, %f94;
	mul.f32 	%f270, %f267, %f269;
	sub.f32 	%f271, %f266, %f270;
	mul.f32 	%f272, %f91, %f269;
	fma.rn.f32 	%f98, %f269, %f272, %f271;
	cvt.f64.f32	%fd284, %f98;
	neg.f64 	%fd895, %fd284;
	mov.f64 	%fd896, 0d4338000000000000;
	mov.f64 	%fd897, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd898, %fd895, %fd897, %fd896;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r96, %temp}, %fd898;
	}
	mov.f64 	%fd899, 0dC338000000000000;
	add.rn.f64 	%fd900, %fd898, %fd899;
	mov.f64 	%fd901, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd902, %fd900, %fd901, %fd895;
	mov.f64 	%fd903, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd904, %fd900, %fd903, %fd902;
	mov.f64 	%fd905, 0d3E928AF3FCA213EA;
	mov.f64 	%fd906, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd907, %fd906, %fd904, %fd905;
	mov.f64 	%fd908, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd909, %fd907, %fd904, %fd908;
	mov.f64 	%fd910, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd911, %fd909, %fd904, %fd910;
	mov.f64 	%fd912, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd913, %fd911, %fd904, %fd912;
	mov.f64 	%fd914, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd915, %fd913, %fd904, %fd914;
	mov.f64 	%fd916, 0d3F81111111122322;
	fma.rn.f64 	%fd917, %fd915, %fd904, %fd916;
	mov.f64 	%fd918, 0d3FA55555555502A1;
	fma.rn.f64 	%fd919, %fd917, %fd904, %fd918;
	mov.f64 	%fd920, 0d3FC5555555555511;
	fma.rn.f64 	%fd921, %fd919, %fd904, %fd920;
	mov.f64 	%fd922, 0d3FE000000000000B;
	fma.rn.f64 	%fd923, %fd921, %fd904, %fd922;
	mov.f64 	%fd924, 0d3FF0000000000000;
	fma.rn.f64 	%fd925, %fd923, %fd904, %fd924;
	fma.rn.f64 	%fd926, %fd925, %fd904, %fd924;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r97, %temp}, %fd926;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r98}, %fd926;
	}
	shl.b32 	%r290, %r96, 20;
	add.s32 	%r291, %r98, %r290;
	mov.b64 	%fd994, {%r97, %r291};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r292}, %fd895;
	}
	mov.b32 	 %f273, %r292;
	abs.f32 	%f99, %f273;
	setp.lt.f32	%p140, %f99, 0f4086232B;
	@%p140 bra 	BB0_211;

	setp.gt.f32	%p141, %f98, 0f80000000;
	mov.f64 	%fd927, 0d7FF0000000000000;
	sub.f64 	%fd928, %fd927, %fd284;
	selp.f64	%fd994, 0d0000000000000000, %fd928, %p141;
	setp.geu.f32	%p142, %f99, 0f40874800;
	@%p142 bra 	BB0_211;

	shr.u32 	%r293, %r96, 31;
	add.s32 	%r294, %r96, %r293;
	shr.s32 	%r295, %r294, 1;
	shl.b32 	%r296, %r295, 20;
	add.s32 	%r297, %r296, %r98;
	mov.b64 	%fd929, {%r97, %r297};
	sub.s32 	%r298, %r96, %r295;
	shl.b32 	%r299, %r298, 20;
	add.s32 	%r300, %r299, 1072693248;
	mov.u32 	%r301, 0;
	mov.b64 	%fd930, {%r301, %r300};
	mul.f64 	%fd994, %fd929, %fd930;

BB0_211:
	mul.f64 	%fd931, %fd283, %fd994;
	cvt.rn.f32.f64	%f274, %fd931;
	add.f32 	%f275, %f95, %f274;
	add.f32 	%f312, %f312, %f275;
	ld.global.u32 	%r302, [%rd197];
	cvt.rn.f32.s32	%f276, %r302;
	sub.f32 	%f277, %f275, %f276;
	fma.rn.f32 	%f313, %f277, %f277, %f313;
	add.s64 	%rd197, %rd197, 4;
	add.s32 	%r341, %r341, 1;
	setp.lt.s32	%p143, %r341, %r2;
	@%p143 bra 	BB0_208;

BB0_212:
	st.global.f32 	[%rd2], %f312;
	div.rn.f32 	%f278, %f313, %f292;
	mov.f32 	%f279, 0f3F800000;
	sub.f32 	%f280, %f279, %f278;
	st.global.f32 	[%rd2+24], %f280;

BB0_213:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot1[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot1;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB1_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd93, %rd1;
	@%p2 bra 	BB1_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	mov.u64 	%rd92, %rd1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	neg.s32 	%r25, %r24;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd45, %rd43, %rd44;
	add.s64 	%rd90, %rd45, 120;
	mov.u64 	%rd94, 0;
	mov.u64 	%rd91, %rd1;
	mov.u32 	%r39, %r7;

BB1_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r39;
	mov.u64 	%rd7, %rd91;
	ld.const.u64 	%rd48, [%rd90];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd48;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd94;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd46, {r0,r1};      
	mov.b64         %rd94, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd92], %rd46;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r26, %r9, %r7;
	mul.wide.s32 	%rd51, %r26, 8;
	add.s64 	%rd92, %rd1, %rd51;
	add.s64 	%rd13, %rd7, 8;
	mov.u64 	%rd93, %rd13;
	add.s64 	%rd90, %rd90, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd91, %rd13;
	mov.u32 	%r39, %r9;
	@%p3 bra 	BB1_3;

BB1_4:
	st.local.u64 	[%rd93], %rd94;
	ld.local.u64 	%rd95, [%rd1+16];
	ld.local.u64 	%rd96, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB1_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r10;
	shl.b64 	%rd52, %rd96, %r10;
	shr.u64 	%rd53, %rd95, %r28;
	or.b64  	%rd96, %rd52, %rd53;
	shl.b64 	%rd54, %rd95, %r10;
	ld.local.u64 	%rd55, [%rd1+8];
	shr.u64 	%rd56, %rd55, %r28;
	or.b64  	%rd95, %rd56, %rd54;

BB1_6:
	cvta.to.local.u64 	%rd57, %rd37;
	shr.u64 	%rd58, %rd96, 62;
	cvt.u32.u64	%r29, %rd58;
	shr.u64 	%rd59, %rd95, 62;
	shl.b64 	%rd60, %rd96, 2;
	or.b64  	%rd98, %rd60, %rd59;
	shl.b64 	%rd97, %rd95, 2;
	shr.u64 	%rd61, %rd96, 61;
	cvt.u32.u64	%r30, %rd61;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.ne.s32	%p5, %r40, 0;
	selp.b32	%r34, %r33, %r32, %p5;
	st.local.u32 	[%rd57], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB1_8;

	mov.u64 	%rd65, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd65;
	mov.b64         {a2,a3}, %rd65;
	mov.b64         {b0,b1}, %rd97;
	mov.b64         {b2,b3}, %rd98;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd97, {r0,r1};
	mov.b64         %rd98, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB1_8:
	clz.b64 	%r41, %rd98;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB1_10;

	shl.b64 	%rd68, %rd98, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd69, %rd97, %r36;
	or.b64  	%rd98, %rd69, %rd68;

BB1_10:
	mov.u64 	%rd73, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd98;   
	mov.b64         {blo,bhi}, %rd73;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd70, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB1_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd70;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd70;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd74, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB1_12:
	cvt.u64.u32	%rd80, %r40;
	shl.b64 	%rd81, %rd80, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd82, %r38;
	shl.b64 	%rd83, %rd82, 52;
	add.s64 	%rd84, %rd99, 1;
	shr.u64 	%rd85, %rd84, 10;
	add.s64 	%rd86, %rd85, 1;
	shr.u64 	%rd87, %rd86, 1;
	add.s64 	%rd88, %rd87, %rd83;
	or.b64  	%rd89, %rd88, %rd81;
	mov.b64 	 %fd4, %rd89;

BB1_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


